{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "emotional-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alleged-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for experiment\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # making sure GPU runs are deterministic even if they are slower\n",
    "    print(\"Seeded everything: {}\".format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "protected-raising",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeded everything: 42\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-jungle",
   "metadata": {},
   "source": [
    "# Test how the straight through estimator works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-mailman",
   "metadata": {},
   "source": [
    "$$\\text{With indicators:}\\quad f(x_1, x_2) = \\mathbb{1}_{x_1 > 5}x_1^2 + \\mathbb{1}_{x_2 > 5}x_2^2$$\n",
    "$$\\text{Without indicators:}\\quad g(x_1, x_2) = x_1^2 + x_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accepted-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns 1_{x > 5}\n",
    "class GetIndicators(autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "            In the forward pass we receive a Tensor containing the input and return\n",
    "            a Tensor containing the output. ctx is a context object that can be used\n",
    "            to stash information for backward computation. You can cache arbitrary\n",
    "            objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        mask = (x > 5).int()\n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        \"\"\"\n",
    "            In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "            with respect to the output, and we need to compute the gradient of the loss\n",
    "            with respect to the input.\n",
    "            NOTE: We need only \"g\" here because we have only one input -> x\n",
    "        \"\"\"\n",
    "        # NOTE: This isn't getting printed! I don't know why\n",
    "        print(\"Incoming grad = Outgoing grad = {}\".format(g))\n",
    "        # send the gradient g straight-through on the backward pass.\n",
    "        return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-reviewer",
   "metadata": {},
   "source": [
    "## See if the indicator function works as expected (it does)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "enclosed-railway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([1, 6])\n",
    "GetIndicators.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "boolean-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    mask = GetIndicators.apply(x)\n",
    "    out = (mask * x** 2).sum()\n",
    "    return out\n",
    "\n",
    "def g(x):\n",
    "    out = (x**2).sum()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-defense",
   "metadata": {},
   "source": [
    "$$\\nabla g(x) = \\begin{bmatrix} 2x_1\\\\ 2x_2 \\end{bmatrix}$$\n",
    "\n",
    "### Assuming STE\n",
    "$$\\frac{\\partial f(x)}{\\partial x_i} = \\mathbb{1}_{x_i > 5}\\frac{\\partial x_i^2}{x_i} + x_i^2 \\frac{\\partial \\mathbb{1}_{x_i > 5}}{\\partial x_i} = 2x_i\\mathbb{1}_{x_i > 5} + x_i^2 \\frac{\\partial \\mathbb{1}_{x_i > 5}}{\\partial x_i} = ?$$\n",
    "\n",
    "(Is the grad of indicator just 0? That's not STE, is it?)\n",
    "\n",
    "### Now check if that is what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acknowledged-error",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad(f(x)) = tensor([ 0., 12.])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2, 6]), requires_grad=True)\n",
    "f_out = f(x)\n",
    "f_out.backward()\n",
    "print(\"grad(f(x)) = {}\".format(x.grad.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "parallel-saturday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad(g(x)) = tensor([ 4., 12.])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2, 6]), requires_grad=True)\n",
    "g_out = g(x)\n",
    "g_out.backward()\n",
    "print(\"grad(g(x)) = {}\".format(x.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-assist",
   "metadata": {},
   "source": [
    "## This doesn't make sense. The gradient of g is as expected (2x) but the gradient of f seems to be $\\frac{\\partial f(x)}{\\partial x_i} = \\mathbb{1}_{x_i > 5} 2x_i$? I expected it to apply the chain rule and then use the STE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-priest",
   "metadata": {},
   "source": [
    "# Now, try a more typical usage. Threshold of a function like Bengio's example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-shell",
   "metadata": {},
   "source": [
    "$$f(x_1, x_2) = \\mathbf{1}^T \\mathbb{1}_{x^2 > 25} = \\mathbb{1}_{x_1^2 > 25} + \\mathbb{1}_{x_2^2 > 25}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "toxic-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns 1_{x^2 > 25} (which is actually the same as x > 5)\n",
    "class GetIndicators(autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "            In the forward pass we receive a Tensor containing the input and return\n",
    "            a Tensor containing the output. ctx is a context object that can be used\n",
    "            to stash information for backward computation. You can cache arbitrary\n",
    "            objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        mask = (x**2 > 25).int()\n",
    "        return mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        \"\"\"\n",
    "            In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "            with respect to the output, and we need to compute the gradient of the loss\n",
    "            with respect to the input.\n",
    "            NOTE: We need only \"g\" here because we have only one input -> x\n",
    "        \"\"\"\n",
    "        # send the gradient g straight-through on the backward pass.\n",
    "        print(\"Incoming grad = Outgoing grad = {}\".format(g))\n",
    "        return g\n",
    "\n",
    "def f(x):\n",
    "    mask = GetIndicators.apply(x)\n",
    "    # if I don't include a .float(), then I can't set requires_grad=True\n",
    "    out = mask.sum().float()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "magnetic-battlefield",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1bbfd3d5756f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mf_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad(f(x)) = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2, 6]), requires_grad=True)\n",
    "# need to set requires_grad=True, otherwise it says it can't differentiate f\n",
    "f_out = Variable(f(x), requires_grad=True)\n",
    "f_out.backward()\n",
    "print(\"grad(f(x)) = {}\".format(x.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-shark",
   "metadata": {},
   "source": [
    "## Gradient is empty? Wut!?!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-property",
   "metadata": {},
   "source": [
    "# Let's see how this plays with clamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-mortgage",
   "metadata": {},
   "source": [
    "$$ f(x_1, x_2) = \\mathbf{1}^T((P_{[1, 4]}(x))^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "enormous-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = torch.clamp(x, 1, 4)\n",
    "    out = (x**2).sum()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "judicial-efficiency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad(f(x)) = tensor([4., 0.])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2, 6]), requires_grad=True)\n",
    "# need to set requires_grad=True, otherwise it says it can't differentiate f\n",
    "f_out = f(x)\n",
    "f_out.backward()\n",
    "print(\"grad(f(x)) = {}\".format(x.grad.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-export",
   "metadata": {},
   "source": [
    "## As expected, any projected value just gets 0 gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-valve",
   "metadata": {},
   "source": [
    "## The right way to project imo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "motivated-headset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad(f(x)) = tensor([4., 8.])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.Tensor([2, 6]), requires_grad=True)\n",
    "# need to set requires_grad=True, otherwise it says it can't differentiate f\n",
    "with torch.no_grad():\n",
    "    x = torch.clamp(x, 1, 4)\n",
    "# need to reset requires_grad\n",
    "x.requires_grad=True\n",
    "f_out = f(x)\n",
    "f_out.backward()\n",
    "print(\"grad(f(x)) = {}\".format(x.grad.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-execution",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
