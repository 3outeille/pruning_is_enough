nohup: ignoring input
=> Reading YAML config from configs/hypercube/transformer/transformer_base.yml
Namespace(algo='hc_iter', alpha=1.0, alpha_prime=1.0, arch='transformer', batch_size=20, bias=False, bn_type='NonAffineBatchNorm', bottom_k_on_forward=False, checkpoint_at_prune=False, chg_mask=False, chg_weight=False, ckpt_interval=-1, config='configs/hypercube/transformer/transformer_base.yml', conv_type='SubnetConv', data='transformer_data/wikitext-2', data_dir='../data', dataset='MNIST', differentiate_clamp=False, dist_backend='nccl', epochs=6, evaluate=False, evaluate_only=False, fine_tune_lr=5.0, fine_tune_lr_policy='multistep_lr', fine_tune_optimizer='sgd', fine_tune_wd=0.0, first_layer_dense=False, first_layer_type=None, fixed_init=False, flips=None, freeze_weights=True, gamma=0.1, gpu=3, hc_period=1, hc_quantized=True, hc_warmup=9999, hidden_size=500, how_to_connect='prob', how_to_prune='random', imp_no_rewind=False, imp_resume_round=-1, imp_rewind_iter=1000, imp_rewind_model='short_imp/Liu_checkpoint_model_correct.pth', init='signed_constant', interpolate='prob', invert_sanity_check=False, iter_period=1, iter_start=0, label_smoothing=None, lam_finetune_loss=-1, last_layer_dense=False, lmbda=1e-05, load_ckpt=None, log_dir=None, log_interval=2, loss='cross-entropy-loss', lr=5.0, lr_adjust=50, lr_gamma=0.25, lr_policy='multistep_lr', max_iter=100000, metric='loss', milestones=[50, 100, 150, 200], mixed_precision=0, mode='fan_in', mode_connect=False, mode_connect_filename=None, momentum=0.0, multiprocessing_distributed=False, name=None, nesterov=False, no_bn_decay=False, no_cuda=False, noise=True, noise_ratio=0, nonlinearity='relu', num_round=1, num_step_finetune=10, num_test=1, num_trial=1, num_workers=4, optimizer='sgd', override_prune_rate=False, plot_hc_convergence=False, pretrained=None, pretrained2=None, print_freq=10, project_freq=1, prune_rate=0.5, prune_type='BottomK', pruning_strategy=None, quantize_threshold=0.5, random_subnet=False, rank=-1, regularization='L2', reinit=False, results_filename=None, resume=None, rewind_score=False, rewind_to_epoch=-1, round='naive', run_idx=None, save_every=-1, save_model=False, save_plot_data=False, scale_fan=False, score_init='unif', score_init_constant=None, seed=42, seed_fixed_init=24, shift=0.0, shuffle=False, skip_fine_tune=True, skip_sanity_checks=False, smart_ratio=-1, start_epoch=None, start_from_nothing=False, subfolder='transformer_50_sanity', submask_size=1, target_sparsity=50.0, td=0.99, temp=100000, trainer='default', transformer_bptt=35, transformer_clip=0.25, transformer_dropout=0.2, transformer_emsize=200, transformer_nhead=2, transformer_nhid=200, transformer_nlayers=2, trial_num=1, unflag_before_finetune=True, warmup_length=0, wd=0.0005, weight_training=False, width=1.0, width_mult=1.0, workers=4, world_size=-1, **{'compare-rounding': False})
Seeded everything: 42
Use GPU: 3 for training
transformer_data/wikitext-2/train.txt
transformer_data/wikitext-2/valid.txt
transformer_data/wikitext-2/test.txt
==> Conv Type: SubnetConv
==> BN Type: NonAffineBatchNorm
Computing prune_rate for target_sparsity 50.0 with iter_period 1
Setting prune_rate to 0.12944943670387588
| epoch   0 |   200/ 2983 batches | lr 5.00 | ms/batch 78.29 | loss 15.97 | ppl 8604938.76
| epoch   0 |   400/ 2983 batches | lr 5.00 | ms/batch 62.70 | loss 10.49 | ppl 36126.24
| epoch   0 |   600/ 2983 batches | lr 5.00 | ms/batch 61.80 | loss  8.33 | ppl  4148.66
| epoch   0 |   800/ 2983 batches | lr 5.00 | ms/batch 62.14 | loss  7.99 | ppl  2954.24
| epoch   0 |  1000/ 2983 batches | lr 5.00 | ms/batch 61.62 | loss  7.61 | ppl  2020.09
| epoch   0 |  1200/ 2983 batches | lr 5.00 | ms/batch 62.39 | loss  7.65 | ppl  2091.16
| epoch   0 |  1400/ 2983 batches | lr 5.00 | ms/batch 62.65 | loss  7.44 | ppl  1704.49
| epoch   0 |  1600/ 2983 batches | lr 5.00 | ms/batch 62.07 | loss  7.15 | ppl  1276.59
| epoch   0 |  1800/ 2983 batches | lr 5.00 | ms/batch 62.00 | loss  7.34 | ppl  1541.30
| epoch   0 |  2000/ 2983 batches | lr 5.00 | ms/batch 61.83 | loss  7.18 | ppl  1318.77
| epoch   0 |  2200/ 2983 batches | lr 5.00 | ms/batch 62.88 | loss  7.40 | ppl  1630.97
| epoch   0 |  2400/ 2983 batches | lr 5.00 | ms/batch 61.96 | loss  7.33 | ppl  1530.59
| epoch   0 |  2600/ 2983 batches | lr 5.00 | ms/batch 62.37 | loss  7.36 | ppl  1568.33
| epoch   0 |  2800/ 2983 batches | lr 5.00 | ms/batch 61.78 | loss  7.61 | ppl  2011.40
-----------------------------------------------------------------------------------------
| end of epoch   0 | time: 198.41s | valid loss  7.01 | valid ppl  1112.44
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 7.01962853781879e-05
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   33470 /   40000 ( 83.67%) | total_pruned =    6530 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   33378 /   40000 ( 83.44%) | total_pruned =    6622 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   20282 /   40000 ( 50.70%) | total_pruned =   19718 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   32809 /   40000 ( 82.02%) | total_pruned =    7191 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   32877 /   40000 ( 82.19%) | total_pruned =    7123 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   33478 /   40000 ( 83.69%) | total_pruned =    6522 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   33375 /   40000 ( 83.44%) | total_pruned =    6625 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   20464 /   40000 ( 51.16%) | total_pruned =   19536 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   32419 /   40000 ( 81.05%) | total_pruned =    7581 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   32753 /   40000 ( 81.88%) | total_pruned =    7247 | shape = (200, 200)
alive: 305305, pruned : 94695, total: 400000, ( 76.33% remained)
Model avg sparsity: 0.7632625
| epoch   1 |   200/ 2983 batches | lr 5.00 | ms/batch 62.97 | loss  7.43 | ppl  1684.53
| epoch   1 |   400/ 2983 batches | lr 5.00 | ms/batch 62.93 | loss  7.39 | ppl  1623.10
| epoch   1 |   600/ 2983 batches | lr 5.00 | ms/batch 62.38 | loss  7.15 | ppl  1278.73
| epoch   1 |   800/ 2983 batches | lr 5.00 | ms/batch 62.02 | loss  7.62 | ppl  2030.00
| epoch   1 |  1000/ 2983 batches | lr 5.00 | ms/batch 62.40 | loss  7.43 | ppl  1693.70
| epoch   1 |  1200/ 2983 batches | lr 5.00 | ms/batch 62.62 | loss  7.60 | ppl  1997.23
| epoch   1 |  1400/ 2983 batches | lr 5.00 | ms/batch 63.19 | loss  7.44 | ppl  1699.46
| epoch   1 |  1600/ 2983 batches | lr 5.00 | ms/batch 61.97 | loss  7.15 | ppl  1277.83
| epoch   1 |  1800/ 2983 batches | lr 5.00 | ms/batch 62.42 | loss  7.28 | ppl  1446.81
| epoch   1 |  2000/ 2983 batches | lr 5.00 | ms/batch 62.47 | loss  7.21 | ppl  1353.58
| epoch   1 |  2200/ 2983 batches | lr 5.00 | ms/batch 62.52 | loss  7.41 | ppl  1659.99
| epoch   1 |  2400/ 2983 batches | lr 5.00 | ms/batch 62.20 | loss  7.36 | ppl  1573.47
| epoch   1 |  2600/ 2983 batches | lr 5.00 | ms/batch 62.12 | loss  7.36 | ppl  1566.30
| epoch   1 |  2800/ 2983 batches | lr 5.00 | ms/batch 62.37 | loss  7.56 | ppl  1925.90
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 196.02s | valid loss  6.97 | valid ppl  1068.82
-----------------------------------------------------------------------------------------
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   33470 /   40000 ( 83.67%) | total_pruned =    6530 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   33378 /   40000 ( 83.44%) | total_pruned =    6622 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   20282 /   40000 ( 50.70%) | total_pruned =   19718 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   32809 /   40000 ( 82.02%) | total_pruned =    7191 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   32877 /   40000 ( 82.19%) | total_pruned =    7123 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   33478 /   40000 ( 83.69%) | total_pruned =    6522 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   33375 /   40000 ( 83.44%) | total_pruned =    6625 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   20464 /   40000 ( 51.16%) | total_pruned =   19536 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   32419 /   40000 ( 81.05%) | total_pruned =    7581 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   32753 /   40000 ( 81.88%) | total_pruned =    7247 | shape = (200, 200)
alive: 305305, pruned : 94695, total: 400000, ( 76.33% remained)
Model avg sparsity: 0.7632625
| epoch   2 |   200/ 2983 batches | lr 5.00 | ms/batch 63.39 | loss  7.46 | ppl  1730.19
| epoch   2 |   400/ 2983 batches | lr 5.00 | ms/batch 61.84 | loss  7.38 | ppl  1603.10
| epoch   2 |   600/ 2983 batches | lr 5.00 | ms/batch 62.32 | loss  7.22 | ppl  1360.98
| epoch   2 |   800/ 2983 batches | lr 5.00 | ms/batch 61.85 | loss  7.53 | ppl  1864.67
| epoch   2 |  1000/ 2983 batches | lr 5.00 | ms/batch 62.71 | loss  7.44 | ppl  1699.79
| epoch   2 |  1200/ 2983 batches | lr 5.00 | ms/batch 62.01 | loss  7.58 | ppl  1960.61
| epoch   2 |  1400/ 2983 batches | lr 5.00 | ms/batch 62.30 | loss  7.41 | ppl  1655.22
| epoch   2 |  1600/ 2983 batches | lr 5.00 | ms/batch 62.13 | loss  7.13 | ppl  1242.81
| epoch   2 |  1800/ 2983 batches | lr 5.00 | ms/batch 62.80 | loss  7.32 | ppl  1507.11
| epoch   2 |  2000/ 2983 batches | lr 5.00 | ms/batch 62.51 | loss  7.23 | ppl  1378.90
| epoch   2 |  2200/ 2983 batches | lr 5.00 | ms/batch 61.98 | loss  7.37 | ppl  1587.64
| epoch   2 |  2400/ 2983 batches | lr 5.00 | ms/batch 62.33 | loss  7.34 | ppl  1542.21
| epoch   2 |  2600/ 2983 batches | lr 5.00 | ms/batch 62.43 | loss  7.33 | ppl  1525.53
| epoch   2 |  2800/ 2983 batches | lr 5.00 | ms/batch 63.01 | loss  7.57 | ppl  1942.07
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 195.67s | valid loss  7.04 | valid ppl  1136.80
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 2.2277995420649788e-11
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   28400 /   40000 ( 71.00%) | total_pruned =   11600 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   28320 /   40000 ( 70.80%) | total_pruned =   11680 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   19185 /   40000 ( 47.96%) | total_pruned =   20815 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   27914 /   40000 ( 69.78%) | total_pruned =   12086 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   28017 /   40000 ( 70.04%) | total_pruned =   11983 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   28381 /   40000 ( 70.95%) | total_pruned =   11619 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   28318 /   40000 ( 70.80%) | total_pruned =   11682 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   19200 /   40000 ( 48.00%) | total_pruned =   20800 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   27662 /   40000 ( 69.16%) | total_pruned =   12338 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   27965 /   40000 ( 69.91%) | total_pruned =   12035 | shape = (200, 200)
alive: 263362, pruned : 136638, total: 400000, ( 65.84% remained)
Model avg sparsity: 0.658405
| epoch   3 |   200/ 2983 batches | lr 5.00 | ms/batch 62.77 | loss  7.46 | ppl  1734.51
| epoch   3 |   400/ 2983 batches | lr 5.00 | ms/batch 62.13 | loss  7.38 | ppl  1597.73
| epoch   3 |   600/ 2983 batches | lr 5.00 | ms/batch 62.63 | loss  7.20 | ppl  1334.81
| epoch   3 |   800/ 2983 batches | lr 5.00 | ms/batch 62.66 | loss  7.57 | ppl  1941.97
| epoch   3 |  1000/ 2983 batches | lr 5.00 | ms/batch 62.32 | loss  7.43 | ppl  1690.95
| epoch   3 |  1200/ 2983 batches | lr 5.00 | ms/batch 62.22 | loss  7.59 | ppl  1984.19
| epoch   3 |  1400/ 2983 batches | lr 5.00 | ms/batch 62.75 | loss  7.42 | ppl  1669.37
| epoch   3 |  1600/ 2983 batches | lr 5.00 | ms/batch 62.69 | loss  7.12 | ppl  1237.39
| epoch   3 |  1800/ 2983 batches | lr 5.00 | ms/batch 62.02 | loss  7.31 | ppl  1495.02
| epoch   3 |  2000/ 2983 batches | lr 5.00 | ms/batch 62.34 | loss  7.20 | ppl  1334.78
| epoch   3 |  2200/ 2983 batches | lr 5.00 | ms/batch 62.49 | loss  7.40 | ppl  1635.68
| epoch   3 |  2400/ 2983 batches | lr 5.00 | ms/batch 63.24 | loss  7.33 | ppl  1531.17
| epoch   3 |  2600/ 2983 batches | lr 5.00 | ms/batch 62.22 | loss  7.33 | ppl  1529.88
| epoch   3 |  2800/ 2983 batches | lr 5.00 | ms/batch 62.40 | loss  7.53 | ppl  1861.86
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 196.11s | valid loss  6.99 | valid ppl  1080.51
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 1.2992059854430738e-14
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   24040 /   40000 ( 60.10%) | total_pruned =   15960 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   23925 /   40000 ( 59.81%) | total_pruned =   16075 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   18997 /   40000 ( 47.49%) | total_pruned =   21003 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   23705 /   40000 ( 59.26%) | total_pruned =   16295 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   23826 /   40000 ( 59.56%) | total_pruned =   16174 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   24052 /   40000 ( 60.13%) | total_pruned =   15948 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   23932 /   40000 ( 59.83%) | total_pruned =   16068 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   18981 /   40000 ( 47.45%) | total_pruned =   21019 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   23539 /   40000 ( 58.85%) | total_pruned =   16461 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   23844 /   40000 ( 59.61%) | total_pruned =   16156 | shape = (200, 200)
alive: 228841, pruned : 171159, total: 400000, ( 57.21% remained)
Model avg sparsity: 0.5721025
| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 63.00 | loss  7.47 | ppl  1747.30
| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 62.45 | loss  7.38 | ppl  1610.75
| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 62.38 | loss  7.17 | ppl  1297.57
| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 62.39 | loss  7.53 | ppl  1863.38
| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 62.50 | loss  7.40 | ppl  1634.19
| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 62.88 | loss  7.60 | ppl  1996.60
| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch 62.25 | loss  7.38 | ppl  1602.21
| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch 62.48 | loss  7.14 | ppl  1263.68
| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch 62.43 | loss  7.33 | ppl  1525.89
| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch 63.10 | loss  7.21 | ppl  1350.69
| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch 62.14 | loss  7.39 | ppl  1619.67
| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch 62.73 | loss  7.33 | ppl  1519.72
| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch 62.24 | loss  7.36 | ppl  1568.76
| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch 63.08 | loss  7.54 | ppl  1877.35
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 196.21s | valid loss  6.96 | valid ppl  1056.25
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 6.8057215475989474e-18
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   20202 /   40000 ( 50.51%) | total_pruned =   19798 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   20204 /   40000 ( 50.51%) | total_pruned =   19796 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   18910 /   40000 ( 47.27%) | total_pruned =   21090 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   20041 /   40000 ( 50.10%) | total_pruned =   19959 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   20165 /   40000 ( 50.41%) | total_pruned =   19835 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   20219 /   40000 ( 50.55%) | total_pruned =   19781 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   20198 /   40000 ( 50.49%) | total_pruned =   19802 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   18914 /   40000 ( 47.28%) | total_pruned =   21086 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   20018 /   40000 ( 50.05%) | total_pruned =   19982 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   20190 /   40000 ( 50.48%) | total_pruned =   19810 | shape = (200, 200)
alive: 199061, pruned : 200939, total: 400000, ( 49.77% remained)
Model avg sparsity: 0.4976525
| epoch   5 |   200/ 2983 batches | lr 5.00 | ms/batch 62.62 | loss  7.44 | ppl  1710.63
| epoch   5 |   400/ 2983 batches | lr 5.00 | ms/batch 62.05 | loss  7.40 | ppl  1638.72
| epoch   5 |   600/ 2983 batches | lr 5.00 | ms/batch 60.16 | loss  7.15 | ppl  1278.55
| epoch   5 |   800/ 2983 batches | lr 5.00 | ms/batch 63.03 | loss  7.62 | ppl  2034.45
| epoch   5 |  1000/ 2983 batches | lr 5.00 | ms/batch 62.14 | loss  7.43 | ppl  1682.27
| epoch   5 |  1200/ 2983 batches | lr 5.00 | ms/batch 62.43 | loss  7.57 | ppl  1930.30
| epoch   5 |  1400/ 2983 batches | lr 5.00 | ms/batch 62.18 | loss  7.38 | ppl  1604.76
| epoch   5 |  1600/ 2983 batches | lr 5.00 | ms/batch 62.69 | loss  7.14 | ppl  1260.23
| epoch   5 |  1800/ 2983 batches | lr 5.00 | ms/batch 62.05 | loss  7.33 | ppl  1524.29
| epoch   5 |  2000/ 2983 batches | lr 5.00 | ms/batch 62.26 | loss  7.21 | ppl  1353.17
| epoch   5 |  2200/ 2983 batches | lr 5.00 | ms/batch 62.50 | loss  7.42 | ppl  1669.31
| epoch   5 |  2400/ 2983 batches | lr 5.00 | ms/batch 62.61 | loss  7.33 | ppl  1518.88
| epoch   5 |  2600/ 2983 batches | lr 5.00 | ms/batch 62.38 | loss  7.33 | ppl  1522.02
| epoch   5 |  2800/ 2983 batches | lr 5.00 | ms/batch 62.13 | loss  7.54 | ppl  1876.58
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 195.35s | valid loss  6.98 | valid ppl  1073.27
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 3.3633284875441147e-21
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   16889 /   40000 ( 42.22%) | total_pruned =   23111 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   16994 /   40000 ( 42.48%) | total_pruned =   23006 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   18827 /   40000 ( 47.07%) | total_pruned =   21173 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   16870 /   40000 ( 42.17%) | total_pruned =   23130 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   16898 /   40000 ( 42.24%) | total_pruned =   23102 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   16883 /   40000 ( 42.21%) | total_pruned =   23117 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   16991 /   40000 ( 42.48%) | total_pruned =   23009 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   18846 /   40000 ( 47.12%) | total_pruned =   21154 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   16961 /   40000 ( 42.40%) | total_pruned =   23039 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   16967 /   40000 ( 42.42%) | total_pruned =   23033 | shape = (200, 200)
alive: 173126, pruned : 226874, total: 400000, ( 43.28% remained)
Model avg sparsity: 0.432815
=========================================================================================
| End of training | test loss  6.90 | test ppl   994.40
=========================================================================================
Beginning Sanity Checks:
Sanity Check 1: Weight Reinit
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =   16889 /   40000 ( 42.22%) | total_pruned =   23111 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   16994 /   40000 ( 42.48%) | total_pruned =   23006 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   18827 /   40000 ( 47.07%) | total_pruned =   21173 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   16870 /   40000 ( 42.17%) | total_pruned =   23130 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   16898 /   40000 ( 42.24%) | total_pruned =   23102 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   16883 /   40000 ( 42.21%) | total_pruned =   23117 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   16991 /   40000 ( 42.48%) | total_pruned =   23009 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   18846 /   40000 ( 47.12%) | total_pruned =   21154 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   16961 /   40000 ( 42.40%) | total_pruned =   23039 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   16967 /   40000 ( 42.42%) | total_pruned =   23033 | shape = (200, 200)
alive: 173126, pruned : 226874, total: 400000, ( 43.28% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 55.04 | loss  7.33 | ppl  1526.83
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 54.45 | loss  6.86 | ppl   957.51
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 55.25 | loss  6.43 | ppl   622.47
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 54.52 | loss  6.70 | ppl   811.82
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 54.48 | loss  6.33 | ppl   562.64
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 55.40 | loss  6.37 | ppl   581.31
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 54.59 | loss  6.33 | ppl   560.94
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 54.59 | loss  6.06 | ppl   429.07
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 55.28 | loss  6.04 | ppl   417.94
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 54.72 | loss  5.98 | ppl   397.00
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 54.46 | loss  5.90 | ppl   364.88
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 55.33 | loss  6.02 | ppl   409.84
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 54.18 | loss  5.88 | ppl   356.31
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 54.70 | loss  5.89 | ppl   361.71
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 183.97s | valid loss  5.78 | valid ppl   323.82
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 55.48 | loss  5.66 | ppl   287.76
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 54.32 | loss  5.98 | ppl   393.77
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 55.28 | loss  5.40 | ppl   221.09
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 54.70 | loss  5.83 | ppl   339.88
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 54.49 | loss  5.57 | ppl   261.76
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 55.30 | loss  5.77 | ppl   320.49
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 54.77 | loss  5.80 | ppl   331.15
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 52.09 | loss  5.43 | ppl   227.88
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 54.46 | loss  5.49 | ppl   243.09
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 55.08 | loss  5.41 | ppl   222.70
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 54.41 | loss  5.33 | ppl   207.41
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 54.97 | loss  5.55 | ppl   258.39
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 54.75 | loss  5.55 | ppl   256.97
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 54.66 | loss  5.41 | ppl   223.29
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 183.47s | valid loss  5.57 | valid ppl   262.03
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 55.59 | loss  5.30 | ppl   199.96
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 54.20 | loss  5.59 | ppl   267.19
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 54.43 | loss  5.13 | ppl   169.59
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 54.86 | loss  5.48 | ppl   240.19
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 54.27 | loss  5.27 | ppl   194.84
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 54.00 | loss  5.50 | ppl   245.20
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 54.79 | loss  5.52 | ppl   249.51
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 54.28 | loss  5.09 | ppl   162.95
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 54.26 | loss  5.25 | ppl   191.17
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 54.38 | loss  5.05 | ppl   156.70
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 54.54 | loss  5.07 | ppl   159.94
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 54.14 | loss  5.24 | ppl   189.30
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 54.21 | loss  5.26 | ppl   192.79
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 54.59 | loss  5.07 | ppl   159.76
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 182.66s | valid loss  5.49 | valid ppl   243.32
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 54.29 | loss  4.99 | ppl   146.40
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 53.72 | loss  5.29 | ppl   197.81
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 54.82 | loss  4.80 | ppl   122.03
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 53.57 | loss  5.18 | ppl   178.51
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 54.10 | loss  4.93 | ppl   138.68
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 54.17 | loss  5.13 | ppl   168.72
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 54.30 | loss  5.14 | ppl   170.55
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 53.46 | loss  4.70 | ppl   109.81
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 54.35 | loss  4.86 | ppl   129.23
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 54.07 | loss  4.77 | ppl   118.15
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 54.08 | loss  4.74 | ppl   114.63
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 53.37 | loss  4.82 | ppl   124.54
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 54.64 | loss  4.94 | ppl   139.75
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 53.46 | loss  4.64 | ppl   103.10
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 181.39s | valid loss  5.35 | valid ppl   211.20
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 53.90 | loss  4.82 | ppl   123.49
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 54.47 | loss  5.14 | ppl   171.42
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 53.93 | loss  4.64 | ppl   104.02
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 54.04 | loss  5.14 | ppl   171.36
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 53.62 | loss  4.85 | ppl   127.40
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 54.51 | loss  5.04 | ppl   154.88
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 53.33 | loss  4.99 | ppl   147.02
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 53.96 | loss  4.60 | ppl    99.69
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 54.09 | loss  4.84 | ppl   126.20
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 54.10 | loss  4.66 | ppl   105.93
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 53.47 | loss  4.65 | ppl   104.66
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 54.29 | loss  4.79 | ppl   120.56
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 53.99 | loss  4.86 | ppl   128.84
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 54.07 | loss  4.61 | ppl   100.59
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 181.13s | valid loss  5.34 | valid ppl   207.85
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 54.15 | loss  4.77 | ppl   117.93
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 54.20 | loss  5.08 | ppl   160.20
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 53.89 | loss  4.59 | ppl    98.56
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 53.40 | loss  5.03 | ppl   152.89
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 54.23 | loss  4.71 | ppl   110.59
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 53.93 | loss  4.91 | ppl   135.28
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 54.02 | loss  4.93 | ppl   139.01
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 53.60 | loss  4.49 | ppl    88.73
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 54.64 | loss  4.73 | ppl   112.99
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 53.41 | loss  4.61 | ppl   100.68
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 53.96 | loss  4.56 | ppl    95.84
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 54.02 | loss  4.72 | ppl   111.68
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 53.97 | loss  4.81 | ppl   122.51
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 53.38 | loss  4.47 | ppl    87.65
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 181.03s | valid loss  5.33 | valid ppl   206.76
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.25 | test ppl   190.45
=========================================================================================
Sanity Check 2: Mask Reshuffle
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =   16889 /   40000 ( 42.22%) | total_pruned =   23111 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   16994 /   40000 ( 42.48%) | total_pruned =   23006 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   18827 /   40000 ( 47.07%) | total_pruned =   21173 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   16870 /   40000 ( 42.17%) | total_pruned =   23130 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   16898 /   40000 ( 42.24%) | total_pruned =   23102 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   16883 /   40000 ( 42.21%) | total_pruned =   23117 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   16991 /   40000 ( 42.48%) | total_pruned =   23009 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   18846 /   40000 ( 47.12%) | total_pruned =   21154 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   16961 /   40000 ( 42.40%) | total_pruned =   23039 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   16967 /   40000 ( 42.42%) | total_pruned =   23033 | shape = (200, 200)
alive: 173126, pruned : 226874, total: 400000, ( 43.28% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 54.76 | loss  7.19 | ppl  1321.56
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 53.47 | loss  6.78 | ppl   877.44
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 53.90 | loss  6.42 | ppl   614.88
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 54.00 | loss  6.66 | ppl   779.34
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 54.07 | loss  6.29 | ppl   541.08
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 53.54 | loss  6.49 | ppl   656.81
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 53.79 | loss  6.34 | ppl   568.63
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 54.00 | loss  6.04 | ppl   420.98
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 53.76 | loss  6.10 | ppl   445.86
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 53.43 | loss  5.95 | ppl   383.94
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 54.60 | loss  5.91 | ppl   370.47
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 53.61 | loss  5.97 | ppl   390.47
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 53.90 | loss  5.91 | ppl   370.06
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 53.69 | loss  5.88 | ppl   357.94
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 180.89s | valid loss  5.77 | valid ppl   319.65
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 54.58 | loss  5.69 | ppl   296.57
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 53.60 | loss  5.84 | ppl   342.34
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 53.58 | loss  5.47 | ppl   237.89
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 54.36 | loss  5.84 | ppl   342.70
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 53.53 | loss  5.57 | ppl   263.07
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 53.82 | loss  5.80 | ppl   329.51
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 53.83 | loss  5.78 | ppl   322.50
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 54.34 | loss  5.43 | ppl   227.23
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 53.54 | loss  5.49 | ppl   243.02
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 53.82 | loss  5.29 | ppl   198.09
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 54.34 | loss  5.33 | ppl   206.27
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 53.75 | loss  5.49 | ppl   243.01
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 53.64 | loss  5.57 | ppl   261.99
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 53.90 | loss  5.35 | ppl   211.55
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 180.84s | valid loss  5.59 | valid ppl   267.52
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 54.54 | loss  5.29 | ppl   197.72
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 53.63 | loss  5.59 | ppl   267.10
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 53.76 | loss  5.13 | ppl   168.71
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 54.39 | loss  5.51 | ppl   247.34
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 53.62 | loss  5.19 | ppl   179.62
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 53.63 | loss  5.49 | ppl   241.58
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 54.06 | loss  5.51 | ppl   246.77
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 53.86 | loss  5.04 | ppl   155.17
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 53.60 | loss  5.26 | ppl   191.97
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 53.72 | loss  5.01 | ppl   150.42
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 54.20 | loss  5.06 | ppl   158.23
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 53.62 | loss  5.17 | ppl   175.84
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 53.64 | loss  5.39 | ppl   218.12
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 54.29 | loss  5.12 | ppl   166.75
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 180.83s | valid loss  5.52 | valid ppl   249.72
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 54.37 | loss  5.01 | ppl   150.60
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 53.65 | loss  5.25 | ppl   191.04
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 53.65 | loss  4.80 | ppl   121.45
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 54.17 | loss  5.24 | ppl   188.62
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 53.56 | loss  4.85 | ppl   127.60
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 53.60 | loss  5.19 | ppl   178.95
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 54.29 | loss  5.07 | ppl   159.75
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 53.83 | loss  4.65 | ppl   104.45
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 53.75 | loss  4.88 | ppl   131.54
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 53.68 | loss  4.69 | ppl   108.43
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 51.92 | loss  4.69 | ppl   109.10
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 53.53 | loss  4.86 | ppl   129.60
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 53.63 | loss  4.96 | ppl   142.40
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 53.91 | loss  4.71 | ppl   110.78
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 180.22s | valid loss  5.35 | valid ppl   211.02
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 54.18 | loss  4.89 | ppl   132.40
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 53.90 | loss  5.08 | ppl   161.58
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 53.43 | loss  4.63 | ppl   102.24
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 54.71 | loss  5.12 | ppl   166.79
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 53.43 | loss  4.75 | ppl   116.14
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 53.97 | loss  5.01 | ppl   149.70
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 53.58 | loss  4.99 | ppl   147.33
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 54.48 | loss  4.52 | ppl    92.20
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 53.38 | loss  4.80 | ppl   121.15
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 53.83 | loss  4.65 | ppl   104.26
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 54.01 | loss  4.63 | ppl   102.87
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 50.88 | loss  4.76 | ppl   117.05
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 53.65 | loss  4.91 | ppl   135.18
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 53.97 | loss  4.60 | ppl    99.24
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 180.22s | valid loss  5.34 | valid ppl   209.32
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 54.39 | loss  4.74 | ppl   114.94
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 53.64 | loss  4.99 | ppl   147.40
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 53.74 | loss  4.56 | ppl    95.88
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 54.41 | loss  5.02 | ppl   151.85
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 53.37 | loss  4.66 | ppl   105.30
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 53.48 | loss  5.02 | ppl   151.84
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 54.01 | loss  4.91 | ppl   136.26
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 54.02 | loss  4.45 | ppl    85.83
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 53.66 | loss  4.77 | ppl   117.57
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 53.77 | loss  4.54 | ppl    93.27
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 54.32 | loss  4.54 | ppl    93.70
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 53.71 | loss  4.74 | ppl   113.88
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 53.62 | loss  4.79 | ppl   120.46
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 54.27 | loss  4.55 | ppl    94.51
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 180.78s | valid loss  5.34 | valid ppl   208.65
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.25 | test ppl   191.49
=========================================================================================
