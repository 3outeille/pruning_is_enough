nohup: ignoring input
=> Reading YAML config from configs/hypercube/transformer/transformer_invert.yml
Namespace(algo='hc_iter', alpha=1.0, alpha_prime=1.0, arch='transformer', batch_size=20, bias=False, bn_type='NonAffineBatchNorm', bottom_k_on_forward=False, checkpoint_at_prune=False, chg_mask=False, chg_weight=False, ckpt_interval=-1, config='configs/hypercube/transformer/transformer_invert.yml', conv_type='SubnetConv', data='transformer_data/wikitext-2', data_dir='../data', dataset='MNIST', differentiate_clamp=False, dist_backend='nccl', epochs=6, evaluate=False, evaluate_only=False, fine_tune_lr=5.0, fine_tune_lr_policy='multistep_lr', fine_tune_optimizer='sgd', fine_tune_wd=0.0, first_layer_dense=False, first_layer_type=None, fixed_init=False, flips=None, freeze_weights=True, gamma=0.1, gpu=1, hc_period=1, hc_quantized=True, hc_warmup=9999, hidden_size=500, how_to_connect='prob', how_to_prune='random', imp_no_rewind=False, imp_resume_round=-1, imp_rewind_iter=1000, imp_rewind_model='short_imp/Liu_checkpoint_model_correct.pth', init='signed_constant', interpolate='prob', invert_sanity_check=True, iter_period=1, iter_start=0, label_smoothing=None, lam_finetune_loss=-1, last_layer_dense=False, lmbda=1e-05, load_ckpt=None, log_dir=None, log_interval=2, loss='cross-entropy-loss', lr=5.0, lr_adjust=50, lr_gamma=0.25, lr_policy='multistep_lr', max_iter=100000, metric='loss', milestones=[50, 100, 150, 200], mixed_precision=0, mode='fan_in', mode_connect=False, mode_connect_filename=None, momentum=0.0, multiprocessing_distributed=False, name=None, nesterov=False, no_bn_decay=False, no_cuda=False, noise=True, noise_ratio=0, nonlinearity='relu', num_round=1, num_step_finetune=10, num_test=1, num_trial=1, num_workers=4, optimizer='sgd', override_prune_rate=False, plot_hc_convergence=False, pretrained=None, pretrained2=None, print_freq=10, project_freq=1, prune_rate=0.5, prune_type='BottomK', pruning_strategy=None, quantize_threshold=0.5, random_subnet=False, rank=-1, regularization='L2', reinit=False, results_filename=None, resume=None, rewind_score=False, rewind_to_epoch=-1, round='naive', run_idx=None, save_every=-1, save_model=False, save_plot_data=False, scale_fan=False, score_init='unif', score_init_constant=None, seed=42, seed_fixed_init=24, shift=0.0, shuffle=False, skip_fine_tune=False, skip_sanity_checks=True, smart_ratio=-1, start_epoch=None, start_from_nothing=False, subfolder='transformer_50_invert', submask_size=1, target_sparsity=50.0, td=0.99, temp=100000, trainer='default', transformer_bptt=35, transformer_clip=0.25, transformer_dropout=0.2, transformer_emsize=200, transformer_nhead=2, transformer_nhid=200, transformer_nlayers=2, trial_num=1, unflag_before_finetune=True, warmup_length=0, wd=0.0005, weight_training=False, width=1.0, width_mult=1.0, workers=4, world_size=-1, **{'compare-rounding': False})
Seeded everything: 42
Use GPU: 1 for training
transformer_data/wikitext-2/train.txt
transformer_data/wikitext-2/valid.txt
transformer_data/wikitext-2/test.txt
==> Conv Type: SubnetConv
==> BN Type: NonAffineBatchNorm
Computing prune_rate for target_sparsity 50.0 with iter_period 1
Setting prune_rate to 0.12944943670387588
| epoch   0 |   200/ 2983 batches | lr 5.00 | ms/batch 59.63 | loss 15.97 | ppl 8604938.76
| epoch   0 |   400/ 2983 batches | lr 5.00 | ms/batch 48.43 | loss 10.49 | ppl 36126.24
| epoch   0 |   600/ 2983 batches | lr 5.00 | ms/batch 47.12 | loss  8.33 | ppl  4148.66
| epoch   0 |   800/ 2983 batches | lr 5.00 | ms/batch 47.73 | loss  7.99 | ppl  2954.24
| epoch   0 |  1000/ 2983 batches | lr 5.00 | ms/batch 48.55 | loss  7.61 | ppl  2020.09
| epoch   0 |  1200/ 2983 batches | lr 5.00 | ms/batch 49.05 | loss  7.65 | ppl  2091.16
| epoch   0 |  1400/ 2983 batches | lr 5.00 | ms/batch 49.12 | loss  7.44 | ppl  1704.49
| epoch   0 |  1600/ 2983 batches | lr 5.00 | ms/batch 47.68 | loss  7.15 | ppl  1276.59
| epoch   0 |  1800/ 2983 batches | lr 5.00 | ms/batch 49.49 | loss  7.34 | ppl  1541.30
| epoch   0 |  2000/ 2983 batches | lr 5.00 | ms/batch 49.82 | loss  7.18 | ppl  1318.77
| epoch   0 |  2200/ 2983 batches | lr 5.00 | ms/batch 49.46 | loss  7.40 | ppl  1630.97
| epoch   0 |  2400/ 2983 batches | lr 5.00 | ms/batch 48.56 | loss  7.33 | ppl  1530.59
| epoch   0 |  2600/ 2983 batches | lr 5.00 | ms/batch 49.46 | loss  7.36 | ppl  1568.33
| epoch   0 |  2800/ 2983 batches | lr 5.00 | ms/batch 50.83 | loss  7.61 | ppl  2011.40
-----------------------------------------------------------------------------------------
| end of epoch   0 | time: 155.69s | valid loss  7.01 | valid ppl  1112.44
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.010062603279948235
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   40000 /   40000 (100.00%) | total_pruned =       0 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   40000 /   40000 (100.00%) | total_pruned =       0 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   25703 /   40000 ( 64.26%) | total_pruned =   14297 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   39316 /   40000 ( 98.29%) | total_pruned =     684 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   39314 /   40000 ( 98.28%) | total_pruned =     686 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   40000 /   40000 (100.00%) | total_pruned =       0 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   40000 /   40000 (100.00%) | total_pruned =       0 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   31493 /   40000 ( 78.73%) | total_pruned =    8507 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   38735 /   40000 ( 96.84%) | total_pruned =    1265 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   39113 /   40000 ( 97.78%) | total_pruned =     887 | shape = (200, 200)
alive: 373674, pruned : 26326, total: 400000, ( 93.42% remained)
Model avg sparsity: 0.934185
| epoch   1 |   200/ 2983 batches | lr 5.00 | ms/batch 51.16 | loss  8.51 | ppl  4964.02
| epoch   1 |   400/ 2983 batches | lr 5.00 | ms/batch 49.82 | loss  8.07 | ppl  3203.82
| epoch   1 |   600/ 2983 batches | lr 5.00 | ms/batch 48.46 | loss  7.65 | ppl  2103.10
| epoch   1 |   800/ 2983 batches | lr 5.00 | ms/batch 49.90 | loss  7.91 | ppl  2727.79
| epoch   1 |  1000/ 2983 batches | lr 5.00 | ms/batch 49.88 | loss  7.66 | ppl  2131.32
| epoch   1 |  1200/ 2983 batches | lr 5.00 | ms/batch 51.97 | loss  7.82 | ppl  2494.85
| epoch   1 |  1400/ 2983 batches | lr 5.00 | ms/batch 51.38 | loss  7.59 | ppl  1984.98
| epoch   1 |  1600/ 2983 batches | lr 5.00 | ms/batch 50.03 | loss  7.33 | ppl  1522.70
| epoch   1 |  1800/ 2983 batches | lr 5.00 | ms/batch 50.74 | loss  7.48 | ppl  1765.95
| epoch   1 |  2000/ 2983 batches | lr 5.00 | ms/batch 50.64 | loss  7.33 | ppl  1527.69
| epoch   1 |  2200/ 2983 batches | lr 5.00 | ms/batch 50.85 | loss  7.52 | ppl  1843.42
| epoch   1 |  2400/ 2983 batches | lr 5.00 | ms/batch 49.34 | loss  7.47 | ppl  1757.08
| epoch   1 |  2600/ 2983 batches | lr 5.00 | ms/batch 50.16 | loss  7.46 | ppl  1735.38
| epoch   1 |  2800/ 2983 batches | lr 5.00 | ms/batch 50.88 | loss  7.66 | ppl  2129.82
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 157.98s | valid loss  7.06 | valid ppl  1161.26
-----------------------------------------------------------------------------------------
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   40000 /   40000 (100.00%) | total_pruned =       0 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   40000 /   40000 (100.00%) | total_pruned =       0 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   25703 /   40000 ( 64.26%) | total_pruned =   14297 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   39316 /   40000 ( 98.29%) | total_pruned =     684 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   39314 /   40000 ( 98.28%) | total_pruned =     686 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   40000 /   40000 (100.00%) | total_pruned =       0 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   40000 /   40000 (100.00%) | total_pruned =       0 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   31493 /   40000 ( 78.73%) | total_pruned =    8507 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   38735 /   40000 ( 96.84%) | total_pruned =    1265 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   39113 /   40000 ( 97.78%) | total_pruned =     887 | shape = (200, 200)
alive: 373674, pruned : 26326, total: 400000, ( 93.42% remained)
Model avg sparsity: 0.934185
| epoch   2 |   200/ 2983 batches | lr 5.00 | ms/batch 51.07 | loss  7.56 | ppl  1927.12
| epoch   2 |   400/ 2983 batches | lr 5.00 | ms/batch 51.14 | loss  7.52 | ppl  1838.59
| epoch   2 |   600/ 2983 batches | lr 5.00 | ms/batch 49.50 | loss  7.33 | ppl  1519.25
| epoch   2 |   800/ 2983 batches | lr 5.00 | ms/batch 51.48 | loss  7.67 | ppl  2134.39
| epoch   2 |  1000/ 2983 batches | lr 5.00 | ms/batch 50.70 | loss  7.54 | ppl  1881.07
| epoch   2 |  1200/ 2983 batches | lr 5.00 | ms/batch 50.69 | loss  7.74 | ppl  2288.01
| epoch   2 |  1400/ 2983 batches | lr 5.00 | ms/batch 50.84 | loss  7.50 | ppl  1812.55
| epoch   2 |  1600/ 2983 batches | lr 5.00 | ms/batch 48.94 | loss  7.28 | ppl  1446.38
| epoch   2 |  1800/ 2983 batches | lr 5.00 | ms/batch 50.63 | loss  7.46 | ppl  1739.47
| epoch   2 |  2000/ 2983 batches | lr 5.00 | ms/batch 51.57 | loss  7.34 | ppl  1540.28
| epoch   2 |  2200/ 2983 batches | lr 5.00 | ms/batch 51.62 | loss  7.52 | ppl  1847.03
| epoch   2 |  2400/ 2983 batches | lr 5.00 | ms/batch 50.50 | loss  7.43 | ppl  1681.83
| epoch   2 |  2600/ 2983 batches | lr 5.00 | ms/batch 49.64 | loss  7.47 | ppl  1749.40
| epoch   2 |  2800/ 2983 batches | lr 5.00 | ms/batch 50.89 | loss  7.66 | ppl  2112.86
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 159.45s | valid loss  7.05 | valid ppl  1152.59
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 3.088457845024095e-08
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   40000 /   40000 (100.00%) | total_pruned =       0 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   40000 /   40000 (100.00%) | total_pruned =       0 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   15803 /   40000 ( 39.51%) | total_pruned =   24197 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   39316 /   40000 ( 98.29%) | total_pruned =     684 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   39314 /   40000 ( 98.28%) | total_pruned =     686 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   40000 /   40000 (100.00%) | total_pruned =       0 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   40000 /   40000 (100.00%) | total_pruned =       0 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   23416 /   40000 ( 58.54%) | total_pruned =   16584 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   38735 /   40000 ( 96.84%) | total_pruned =    1265 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   39113 /   40000 ( 97.78%) | total_pruned =     887 | shape = (200, 200)
alive: 355697, pruned : 44303, total: 400000, ( 88.92% remained)
Model avg sparsity: 0.8892425
| epoch   3 |   200/ 2983 batches | lr 5.00 | ms/batch 53.36 | loss  8.46 | ppl  4740.65
| epoch   3 |   400/ 2983 batches | lr 5.00 | ms/batch 50.10 | loss  8.43 | ppl  4561.11
| epoch   3 |   600/ 2983 batches | lr 5.00 | ms/batch 49.64 | loss  8.17 | ppl  3528.46
| epoch   3 |   800/ 2983 batches | lr 5.00 | ms/batch 50.63 | loss  8.38 | ppl  4338.29
| epoch   3 |  1000/ 2983 batches | lr 5.00 | ms/batch 51.59 | loss  8.16 | ppl  3495.43
| epoch   3 |  1200/ 2983 batches | lr 5.00 | ms/batch 51.02 | loss  8.32 | ppl  4090.63
| epoch   3 |  1400/ 2983 batches | lr 5.00 | ms/batch 50.97 | loss  8.03 | ppl  3084.73
| epoch   3 |  1600/ 2983 batches | lr 5.00 | ms/batch 50.36 | loss  7.78 | ppl  2383.53
| epoch   3 |  1800/ 2983 batches | lr 5.00 | ms/batch 50.36 | loss  7.91 | ppl  2728.19
| epoch   3 |  2000/ 2983 batches | lr 5.00 | ms/batch 51.09 | loss  7.77 | ppl  2363.11
| epoch   3 |  2200/ 2983 batches | lr 5.00 | ms/batch 51.24 | loss  7.97 | ppl  2905.34
| epoch   3 |  2400/ 2983 batches | lr 5.00 | ms/batch 51.34 | loss  7.89 | ppl  2674.24
| epoch   3 |  2600/ 2983 batches | lr 5.00 | ms/batch 52.58 | loss  7.91 | ppl  2732.92
| epoch   3 |  2800/ 2983 batches | lr 5.00 | ms/batch 53.29 | loss  8.06 | ppl  3167.45
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 161.54s | valid loss  7.33 | valid ppl  1520.13
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 3.2298186206699414e-14
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   39644 /   40000 ( 99.11%) | total_pruned =     356 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   39715 /   40000 ( 99.29%) | total_pruned =     285 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   12257 /   40000 ( 30.64%) | total_pruned =   27743 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   38524 /   40000 ( 96.31%) | total_pruned =    1476 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   38528 /   40000 ( 96.32%) | total_pruned =    1472 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   39856 /   40000 ( 99.64%) | total_pruned =     144 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   39896 /   40000 ( 99.74%) | total_pruned =     104 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   17198 /   40000 ( 42.99%) | total_pruned =   22802 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   38129 /   40000 ( 95.32%) | total_pruned =    1871 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   38308 /   40000 ( 95.77%) | total_pruned =    1692 | shape = (200, 200)
alive: 342055, pruned : 57945, total: 400000, ( 85.51% remained)
Model avg sparsity: 0.8551375
| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 54.31 | loss  8.41 | ppl  4483.67
| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 53.38 | loss  8.41 | ppl  4513.05
| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 51.72 | loss  8.18 | ppl  3570.50
| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 53.71 | loss  8.44 | ppl  4639.46
| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 51.08 | loss  8.33 | ppl  4154.79
| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 50.76 | loss  8.55 | ppl  5144.32
| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch 50.51 | loss  8.36 | ppl  4255.66
| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch 49.14 | loss  8.13 | ppl  3397.24
| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch 49.89 | loss  8.29 | ppl  3970.00
| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch 52.36 | loss  8.13 | ppl  3405.64
| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch 53.99 | loss  8.22 | ppl  3699.33
| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch 49.48 | loss  8.11 | ppl  3342.49
| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch 49.59 | loss  8.19 | ppl  3597.57
| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch 52.72 | loss  8.28 | ppl  3941.12
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 162.46s | valid loss  7.60 | valid ppl  1998.81
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 1.3047090850477466e-17
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   38122 /   40000 ( 95.31%) | total_pruned =    1878 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   38126 /   40000 ( 95.31%) | total_pruned =    1874 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   11184 /   40000 ( 27.96%) | total_pruned =   28816 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   37227 /   40000 ( 93.07%) | total_pruned =    2773 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   37197 /   40000 ( 92.99%) | total_pruned =    2803 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   38132 /   40000 ( 95.33%) | total_pruned =    1868 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   38173 /   40000 ( 95.43%) | total_pruned =    1827 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   16533 /   40000 ( 41.33%) | total_pruned =   23467 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   36858 /   40000 ( 92.14%) | total_pruned =    3142 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   36939 /   40000 ( 92.35%) | total_pruned =    3061 | shape = (200, 200)
alive: 328491, pruned : 71509, total: 400000, ( 82.12% remained)
Model avg sparsity: 0.8212275
| epoch   5 |   200/ 2983 batches | lr 5.00 | ms/batch 52.66 | loss  8.41 | ppl  4484.88
| epoch   5 |   400/ 2983 batches | lr 5.00 | ms/batch 51.98 | loss  8.42 | ppl  4518.43
| epoch   5 |   600/ 2983 batches | lr 5.00 | ms/batch 51.43 | loss  8.14 | ppl  3427.18
| epoch   5 |   800/ 2983 batches | lr 5.00 | ms/batch 51.07 | loss  8.47 | ppl  4766.77
| epoch   5 |  1000/ 2983 batches | lr 5.00 | ms/batch 55.83 | loss  8.33 | ppl  4145.23
| epoch   5 |  1200/ 2983 batches | lr 5.00 | ms/batch 55.38 | loss  8.54 | ppl  5126.43
| epoch   5 |  1400/ 2983 batches | lr 5.00 | ms/batch 53.86 | loss  8.36 | ppl  4285.33
| epoch   5 |  1600/ 2983 batches | lr 5.00 | ms/batch 51.63 | loss  8.10 | ppl  3296.64
| epoch   5 |  1800/ 2983 batches | lr 5.00 | ms/batch 54.49 | loss  8.25 | ppl  3826.01
| epoch   5 |  2000/ 2983 batches | lr 5.00 | ms/batch 55.29 | loss  8.10 | ppl  3278.50
| epoch   5 |  2200/ 2983 batches | lr 5.00 | ms/batch 53.43 | loss  8.33 | ppl  4158.26
| epoch   5 |  2400/ 2983 batches | lr 5.00 | ms/batch 52.72 | loss  8.27 | ppl  3888.22
| epoch   5 |  2600/ 2983 batches | lr 5.00 | ms/batch 55.16 | loss  8.30 | ppl  4040.99
| epoch   5 |  2800/ 2983 batches | lr 5.00 | ms/batch 55.43 | loss  8.36 | ppl  4275.38
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 171.55s | valid loss  7.82 | valid ppl  2481.81
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 5.295115104107945e-21
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   36542 /   40000 ( 91.36%) | total_pruned =    3458 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   36552 /   40000 ( 91.38%) | total_pruned =    3448 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    9247 /   40000 ( 23.12%) | total_pruned =   30753 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   35853 /   40000 ( 89.63%) | total_pruned =    4147 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   35830 /   40000 ( 89.58%) | total_pruned =    4170 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   36568 /   40000 ( 91.42%) | total_pruned =    3432 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   36611 /   40000 ( 91.53%) | total_pruned =    3389 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   15985 /   40000 ( 39.96%) | total_pruned =   24015 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   35497 /   40000 ( 88.74%) | total_pruned =    4503 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   35578 /   40000 ( 88.94%) | total_pruned =    4422 | shape = (200, 200)
alive: 314263, pruned : 85737, total: 400000, ( 78.57% remained)
Model avg sparsity: 0.7856575
=========================================================================================
| End of training | test loss  7.85 | test ppl  2566.30
=========================================================================================
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =   36542 /   40000 ( 91.36%) | total_pruned =    3458 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   36552 /   40000 ( 91.38%) | total_pruned =    3448 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    9247 /   40000 ( 23.12%) | total_pruned =   30753 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   35853 /   40000 ( 89.63%) | total_pruned =    4147 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   35830 /   40000 ( 89.58%) | total_pruned =    4170 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   36568 /   40000 ( 91.42%) | total_pruned =    3432 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   36611 /   40000 ( 91.53%) | total_pruned =    3389 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   15985 /   40000 ( 39.96%) | total_pruned =   24015 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   35497 /   40000 ( 88.74%) | total_pruned =    4503 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   35578 /   40000 ( 88.94%) | total_pruned =    4422 | shape = (200, 200)
alive: 314263, pruned : 85737, total: 400000, ( 78.57% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 54.71 | loss  7.55 | ppl  1892.81
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 53.90 | loss  6.95 | ppl  1046.54
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 56.29 | loss  6.49 | ppl   658.71
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 56.30 | loss  6.80 | ppl   895.30
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 55.92 | loss  6.35 | ppl   570.05
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 52.74 | loss  6.69 | ppl   804.04
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 53.33 | loss  6.57 | ppl   714.16
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 56.03 | loss  6.06 | ppl   429.18
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 51.48 | loss  6.12 | ppl   454.20
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 48.63 | loss  6.04 | ppl   418.84
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 49.85 | loss  5.99 | ppl   400.50
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 52.19 | loss  6.21 | ppl   497.87
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 52.25 | loss  6.06 | ppl   426.43
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 53.96 | loss  6.00 | ppl   401.82
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 177.69s | valid loss  5.89 | valid ppl   362.95
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 52.78 | loss  5.89 | ppl   359.76
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 51.48 | loss  6.11 | ppl   450.01
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 53.74 | loss  5.53 | ppl   252.20
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 54.58 | loss  6.02 | ppl   411.81
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 54.99 | loss  5.68 | ppl   292.79
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 51.46 | loss  5.92 | ppl   370.73
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 50.40 | loss  5.94 | ppl   378.89
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 54.62 | loss  5.55 | ppl   257.15
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 54.27 | loss  5.67 | ppl   289.79
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 51.04 | loss  5.52 | ppl   248.64
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 51.31 | loss  5.46 | ppl   235.10
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 54.65 | loss  5.64 | ppl   281.67
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 54.65 | loss  5.70 | ppl   299.31
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 56.27 | loss  5.49 | ppl   243.32
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 179.32s | valid loss  5.63 | valid ppl   279.15
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 52.18 | loss  5.41 | ppl   223.63
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 47.40 | loss  5.74 | ppl   310.65
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 47.18 | loss  5.19 | ppl   178.91
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 46.10 | loss  5.67 | ppl   288.80
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 45.61 | loss  5.27 | ppl   194.07
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 44.10 | loss  5.60 | ppl   269.53
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 43.17 | loss  5.65 | ppl   284.39
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 43.97 | loss  5.23 | ppl   187.14
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 44.32 | loss  5.34 | ppl   208.64
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 42.69 | loss  5.22 | ppl   184.66
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 41.42 | loss  5.13 | ppl   168.83
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 41.23 | loss  5.34 | ppl   208.99
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 33.92 | loss  5.45 | ppl   232.98
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 34.08 | loss  5.18 | ppl   178.03
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 140.04s | valid loss  5.52 | valid ppl   249.50
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 33.30 | loss  5.09 | ppl   161.99
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 32.54 | loss  5.35 | ppl   209.99
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 32.75 | loss  4.86 | ppl   129.38
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 33.60 | loss  5.37 | ppl   214.50
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 33.42 | loss  4.96 | ppl   142.12
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 31.98 | loss  5.25 | ppl   189.97
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 31.77 | loss  5.21 | ppl   183.84
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 32.02 | loss  4.79 | ppl   120.44
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 33.66 | loss  4.95 | ppl   140.96
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 33.33 | loss  4.79 | ppl   120.64
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 33.25 | loss  4.82 | ppl   124.01
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 33.13 | loss  4.98 | ppl   146.01
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 32.94 | loss  5.04 | ppl   154.47
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 32.95 | loss  4.78 | ppl   119.68
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 110.16s | valid loss  5.35 | valid ppl   211.41
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 32.23 | loss  5.00 | ppl   147.69
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 31.39 | loss  5.19 | ppl   179.73
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 31.90 | loss  4.70 | ppl   110.30
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 32.84 | loss  5.27 | ppl   194.07
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 32.93 | loss  4.88 | ppl   132.00
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 31.56 | loss  5.18 | ppl   178.52
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 30.99 | loss  5.10 | ppl   164.27
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 30.74 | loss  4.65 | ppl   104.25
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 26.36 | loss  4.88 | ppl   131.95
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 24.27 | loss  4.68 | ppl   107.37
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 24.29 | loss  4.81 | ppl   122.14
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 24.09 | loss  4.88 | ppl   131.76
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 24.31 | loss  4.96 | ppl   142.77
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 24.26 | loss  4.70 | ppl   110.04
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 93.73s | valid loss  5.33 | valid ppl   205.94
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 23.37 | loss  4.84 | ppl   126.45
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 21.99 | loss  5.08 | ppl   161.53
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 21.16 | loss  4.67 | ppl   106.72
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 17.68 | loss  5.14 | ppl   171.35
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 15.05 | loss  4.73 | ppl   112.89
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 15.50 | loss  5.08 | ppl   160.00
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 14.93 | loss  5.06 | ppl   158.17
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 15.37 | loss  4.58 | ppl    97.30
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 14.84 | loss  4.81 | ppl   122.44
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 15.15 | loss  4.66 | ppl   105.71
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 14.85 | loss  4.70 | ppl   109.41
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 15.06 | loss  4.85 | ppl   127.42
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 14.75 | loss  4.94 | ppl   140.42
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 15.79 | loss  4.64 | ppl   103.31
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 56.44s | valid loss  5.31 | valid ppl   203.21
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.22 | test ppl   185.63
=========================================================================================
