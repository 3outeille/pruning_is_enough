nohup: ignoring input
=> Reading YAML config from configs/hypercube/transformer/transformer_base.yml
Namespace(algo='hc_iter', alpha=1.0, alpha_prime=1.0, arch='transformer', batch_size=20, bias=False, bn_type='NonAffineBatchNorm', bottom_k_on_forward=False, checkpoint_at_prune=False, chg_mask=False, chg_weight=False, ckpt_interval=-1, config='configs/hypercube/transformer/transformer_base.yml', conv_type='SubnetConv', data='transformer_data/wikitext-2', data_dir='../data', dataset='MNIST', differentiate_clamp=False, dist_backend='nccl', epochs=6, evaluate=False, evaluate_only=False, fine_tune_lr=5.0, fine_tune_lr_policy='multistep_lr', fine_tune_optimizer='sgd', fine_tune_wd=0.0, first_layer_dense=False, first_layer_type=None, fixed_init=False, flips=None, freeze_weights=True, gamma=0.1, gpu=0, hc_period=1, hc_quantized=True, hc_warmup=9999, hidden_size=500, how_to_connect='prob', how_to_prune='random', imp_no_rewind=False, imp_resume_round=-1, imp_rewind_iter=1000, imp_rewind_model='short_imp/Liu_checkpoint_model_correct.pth', init='signed_constant', interpolate='prob', invert_sanity_check=False, iter_period=1, iter_start=0, label_smoothing=None, lam_finetune_loss=-1, last_layer_dense=False, lmbda=1e-05, load_ckpt=None, log_dir=None, log_interval=2, loss='cross-entropy-loss', lr=5.0, lr_adjust=50, lr_gamma=0.25, lr_policy='multistep_lr', max_iter=100000, metric='loss', milestones=[50, 100, 150, 200], mixed_precision=0, mode='fan_in', mode_connect=False, mode_connect_filename=None, momentum=0.0, multiprocessing_distributed=False, name=None, nesterov=False, no_bn_decay=False, no_cuda=False, noise=True, noise_ratio=0, nonlinearity='relu', num_round=1, num_step_finetune=10, num_test=1, num_trial=1, num_workers=4, optimizer='sgd', override_prune_rate=False, plot_hc_convergence=False, pretrained=None, pretrained2=None, print_freq=10, project_freq=1, prune_rate=0.5, prune_type='BottomK', pruning_strategy=None, quantize_threshold=0.5, random_subnet=False, rank=-1, regularization='L2', reinit=False, results_filename=None, resume=None, rewind_score=False, rewind_to_epoch=-1, round='naive', run_idx=None, save_every=-1, save_model=False, save_plot_data=False, scale_fan=False, score_init='unif', score_init_constant=None, seed=42, seed_fixed_init=24, shift=0.0, shuffle=False, skip_fine_tune=True, skip_sanity_checks=False, smart_ratio=-1, start_epoch=None, start_from_nothing=False, subfolder='transformer_1_8_sanity', submask_size=1, target_sparsity=1.8, td=0.99, temp=100000, trainer='default', transformer_bptt=35, transformer_clip=0.25, transformer_dropout=0.2, transformer_emsize=200, transformer_nhead=2, transformer_nhid=200, transformer_nlayers=2, trial_num=1, unflag_before_finetune=True, warmup_length=0, wd=0.0005, weight_training=False, width=1.0, width_mult=1.0, workers=4, world_size=-1, **{'compare-rounding': False})
Seeded everything: 42
Use GPU: 0 for training
transformer_data/wikitext-2/train.txt
transformer_data/wikitext-2/valid.txt
transformer_data/wikitext-2/test.txt
==> Conv Type: SubnetConv
==> BN Type: NonAffineBatchNorm
Computing prune_rate for target_sparsity 1.8 with iter_period 1
Setting prune_rate to 0.5522305073059569
| epoch   0 |   200/ 2983 batches | lr 5.00 | ms/batch 18.42 | loss 15.97 | ppl 8604938.76
| epoch   0 |   400/ 2983 batches | lr 5.00 | ms/batch 14.90 | loss 10.49 | ppl 36126.24
| epoch   0 |   600/ 2983 batches | lr 5.00 | ms/batch 15.13 | loss  8.33 | ppl  4148.66
| epoch   0 |   800/ 2983 batches | lr 5.00 | ms/batch 14.90 | loss  7.99 | ppl  2954.24
| epoch   0 |  1000/ 2983 batches | lr 5.00 | ms/batch 14.91 | loss  7.61 | ppl  2020.09
| epoch   0 |  1200/ 2983 batches | lr 5.00 | ms/batch 15.37 | loss  7.65 | ppl  2091.16
| epoch   0 |  1400/ 2983 batches | lr 5.00 | ms/batch 20.20 | loss  7.44 | ppl  1704.49
| epoch   0 |  1600/ 2983 batches | lr 5.00 | ms/batch 21.85 | loss  7.15 | ppl  1276.59
| epoch   0 |  1800/ 2983 batches | lr 5.00 | ms/batch 22.38 | loss  7.34 | ppl  1541.30
| epoch   0 |  2000/ 2983 batches | lr 5.00 | ms/batch 22.60 | loss  7.18 | ppl  1318.77
| epoch   0 |  2200/ 2983 batches | lr 5.00 | ms/batch 29.95 | loss  7.40 | ppl  1630.97
| epoch   0 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.16 | loss  7.33 | ppl  1530.59
| epoch   0 |  2600/ 2983 batches | lr 5.00 | ms/batch 29.99 | loss  7.36 | ppl  1568.33
| epoch   0 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.71 | loss  7.61 | ppl  2011.40
-----------------------------------------------------------------------------------------
| end of epoch   0 | time: 69.88s | valid loss  7.01 | valid ppl  1112.44
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.0003001487348228693
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   12013 /   40000 ( 30.03%) | total_pruned =   27987 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   12023 /   40000 ( 30.06%) | total_pruned =   27977 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   20173 /   40000 ( 50.43%) | total_pruned =   19827 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   12170 /   40000 ( 30.43%) | total_pruned =   27830 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   12273 /   40000 ( 30.68%) | total_pruned =   27727 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   11958 /   40000 ( 29.89%) | total_pruned =   28042 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   11961 /   40000 ( 29.90%) | total_pruned =   28039 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   19908 /   40000 ( 49.77%) | total_pruned =   20092 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   12344 /   40000 ( 30.86%) | total_pruned =   27656 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   12469 /   40000 ( 31.17%) | total_pruned =   27531 | shape = (200, 200)
alive: 137292, pruned : 262708, total: 400000, ( 34.32% remained)
Model avg sparsity: 0.34323
| epoch   1 |   200/ 2983 batches | lr 5.00 | ms/batch 30.72 | loss  7.43 | ppl  1684.50
| epoch   1 |   400/ 2983 batches | lr 5.00 | ms/batch 30.22 | loss  7.39 | ppl  1625.87
| epoch   1 |   600/ 2983 batches | lr 5.00 | ms/batch 30.20 | loss  7.20 | ppl  1344.18
| epoch   1 |   800/ 2983 batches | lr 5.00 | ms/batch 30.27 | loss  7.61 | ppl  2014.45
| epoch   1 |  1000/ 2983 batches | lr 5.00 | ms/batch 29.91 | loss  7.44 | ppl  1706.91
| epoch   1 |  1200/ 2983 batches | lr 5.00 | ms/batch 30.92 | loss  7.56 | ppl  1924.62
| epoch   1 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.57 | loss  7.45 | ppl  1713.72
| epoch   1 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.13 | loss  7.16 | ppl  1281.70
| epoch   1 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.96 | loss  7.28 | ppl  1452.54
| epoch   1 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.46 | loss  7.21 | ppl  1359.39
| epoch   1 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.29 | loss  7.40 | ppl  1632.09
| epoch   1 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.29 | loss  7.36 | ppl  1574.98
| epoch   1 |  2600/ 2983 batches | lr 5.00 | ms/batch 30.54 | loss  7.33 | ppl  1519.18
| epoch   1 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.48 | loss  7.56 | ppl  1919.21
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 94.70s | valid loss  7.00 | valid ppl  1099.30
-----------------------------------------------------------------------------------------
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   12013 /   40000 ( 30.03%) | total_pruned =   27987 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   12023 /   40000 ( 30.06%) | total_pruned =   27977 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   20173 /   40000 ( 50.43%) | total_pruned =   19827 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   12170 /   40000 ( 30.43%) | total_pruned =   27830 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   12273 /   40000 ( 30.68%) | total_pruned =   27727 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   11958 /   40000 ( 29.89%) | total_pruned =   28042 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   11961 /   40000 ( 29.90%) | total_pruned =   28039 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   19908 /   40000 ( 49.77%) | total_pruned =   20092 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   12344 /   40000 ( 30.86%) | total_pruned =   27656 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   12469 /   40000 ( 31.17%) | total_pruned =   27531 | shape = (200, 200)
alive: 137292, pruned : 262708, total: 400000, ( 34.32% remained)
Model avg sparsity: 0.34323
| epoch   2 |   200/ 2983 batches | lr 5.00 | ms/batch 30.93 | loss  7.48 | ppl  1771.78
| epoch   2 |   400/ 2983 batches | lr 5.00 | ms/batch 30.59 | loss  7.39 | ppl  1613.43
| epoch   2 |   600/ 2983 batches | lr 5.00 | ms/batch 30.35 | loss  7.24 | ppl  1393.02
| epoch   2 |   800/ 2983 batches | lr 5.00 | ms/batch 30.18 | loss  7.53 | ppl  1870.80
| epoch   2 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.35 | loss  7.43 | ppl  1685.10
| epoch   2 |  1200/ 2983 batches | lr 5.00 | ms/batch 30.14 | loss  7.58 | ppl  1966.85
| epoch   2 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.78 | loss  7.40 | ppl  1636.28
| epoch   2 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.49 | loss  7.12 | ppl  1236.45
| epoch   2 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.71 | loss  7.32 | ppl  1507.27
| epoch   2 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.09 | loss  7.21 | ppl  1349.20
| epoch   2 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.12 | loss  7.37 | ppl  1585.28
| epoch   2 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.16 | loss  7.34 | ppl  1540.67
| epoch   2 |  2600/ 2983 batches | lr 5.00 | ms/batch 30.32 | loss  7.35 | ppl  1549.91
| epoch   2 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.44 | loss  7.57 | ppl  1939.50
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 94.68s | valid loss  6.99 | valid ppl  1082.69
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 7.240531091357028e-11
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =    2134 /   40000 (  5.33%) | total_pruned =   37866 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    2177 /   40000 (  5.44%) | total_pruned =   37823 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   19193 /   40000 ( 47.98%) | total_pruned =   20807 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    2995 /   40000 (  7.49%) | total_pruned =   37005 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    3031 /   40000 (  7.58%) | total_pruned =   36969 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    2161 /   40000 (  5.40%) | total_pruned =   37839 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    2162 /   40000 (  5.41%) | total_pruned =   37838 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   19055 /   40000 ( 47.64%) | total_pruned =   20945 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    3385 /   40000 (  8.46%) | total_pruned =   36615 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    3287 /   40000 (  8.22%) | total_pruned =   36713 | shape = (200, 200)
alive: 59580, pruned : 340420, total: 400000, ( 14.89% remained)
Model avg sparsity: 0.14895
| epoch   3 |   200/ 2983 batches | lr 5.00 | ms/batch 30.51 | loss  7.46 | ppl  1735.48
| epoch   3 |   400/ 2983 batches | lr 5.00 | ms/batch 30.47 | loss  7.39 | ppl  1612.80
| epoch   3 |   600/ 2983 batches | lr 5.00 | ms/batch 30.40 | loss  7.18 | ppl  1317.09
| epoch   3 |   800/ 2983 batches | lr 5.00 | ms/batch 30.48 | loss  7.57 | ppl  1939.79
| epoch   3 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.48 | loss  7.43 | ppl  1693.76
| epoch   3 |  1200/ 2983 batches | lr 5.00 | ms/batch 30.60 | loss  7.59 | ppl  1974.60
| epoch   3 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.87 | loss  7.42 | ppl  1667.11
| epoch   3 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.35 | loss  7.13 | ppl  1245.96
| epoch   3 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.72 | loss  7.32 | ppl  1509.95
| epoch   3 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.23 | loss  7.22 | ppl  1373.33
| epoch   3 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.43 | loss  7.39 | ppl  1617.99
| epoch   3 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.38 | loss  7.33 | ppl  1529.07
| epoch   3 |  2600/ 2983 batches | lr 5.00 | ms/batch 30.21 | loss  7.33 | ppl  1530.35
| epoch   3 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.60 | loss  7.55 | ppl  1906.94
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 94.88s | valid loss  7.00 | valid ppl  1096.67
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.007824445143342018
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   14698 /   40000 ( 36.74%) | total_pruned =   25302 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     682 /   40000 (  1.71%) | total_pruned =   39318 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     665 /   40000 (  1.66%) | total_pruned =   39335 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =    8505 /   40000 ( 21.26%) | total_pruned =   31495 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    1266 /   40000 (  3.17%) | total_pruned =   38734 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     826 /   40000 (  2.06%) | total_pruned =   39174 | shape = (200, 200)
alive: 26642, pruned : 373358, total: 400000, (  6.66% remained)
Model avg sparsity: 0.066605
| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 30.60 | loss  7.44 | ppl  1698.82
| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 30.54 | loss  7.34 | ppl  1537.63
| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 30.11 | loss  7.16 | ppl  1291.04
| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 30.29 | loss  7.53 | ppl  1865.22
| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.47 | loss  7.41 | ppl  1652.86
| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 30.66 | loss  7.58 | ppl  1965.65
| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.24 | loss  7.37 | ppl  1593.46
| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.34 | loss  7.14 | ppl  1264.16
| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.05 | loss  7.33 | ppl  1522.90
| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.39 | loss  7.21 | ppl  1357.53
| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.02 | loss  7.36 | ppl  1576.05
| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.23 | loss  7.33 | ppl  1523.48
| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch 30.24 | loss  7.33 | ppl  1525.69
| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.00 | loss  7.54 | ppl  1889.47
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 94.18s | valid loss  6.98 | valid ppl  1070.84
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.03462313488125801
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    5186 /   40000 ( 12.96%) | total_pruned =   34814 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     668 /   40000 (  1.67%) | total_pruned =   39332 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     566 /   40000 (  1.42%) | total_pruned =   39434 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =    3595 /   40000 (  8.99%) | total_pruned =   36405 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    1115 /   40000 (  2.79%) | total_pruned =   38885 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     781 /   40000 (  1.95%) | total_pruned =   39219 | shape = (200, 200)
alive: 11911, pruned : 388089, total: 400000, (  2.98% remained)
Model avg sparsity: 0.0297775
| epoch   5 |   200/ 2983 batches | lr 5.00 | ms/batch 30.19 | loss  7.46 | ppl  1739.46
| epoch   5 |   400/ 2983 batches | lr 5.00 | ms/batch 30.12 | loss  7.41 | ppl  1652.07
| epoch   5 |   600/ 2983 batches | lr 5.00 | ms/batch 30.00 | loss  7.15 | ppl  1269.33
| epoch   5 |   800/ 2983 batches | lr 5.00 | ms/batch 30.57 | loss  7.59 | ppl  1972.75
| epoch   5 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.61 | loss  7.43 | ppl  1692.12
| epoch   5 |  1200/ 2983 batches | lr 5.00 | ms/batch 29.91 | loss  7.58 | ppl  1964.99
| epoch   5 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.07 | loss  7.36 | ppl  1576.52
| epoch   5 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.15 | loss  7.13 | ppl  1252.55
| epoch   5 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.26 | loss  7.33 | ppl  1520.58
| epoch   5 |  2000/ 2983 batches | lr 5.00 | ms/batch 29.42 | loss  7.20 | ppl  1335.48
| epoch   5 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.48 | loss  7.42 | ppl  1671.30
| epoch   5 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.07 | loss  7.31 | ppl  1499.43
| epoch   5 |  2600/ 2983 batches | lr 5.00 | ms/batch 29.85 | loss  7.33 | ppl  1524.70
| epoch   5 |  2800/ 2983 batches | lr 5.00 | ms/batch 29.99 | loss  7.52 | ppl  1839.83
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 93.75s | valid loss  6.99 | valid ppl  1089.41
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.11201347410678864
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    2029 /   40000 (  5.07%) | total_pruned =   37971 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     593 /   40000 (  1.48%) | total_pruned =   39407 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     276 /   40000 (  0.69%) | total_pruned =   39724 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =    1259 /   40000 (  3.15%) | total_pruned =   38741 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =     547 /   40000 (  1.37%) | total_pruned =   39453 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     629 /   40000 (  1.57%) | total_pruned =   39371 | shape = (200, 200)
alive: 5333, pruned : 394667, total: 400000, (  1.33% remained)
Model avg sparsity: 0.0133325
=========================================================================================
| End of training | test loss  6.92 | test ppl  1012.65
=========================================================================================
Beginning Sanity Checks:
Sanity Check 1: Weight Reinit
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    2029 /   40000 (  5.07%) | total_pruned =   37971 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     593 /   40000 (  1.48%) | total_pruned =   39407 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     276 /   40000 (  0.69%) | total_pruned =   39724 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =    1259 /   40000 (  3.15%) | total_pruned =   38741 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =     547 /   40000 (  1.37%) | total_pruned =   39453 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     629 /   40000 (  1.57%) | total_pruned =   39371 | shape = (200, 200)
alive: 5333, pruned : 394667, total: 400000, (  1.33% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 27.64 | loss  7.02 | ppl  1116.37
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 27.56 | loss  6.85 | ppl   948.61
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 27.31 | loss  6.39 | ppl   595.16
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 28.03 | loss  6.62 | ppl   752.09
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 28.15 | loss  6.40 | ppl   604.85
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 28.16 | loss  6.51 | ppl   672.86
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 28.25 | loss  6.47 | ppl   648.54
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 27.52 | loss  6.06 | ppl   429.90
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 27.50 | loss  6.07 | ppl   431.42
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 26.40 | loss  6.03 | ppl   414.39
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 27.03 | loss  6.06 | ppl   430.16
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 27.07 | loss  6.18 | ppl   481.93
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 27.07 | loss  6.03 | ppl   415.94
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 26.99 | loss  6.15 | ppl   468.90
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 90.70s | valid loss  5.90 | valid ppl   366.69
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 27.24 | loss  5.80 | ppl   329.85
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 27.03 | loss  6.02 | ppl   410.47
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 27.00 | loss  5.57 | ppl   263.64
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 27.17 | loss  6.03 | ppl   414.33
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 26.95 | loss  5.80 | ppl   331.44
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 27.40 | loss  5.95 | ppl   383.94
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 26.27 | loss  5.94 | ppl   378.90
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 26.98 | loss  5.64 | ppl   280.15
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 27.08 | loss  5.75 | ppl   313.66
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 26.82 | loss  5.65 | ppl   283.89
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 26.06 | loss  5.57 | ppl   261.25
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 27.05 | loss  5.78 | ppl   323.52
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 27.07 | loss  5.77 | ppl   319.50
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 27.04 | loss  5.75 | ppl   314.49
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 89.27s | valid loss  5.80 | valid ppl   329.69
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 27.10 | loss  5.50 | ppl   245.61
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 27.01 | loss  5.73 | ppl   309.00
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 27.16 | loss  5.34 | ppl   209.21
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 27.08 | loss  5.71 | ppl   302.18
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 27.13 | loss  5.51 | ppl   247.35
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 27.15 | loss  5.72 | ppl   303.76
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 26.22 | loss  5.78 | ppl   323.78
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 26.87 | loss  5.52 | ppl   249.24
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 26.88 | loss  5.54 | ppl   255.51
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 27.19 | loss  5.43 | ppl   229.26
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 26.04 | loss  5.32 | ppl   205.36
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 27.02 | loss  5.60 | ppl   270.71
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 27.14 | loss  5.49 | ppl   241.26
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 27.04 | loss  5.54 | ppl   255.40
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 88.95s | valid loss  5.75 | valid ppl   313.73
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 27.28 | loss  5.24 | ppl   188.70
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 27.20 | loss  5.48 | ppl   240.13
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 27.25 | loss  5.09 | ppl   162.40
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 27.06 | loss  5.49 | ppl   243.25
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 27.32 | loss  5.24 | ppl   189.09
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 27.17 | loss  5.40 | ppl   221.11
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 26.12 | loss  5.41 | ppl   223.98
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 27.01 | loss  5.11 | ppl   165.30
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 26.94 | loss  5.14 | ppl   171.18
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 26.97 | loss  5.06 | ppl   157.53
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 26.15 | loss  5.06 | ppl   157.26
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 27.05 | loss  5.26 | ppl   193.35
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 26.95 | loss  5.20 | ppl   182.08
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 27.13 | loss  5.17 | ppl   175.11
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 89.27s | valid loss  5.53 | valid ppl   250.96
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 27.18 | loss  5.14 | ppl   171.23
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 27.05 | loss  5.35 | ppl   210.10
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 26.97 | loss  4.99 | ppl   146.62
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 26.99 | loss  5.42 | ppl   226.23
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 27.09 | loss  5.19 | ppl   179.71
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 27.14 | loss  5.32 | ppl   203.87
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 26.46 | loss  5.33 | ppl   206.88
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 27.00 | loss  5.04 | ppl   153.96
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 27.04 | loss  5.12 | ppl   167.35
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 27.01 | loss  5.02 | ppl   151.43
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 25.81 | loss  4.99 | ppl   146.80
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 26.97 | loss  5.22 | ppl   184.50
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 27.07 | loss  5.17 | ppl   176.02
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 27.02 | loss  5.11 | ppl   164.96
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 88.99s | valid loss  5.51 | valid ppl   247.82
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 27.22 | loss  5.09 | ppl   161.67
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 27.21 | loss  5.32 | ppl   204.21
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 27.20 | loss  4.88 | ppl   130.99
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 26.99 | loss  5.38 | ppl   217.50
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 27.43 | loss  5.05 | ppl   155.32
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 26.88 | loss  5.26 | ppl   191.97
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 26.64 | loss  5.32 | ppl   203.91
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 27.22 | loss  4.94 | ppl   139.29
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 27.09 | loss  5.06 | ppl   157.76
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 27.02 | loss  4.94 | ppl   139.56
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 26.45 | loss  4.95 | ppl   141.68
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 27.00 | loss  5.11 | ppl   165.11
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 26.95 | loss  5.12 | ppl   167.03
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 26.97 | loss  5.06 | ppl   158.24
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 89.29s | valid loss  5.51 | valid ppl   247.29
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.41 | test ppl   224.26
=========================================================================================
Sanity Check 2: Mask Reshuffle
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    2029 /   40000 (  5.07%) | total_pruned =   37971 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     593 /   40000 (  1.48%) | total_pruned =   39407 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     276 /   40000 (  0.69%) | total_pruned =   39724 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =    1259 /   40000 (  3.15%) | total_pruned =   38741 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =     547 /   40000 (  1.37%) | total_pruned =   39453 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     629 /   40000 (  1.57%) | total_pruned =   39371 | shape = (200, 200)
alive: 5333, pruned : 394667, total: 400000, (  1.33% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 26.98 | loss  7.06 | ppl  1167.50
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 26.88 | loss  6.64 | ppl   767.37
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 26.68 | loss  6.32 | ppl   554.41
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 26.77 | loss  6.73 | ppl   839.21
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 27.06 | loss  6.44 | ppl   623.93
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 26.13 | loss  6.60 | ppl   737.07
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 26.91 | loss  6.35 | ppl   573.49
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 27.02 | loss  6.21 | ppl   499.76
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 26.84 | loss  6.11 | ppl   450.16
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 26.90 | loss  6.05 | ppl   424.76
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 26.17 | loss  6.05 | ppl   422.11
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 26.87 | loss  6.25 | ppl   516.92
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 26.96 | loss  6.13 | ppl   461.42
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 26.81 | loss  6.14 | ppl   464.35
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 88.43s | valid loss  5.93 | valid ppl   377.67
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 27.14 | loss  5.99 | ppl   399.77
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 27.02 | loss  6.07 | ppl   430.81
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 26.90 | loss  5.64 | ppl   281.36
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 26.91 | loss  6.10 | ppl   447.15
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 26.90 | loss  5.80 | ppl   329.71
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 27.18 | loss  6.01 | ppl   405.99
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 26.01 | loss  6.04 | ppl   419.38
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 27.01 | loss  5.74 | ppl   312.47
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 26.83 | loss  5.79 | ppl   328.26
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 26.97 | loss  5.71 | ppl   300.37
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 26.30 | loss  5.57 | ppl   262.16
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 26.76 | loss  5.79 | ppl   327.24
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 27.01 | loss  5.84 | ppl   343.02
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 27.02 | loss  5.73 | ppl   309.37
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 88.77s | valid loss  5.80 | valid ppl   331.24
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 26.95 | loss  5.48 | ppl   240.39
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 26.88 | loss  5.75 | ppl   315.54
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 27.03 | loss  5.33 | ppl   206.25
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 26.87 | loss  5.73 | ppl   308.04
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 27.03 | loss  5.53 | ppl   253.35
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 27.55 | loss  5.74 | ppl   310.08
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 26.20 | loss  5.82 | ppl   335.35
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 27.05 | loss  5.51 | ppl   246.55
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 26.92 | loss  5.60 | ppl   270.84
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 27.14 | loss  5.48 | ppl   240.47
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 26.17 | loss  5.38 | ppl   216.93
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 26.73 | loss  5.64 | ppl   280.30
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 26.75 | loss  5.67 | ppl   288.88
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 26.93 | loss  5.51 | ppl   247.24
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 89.02s | valid loss  5.75 | valid ppl   315.34
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 26.96 | loss  5.30 | ppl   200.06
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 26.81 | loss  5.46 | ppl   235.07
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 26.83 | loss  5.12 | ppl   167.13
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 26.94 | loss  5.56 | ppl   260.60
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 27.14 | loss  5.21 | ppl   183.39
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 27.29 | loss  5.39 | ppl   219.33
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 26.17 | loss  5.43 | ppl   229.15
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 26.79 | loss  5.07 | ppl   158.60
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 26.96 | loss  5.16 | ppl   173.53
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 27.19 | loss  5.03 | ppl   152.62
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 26.16 | loss  5.07 | ppl   159.69
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 26.79 | loss  5.25 | ppl   190.47
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 26.91 | loss  5.31 | ppl   201.67
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 26.80 | loss  5.09 | ppl   162.38
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 88.89s | valid loss  5.54 | valid ppl   255.69
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 27.12 | loss  5.20 | ppl   181.01
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 26.91 | loss  5.37 | ppl   215.33
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 26.92 | loss  5.02 | ppl   151.98
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 27.20 | loss  5.46 | ppl   235.96
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 27.31 | loss  5.18 | ppl   178.07
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 27.33 | loss  5.29 | ppl   197.49
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 26.09 | loss  5.35 | ppl   211.55
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 26.93 | loss  5.00 | ppl   147.98
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 26.81 | loss  5.11 | ppl   166.18
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 27.41 | loss  5.01 | ppl   149.54
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 26.54 | loss  4.99 | ppl   147.18
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 26.91 | loss  5.22 | ppl   184.51
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 27.00 | loss  5.25 | ppl   190.35
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 26.91 | loss  5.06 | ppl   157.84
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 89.17s | valid loss  5.53 | valid ppl   252.90
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 26.94 | loss  5.10 | ppl   164.72
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 26.78 | loss  5.31 | ppl   201.85
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 26.78 | loss  4.95 | ppl   141.59
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 26.83 | loss  5.40 | ppl   221.76
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 26.91 | loss  5.12 | ppl   166.68
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 27.15 | loss  5.30 | ppl   201.24
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 25.95 | loss  5.29 | ppl   197.38
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 27.60 | loss  4.94 | ppl   140.02
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 26.98 | loss  5.10 | ppl   164.15
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 27.51 | loss  4.91 | ppl   136.06
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 26.82 | loss  4.92 | ppl   137.54
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 26.88 | loss  5.19 | ppl   178.65
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 26.85 | loss  5.20 | ppl   181.33
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 26.93 | loss  5.02 | ppl   151.59
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 89.08s | valid loss  5.53 | valid ppl   250.89
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.42 | test ppl   226.93
=========================================================================================
