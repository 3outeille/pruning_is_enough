nohup: ignoring input
=> Reading YAML config from configs/hypercube/transformer/transformer_base.yml
Namespace(algo='hc_iter', alpha=1.0, alpha_prime=1.0, arch='transformer', batch_size=20, bias=False, bn_type='NonAffineBatchNorm', bottom_k_on_forward=False, checkpoint_at_prune=False, chg_mask=False, chg_weight=False, ckpt_interval=-1, config='configs/hypercube/transformer/transformer_base.yml', conv_type='SubnetConv', data='transformer_data/wikitext-2', data_dir='../data', dataset='MNIST', differentiate_clamp=False, dist_backend='nccl', epochs=6, evaluate=False, evaluate_only=False, fine_tune_lr=5.0, fine_tune_lr_policy='multistep_lr', fine_tune_optimizer='sgd', fine_tune_wd=0.0, first_layer_dense=False, first_layer_type=None, fixed_init=False, flips=None, freeze_weights=True, gamma=0.1, gpu=0, hc_period=1, hc_quantized=True, hc_warmup=9999, hidden_size=500, how_to_connect='prob', how_to_prune='random', imp_no_rewind=False, imp_resume_round=-1, imp_rewind_iter=1000, imp_rewind_model='short_imp/Liu_checkpoint_model_correct.pth', init='signed_constant', interpolate='prob', invert_sanity_check=False, iter_period=1, iter_start=0, label_smoothing=None, lam_finetune_loss=-1, last_layer_dense=False, lmbda=1e-05, load_ckpt=None, log_dir=None, log_interval=2, loss='cross-entropy-loss', lr=5.0, lr_adjust=50, lr_gamma=0.25, lr_policy='multistep_lr', max_iter=100000, metric='loss', milestones=[50, 100, 150, 200], mixed_precision=0, mode='fan_in', mode_connect=False, mode_connect_filename=None, momentum=0.0, multiprocessing_distributed=False, name=None, nesterov=False, no_bn_decay=False, no_cuda=False, noise=True, noise_ratio=0, nonlinearity='relu', num_round=1, num_step_finetune=10, num_test=1, num_trial=1, num_workers=4, optimizer='sgd', override_prune_rate=False, plot_hc_convergence=False, pretrained=None, pretrained2=None, print_freq=10, project_freq=1, prune_rate=0.5, prune_type='BottomK', pruning_strategy=None, quantize_threshold=0.5, random_subnet=False, rank=-1, regularization='L2', reinit=False, results_filename=None, resume=None, rewind_score=False, rewind_to_epoch=-1, round='naive', run_idx=None, save_every=-1, save_model=False, save_plot_data=False, scale_fan=False, score_init='unif', score_init_constant=None, seed=42, seed_fixed_init=24, shift=0.0, shuffle=False, skip_fine_tune=True, skip_sanity_checks=False, smart_ratio=-1, start_epoch=None, start_from_nothing=False, subfolder='transformer_10_sanity', submask_size=1, target_sparsity=10.0, td=0.99, temp=100000, trainer='default', transformer_bptt=35, transformer_clip=0.25, transformer_dropout=0.2, transformer_emsize=200, transformer_nhead=2, transformer_nhid=200, transformer_nlayers=2, trial_num=1, unflag_before_finetune=True, warmup_length=0, wd=0.0005, weight_training=False, width=1.0, width_mult=1.0, workers=4, world_size=-1, **{'compare-rounding': False})
Seeded everything: 42
Use GPU: 0 for training
transformer_data/wikitext-2/train.txt
transformer_data/wikitext-2/valid.txt
transformer_data/wikitext-2/test.txt
==> Conv Type: SubnetConv
==> BN Type: NonAffineBatchNorm
Computing prune_rate for target_sparsity 10.0 with iter_period 1
Setting prune_rate to 0.36904265551980675
| epoch   0 |   200/ 2983 batches | lr 5.00 | ms/batch 36.42 | loss 15.97 | ppl 8604938.76
| epoch   0 |   400/ 2983 batches | lr 5.00 | ms/batch 30.36 | loss 10.49 | ppl 36126.24
| epoch   0 |   600/ 2983 batches | lr 5.00 | ms/batch 30.01 | loss  8.33 | ppl  4148.66
| epoch   0 |   800/ 2983 batches | lr 5.00 | ms/batch 30.24 | loss  7.99 | ppl  2954.24
| epoch   0 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.17 | loss  7.61 | ppl  2020.09
| epoch   0 |  1200/ 2983 batches | lr 5.00 | ms/batch 29.54 | loss  7.65 | ppl  2091.16
| epoch   0 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.24 | loss  7.44 | ppl  1704.49
| epoch   0 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.32 | loss  7.15 | ppl  1276.59
| epoch   0 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.26 | loss  7.34 | ppl  1541.30
| epoch   0 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.30 | loss  7.18 | ppl  1318.77
| epoch   0 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.47 | loss  7.40 | ppl  1630.97
| epoch   0 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.13 | loss  7.33 | ppl  1530.59
| epoch   0 |  2600/ 2983 batches | lr 5.00 | ms/batch 30.51 | loss  7.36 | ppl  1568.33
| epoch   0 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.11 | loss  7.61 | ppl  2011.40
-----------------------------------------------------------------------------------------
| end of epoch   0 | time: 95.03s | valid loss  7.01 | valid ppl  1112.44
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.0002005858113989234
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   21336 /   40000 ( 53.34%) | total_pruned =   18664 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   21247 /   40000 ( 53.12%) | total_pruned =   18753 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   20227 /   40000 ( 50.57%) | total_pruned =   19773 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   21078 /   40000 ( 52.70%) | total_pruned =   18922 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   21250 /   40000 ( 53.12%) | total_pruned =   18750 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   21356 /   40000 ( 53.39%) | total_pruned =   18644 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   21234 /   40000 ( 53.09%) | total_pruned =   18766 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   20144 /   40000 ( 50.36%) | total_pruned =   19856 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   21049 /   40000 ( 52.62%) | total_pruned =   18951 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   21286 /   40000 ( 53.22%) | total_pruned =   18714 | shape = (200, 200)
alive: 210207, pruned : 189793, total: 400000, ( 52.55% remained)
Model avg sparsity: 0.5255175
| epoch   1 |   200/ 2983 batches | lr 5.00 | ms/batch 30.37 | loss  7.43 | ppl  1684.50
| epoch   1 |   400/ 2983 batches | lr 5.00 | ms/batch 30.30 | loss  7.39 | ppl  1625.87
| epoch   1 |   600/ 2983 batches | lr 5.00 | ms/batch 30.50 | loss  7.20 | ppl  1344.18
| epoch   1 |   800/ 2983 batches | lr 5.00 | ms/batch 30.26 | loss  7.61 | ppl  2014.45
| epoch   1 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.18 | loss  7.44 | ppl  1706.91
| epoch   1 |  1200/ 2983 batches | lr 5.00 | ms/batch 30.30 | loss  7.56 | ppl  1924.62
| epoch   1 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.29 | loss  7.42 | ppl  1666.05
| epoch   1 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.32 | loss  7.15 | ppl  1278.74
| epoch   1 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.16 | loss  7.28 | ppl  1452.93
| epoch   1 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.22 | loss  7.21 | ppl  1358.63
| epoch   1 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.22 | loss  7.39 | ppl  1618.49
| epoch   1 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.63 | loss  7.35 | ppl  1555.45
| epoch   1 |  2600/ 2983 batches | lr 5.00 | ms/batch 29.71 | loss  7.32 | ppl  1514.33
| epoch   1 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.66 | loss  7.56 | ppl  1925.17
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 94.27s | valid loss  6.97 | valid ppl  1061.89
-----------------------------------------------------------------------------------------
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   21336 /   40000 ( 53.34%) | total_pruned =   18664 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   21247 /   40000 ( 53.12%) | total_pruned =   18753 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   20227 /   40000 ( 50.57%) | total_pruned =   19773 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   21078 /   40000 ( 52.70%) | total_pruned =   18922 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   21250 /   40000 ( 53.12%) | total_pruned =   18750 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   21356 /   40000 ( 53.39%) | total_pruned =   18644 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   21234 /   40000 ( 53.09%) | total_pruned =   18766 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   20144 /   40000 ( 50.36%) | total_pruned =   19856 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   21049 /   40000 ( 52.62%) | total_pruned =   18951 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   21286 /   40000 ( 53.22%) | total_pruned =   18714 | shape = (200, 200)
alive: 210207, pruned : 189793, total: 400000, ( 52.55% remained)
Model avg sparsity: 0.5255175
| epoch   2 |   200/ 2983 batches | lr 5.00 | ms/batch 31.64 | loss  7.48 | ppl  1770.12
| epoch   2 |   400/ 2983 batches | lr 5.00 | ms/batch 30.20 | loss  7.39 | ppl  1614.24
| epoch   2 |   600/ 2983 batches | lr 5.00 | ms/batch 30.33 | loss  7.23 | ppl  1383.56
| epoch   2 |   800/ 2983 batches | lr 5.00 | ms/batch 30.40 | loss  7.54 | ppl  1887.82
| epoch   2 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.72 | loss  7.42 | ppl  1673.72
| epoch   2 |  1200/ 2983 batches | lr 5.00 | ms/batch 30.09 | loss  7.59 | ppl  1971.88
| epoch   2 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.93 | loss  7.41 | ppl  1649.69
| epoch   2 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.42 | loss  7.13 | ppl  1250.31
| epoch   2 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.37 | loss  7.31 | ppl  1502.54
| epoch   2 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.23 | loss  7.22 | ppl  1370.75
| epoch   2 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.23 | loss  7.38 | ppl  1598.11
| epoch   2 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.73 | loss  7.33 | ppl  1530.17
| epoch   2 |  2600/ 2983 batches | lr 5.00 | ms/batch 30.87 | loss  7.35 | ppl  1554.64
| epoch   2 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.32 | loss  7.57 | ppl  1935.69
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 95.13s | valid loss  7.00 | valid ppl  1095.53
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 5.490703031130195e-11
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   11371 /   40000 ( 28.43%) | total_pruned =   28629 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   11342 /   40000 ( 28.36%) | total_pruned =   28658 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   19243 /   40000 ( 48.11%) | total_pruned =   20757 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   11560 /   40000 ( 28.90%) | total_pruned =   28440 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   11669 /   40000 ( 29.17%) | total_pruned =   28331 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   11333 /   40000 ( 28.33%) | total_pruned =   28667 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   11319 /   40000 ( 28.30%) | total_pruned =   28681 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   19155 /   40000 ( 47.89%) | total_pruned =   20845 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   11753 /   40000 ( 29.38%) | total_pruned =   28247 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   11851 /   40000 ( 29.63%) | total_pruned =   28149 | shape = (200, 200)
alive: 130596, pruned : 269404, total: 400000, ( 32.65% remained)
Model avg sparsity: 0.32649
| epoch   3 |   200/ 2983 batches | lr 5.00 | ms/batch 30.49 | loss  7.46 | ppl  1728.71
| epoch   3 |   400/ 2983 batches | lr 5.00 | ms/batch 30.38 | loss  7.35 | ppl  1562.60
| epoch   3 |   600/ 2983 batches | lr 5.00 | ms/batch 30.38 | loss  7.19 | ppl  1324.43
| epoch   3 |   800/ 2983 batches | lr 5.00 | ms/batch 30.42 | loss  7.57 | ppl  1934.33
| epoch   3 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.42 | loss  7.43 | ppl  1686.54
| epoch   3 |  1200/ 2983 batches | lr 5.00 | ms/batch 30.74 | loss  7.59 | ppl  1975.58
| epoch   3 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.99 | loss  7.41 | ppl  1654.75
| epoch   3 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.82 | loss  7.12 | ppl  1241.27
| epoch   3 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.14 | loss  7.30 | ppl  1476.41
| epoch   3 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.21 | loss  7.25 | ppl  1403.69
| epoch   3 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.23 | loss  7.39 | ppl  1613.92
| epoch   3 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.34 | loss  7.32 | ppl  1517.14
| epoch   3 |  2600/ 2983 batches | lr 5.00 | ms/batch 31.05 | loss  7.33 | ppl  1531.73
| epoch   3 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.21 | loss  7.55 | ppl  1908.89
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 94.90s | valid loss  7.01 | valid ppl  1107.52
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 2.824835905555073e-14
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =    5148 /   40000 ( 12.87%) | total_pruned =   34852 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    5200 /   40000 ( 13.00%) | total_pruned =   34800 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   18958 /   40000 ( 47.40%) | total_pruned =   21042 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    5718 /   40000 ( 14.29%) | total_pruned =   34282 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    5678 /   40000 ( 14.20%) | total_pruned =   34322 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    5095 /   40000 ( 12.74%) | total_pruned =   34905 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    5183 /   40000 ( 12.96%) | total_pruned =   34817 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   19013 /   40000 ( 47.53%) | total_pruned =   20987 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    6050 /   40000 ( 15.12%) | total_pruned =   33950 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    5911 /   40000 ( 14.78%) | total_pruned =   34089 | shape = (200, 200)
alive: 81954, pruned : 318046, total: 400000, ( 20.49% remained)
Model avg sparsity: 0.204885
| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 30.38 | loss  7.44 | ppl  1698.69
| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 30.39 | loss  7.34 | ppl  1536.38
| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 30.24 | loss  7.17 | ppl  1297.82
| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 30.03 | loss  7.53 | ppl  1858.36
| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.05 | loss  7.40 | ppl  1633.75
| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 29.93 | loss  7.60 | ppl  1992.30
| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.23 | loss  7.37 | ppl  1585.68
| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.03 | loss  7.14 | ppl  1258.53
| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.14 | loss  7.34 | ppl  1533.51
| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.30 | loss  7.21 | ppl  1349.71
| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.24 | loss  7.38 | ppl  1597.01
| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.71 | loss  7.32 | ppl  1511.98
| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch 30.11 | loss  7.33 | ppl  1521.27
| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.03 | loss  7.54 | ppl  1881.60
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 94.04s | valid loss  6.97 | valid ppl  1062.87
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 1.3305429281945127e-17
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =    1182 /   40000 (  2.96%) | total_pruned =   38818 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    1144 /   40000 (  2.86%) | total_pruned =   38856 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   18809 /   40000 ( 47.02%) | total_pruned =   21191 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    2147 /   40000 (  5.37%) | total_pruned =   37853 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    2134 /   40000 (  5.33%) | total_pruned =   37866 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    1128 /   40000 (  2.82%) | total_pruned =   38872 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    1059 /   40000 (  2.65%) | total_pruned =   38941 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   18772 /   40000 ( 46.93%) | total_pruned =   21228 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    2533 /   40000 (  6.33%) | total_pruned =   37467 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    2398 /   40000 (  6.00%) | total_pruned =   37602 | shape = (200, 200)
alive: 51306, pruned : 348694, total: 400000, ( 12.83% remained)
Model avg sparsity: 0.128265
| epoch   5 |   200/ 2983 batches | lr 5.00 | ms/batch 30.14 | loss  7.44 | ppl  1710.36
| epoch   5 |   400/ 2983 batches | lr 5.00 | ms/batch 30.06 | loss  7.40 | ppl  1635.67
| epoch   5 |   600/ 2983 batches | lr 5.00 | ms/batch 29.86 | loss  7.15 | ppl  1274.65
| epoch   5 |   800/ 2983 batches | lr 5.00 | ms/batch 29.94 | loss  7.58 | ppl  1966.68
| epoch   5 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.09 | loss  7.45 | ppl  1722.65
| epoch   5 |  1200/ 2983 batches | lr 5.00 | ms/batch 29.97 | loss  7.59 | ppl  1971.41
| epoch   5 |  1400/ 2983 batches | lr 5.00 | ms/batch 31.47 | loss  7.35 | ppl  1551.07
| epoch   5 |  1600/ 2983 batches | lr 5.00 | ms/batch 29.76 | loss  7.14 | ppl  1256.87
| epoch   5 |  1800/ 2983 batches | lr 5.00 | ms/batch 29.28 | loss  7.33 | ppl  1523.47
| epoch   5 |  2000/ 2983 batches | lr 5.00 | ms/batch 29.08 | loss  7.20 | ppl  1336.43
| epoch   5 |  2200/ 2983 batches | lr 5.00 | ms/batch 28.93 | loss  7.41 | ppl  1644.67
| epoch   5 |  2400/ 2983 batches | lr 5.00 | ms/batch 29.68 | loss  7.33 | ppl  1528.60
| epoch   5 |  2600/ 2983 batches | lr 5.00 | ms/batch 32.08 | loss  7.34 | ppl  1547.75
| epoch   5 |  2800/ 2983 batches | lr 5.00 | ms/batch 28.62 | loss  7.56 | ppl  1916.54
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 92.82s | valid loss  6.98 | valid ppl  1072.25
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.0033149488735944033
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   17491 /   40000 ( 43.73%) | total_pruned =   22509 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     683 /   40000 (  1.71%) | total_pruned =   39317 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     667 /   40000 (  1.67%) | total_pruned =   39333 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   11422 /   40000 ( 28.55%) | total_pruned =   28578 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    1269 /   40000 (  3.17%) | total_pruned =   38731 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     826 /   40000 (  2.06%) | total_pruned =   39174 | shape = (200, 200)
alive: 32358, pruned : 367642, total: 400000, (  8.09% remained)
Model avg sparsity: 0.080895
=========================================================================================
| End of training | test loss  6.91 | test ppl   997.30
=========================================================================================
Beginning Sanity Checks:
Sanity Check 1: Weight Reinit
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   17491 /   40000 ( 43.73%) | total_pruned =   22509 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     683 /   40000 (  1.71%) | total_pruned =   39317 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     667 /   40000 (  1.67%) | total_pruned =   39333 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   11422 /   40000 ( 28.55%) | total_pruned =   28578 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    1269 /   40000 (  3.17%) | total_pruned =   38731 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     826 /   40000 (  2.06%) | total_pruned =   39174 | shape = (200, 200)
alive: 32358, pruned : 367642, total: 400000, (  8.09% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 27.21 | loss  7.06 | ppl  1163.23
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 27.03 | loss  6.76 | ppl   860.29
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 27.02 | loss  6.46 | ppl   640.76
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 26.97 | loss  6.71 | ppl   817.75
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 26.96 | loss  6.51 | ppl   672.61
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 25.85 | loss  6.50 | ppl   663.29
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 27.06 | loss  6.47 | ppl   643.50
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 27.03 | loss  6.13 | ppl   457.49
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 27.03 | loss  6.10 | ppl   448.03
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 27.18 | loss  6.14 | ppl   462.63
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 27.04 | loss  6.03 | ppl   416.45
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 27.00 | loss  6.19 | ppl   487.79
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 26.26 | loss  5.98 | ppl   395.87
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 26.98 | loss  6.12 | ppl   456.05
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 89.10s | valid loss  5.87 | valid ppl   353.90
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 27.76 | loss  5.76 | ppl   316.84
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 27.06 | loss  6.05 | ppl   425.52
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 27.06 | loss  5.55 | ppl   255.98
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 27.12 | loss  6.10 | ppl   446.75
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 27.03 | loss  5.76 | ppl   316.16
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 26.28 | loss  5.90 | ppl   364.48
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 26.89 | loss  5.96 | ppl   388.89
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 27.04 | loss  5.61 | ppl   274.32
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 27.06 | loss  5.67 | ppl   290.56
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 27.07 | loss  5.59 | ppl   268.71
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 27.21 | loss  5.52 | ppl   248.42
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 27.22 | loss  5.76 | ppl   316.42
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 26.27 | loss  5.75 | ppl   313.51
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 26.89 | loss  5.69 | ppl   295.71
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 89.23s | valid loss  5.75 | valid ppl   315.52
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 27.28 | loss  5.50 | ppl   244.79
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 27.11 | loss  5.68 | ppl   293.80
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 27.02 | loss  5.25 | ppl   189.63
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 26.91 | loss  5.72 | ppl   304.98
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 27.36 | loss  5.51 | ppl   247.26
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 26.65 | loss  5.64 | ppl   280.67
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 27.16 | loss  5.75 | ppl   313.16
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 27.14 | loss  5.35 | ppl   211.53
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 27.13 | loss  5.51 | ppl   247.61
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 27.17 | loss  5.24 | ppl   189.48
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 27.25 | loss  5.34 | ppl   209.13
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 27.12 | loss  5.61 | ppl   273.90
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 26.37 | loss  5.50 | ppl   245.51
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 27.01 | loss  5.43 | ppl   228.99
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 89.51s | valid loss  5.72 | valid ppl   304.73
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 27.34 | loss  5.19 | ppl   179.90
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 26.95 | loss  5.45 | ppl   231.65
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 26.86 | loss  5.01 | ppl   150.24
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 27.01 | loss  5.45 | ppl   232.78
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 26.73 | loss  5.18 | ppl   177.12
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 26.20 | loss  5.36 | ppl   212.03
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 27.01 | loss  5.36 | ppl   212.83
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 27.04 | loss  4.98 | ppl   145.39
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 27.38 | loss  5.11 | ppl   165.93
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 26.99 | loss  4.96 | ppl   141.94
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 27.09 | loss  4.96 | ppl   142.75
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 27.22 | loss  5.20 | ppl   181.64
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 26.36 | loss  5.18 | ppl   177.18
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 27.04 | loss  5.01 | ppl   150.52
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 89.35s | valid loss  5.51 | valid ppl   247.28
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 27.07 | loss  5.10 | ppl   163.75
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 27.06 | loss  5.29 | ppl   197.71
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 27.03 | loss  4.87 | ppl   130.63
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 26.94 | loss  5.40 | ppl   221.93
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 27.03 | loss  5.10 | ppl   163.85
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 26.13 | loss  5.26 | ppl   192.69
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 27.28 | loss  5.31 | ppl   201.58
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 27.24 | loss  4.93 | ppl   138.53
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 27.07 | loss  5.04 | ppl   154.12
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 26.99 | loss  4.90 | ppl   134.26
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 26.90 | loss  4.91 | ppl   135.03
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 26.26 | loss  5.11 | ppl   166.49
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 26.92 | loss  5.14 | ppl   170.54
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 27.19 | loss  4.98 | ppl   144.77
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 89.10s | valid loss  5.50 | valid ppl   245.80
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 27.06 | loss  5.04 | ppl   154.30
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 26.93 | loss  5.26 | ppl   192.93
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 26.97 | loss  4.85 | ppl   127.47
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 27.17 | loss  5.35 | ppl   210.93
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 27.04 | loss  4.96 | ppl   142.84
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 25.93 | loss  5.16 | ppl   174.78
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 26.95 | loss  5.26 | ppl   192.02
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 26.92 | loss  4.83 | ppl   124.79
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 26.82 | loss  5.01 | ppl   149.57
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 26.67 | loss  4.87 | ppl   130.55
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 26.90 | loss  4.86 | ppl   129.29
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 26.15 | loss  5.02 | ppl   151.21
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 26.94 | loss  5.05 | ppl   155.68
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 27.09 | loss  4.94 | ppl   139.90
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 88.92s | valid loss  5.50 | valid ppl   244.47
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.39 | test ppl   219.92
=========================================================================================
Sanity Check 2: Mask Reshuffle
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   17491 /   40000 ( 43.73%) | total_pruned =   22509 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     683 /   40000 (  1.71%) | total_pruned =   39317 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     667 /   40000 (  1.67%) | total_pruned =   39333 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   11422 /   40000 ( 28.55%) | total_pruned =   28578 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    1269 /   40000 (  3.17%) | total_pruned =   38731 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     826 /   40000 (  2.06%) | total_pruned =   39174 | shape = (200, 200)
alive: 32358, pruned : 367642, total: 400000, (  8.09% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 27.02 | loss  7.05 | ppl  1149.47
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 27.36 | loss  6.88 | ppl   970.85
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 26.82 | loss  6.32 | ppl   556.57
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 26.81 | loss  6.69 | ppl   807.51
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 27.29 | loss  6.47 | ppl   646.76
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 26.27 | loss  6.54 | ppl   689.21
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 26.91 | loss  6.38 | ppl   590.69
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 27.07 | loss  6.20 | ppl   491.29
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 26.82 | loss  6.23 | ppl   509.21
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 26.97 | loss  6.09 | ppl   440.72
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 26.94 | loss  6.00 | ppl   404.80
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 26.36 | loss  6.25 | ppl   516.44
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 26.77 | loss  6.09 | ppl   442.71
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 26.95 | loss  6.11 | ppl   448.31
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 88.64s | valid loss  5.87 | valid ppl   355.15
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 26.93 | loss  5.76 | ppl   317.83
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 27.00 | loss  6.00 | ppl   403.91
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 27.01 | loss  5.61 | ppl   272.53
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 26.84 | loss  6.09 | ppl   441.53
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 26.92 | loss  5.74 | ppl   310.33
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 26.28 | loss  5.93 | ppl   375.17
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 26.77 | loss  5.97 | ppl   390.85
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 26.98 | loss  5.70 | ppl   297.63
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 26.94 | loss  5.73 | ppl   306.44
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 26.92 | loss  5.51 | ppl   246.19
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 27.17 | loss  5.52 | ppl   250.34
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 26.65 | loss  5.75 | ppl   312.70
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 26.95 | loss  5.76 | ppl   318.16
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 27.21 | loss  5.66 | ppl   285.85
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 88.63s | valid loss  5.76 | valid ppl   316.77
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 26.89 | loss  5.55 | ppl   256.33
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 26.74 | loss  5.81 | ppl   333.72
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 26.88 | loss  5.27 | ppl   193.86
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 27.05 | loss  5.73 | ppl   308.61
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 27.01 | loss  5.50 | ppl   245.55
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 26.36 | loss  5.70 | ppl   297.75
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 26.80 | loss  5.67 | ppl   289.81
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 26.81 | loss  5.42 | ppl   225.83
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 26.86 | loss  5.49 | ppl   241.36
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 26.95 | loss  5.35 | ppl   211.27
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 27.34 | loss  5.31 | ppl   202.99
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 26.34 | loss  5.50 | ppl   244.98
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 26.88 | loss  5.57 | ppl   261.67
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 26.76 | loss  5.36 | ppl   212.68
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 88.50s | valid loss  5.69 | valid ppl   296.12
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 26.86 | loss  5.21 | ppl   183.33
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 26.92 | loss  5.42 | ppl   226.41
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 26.82 | loss  5.02 | ppl   152.02
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 26.87 | loss  5.52 | ppl   249.87
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 27.14 | loss  5.16 | ppl   173.55
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 26.35 | loss  5.34 | ppl   208.23
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 26.90 | loss  5.36 | ppl   212.13
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 26.89 | loss  4.99 | ppl   147.63
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 26.96 | loss  5.13 | ppl   168.50
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 27.30 | loss  5.00 | ppl   148.58
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 27.35 | loss  4.99 | ppl   146.70
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 26.42 | loss  5.15 | ppl   172.03
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 26.84 | loss  5.24 | ppl   188.81
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 26.92 | loss  5.02 | ppl   151.58
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 88.55s | valid loss  5.51 | valid ppl   246.72
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 27.07 | loss  5.10 | ppl   164.81
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 26.92 | loss  5.29 | ppl   198.79
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 26.95 | loss  4.91 | ppl   135.67
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 27.00 | loss  5.42 | ppl   225.62
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 26.91 | loss  5.09 | ppl   162.93
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 26.20 | loss  5.27 | ppl   194.09
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 26.82 | loss  5.29 | ppl   197.67
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 26.78 | loss  4.92 | ppl   136.54
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 26.89 | loss  5.04 | ppl   154.48
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 26.85 | loss  4.92 | ppl   137.08
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 26.94 | loss  4.95 | ppl   140.87
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 26.47 | loss  5.10 | ppl   163.52
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 26.63 | loss  5.15 | ppl   171.99
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 27.06 | loss  4.91 | ppl   135.48
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 88.47s | valid loss  5.50 | valid ppl   245.29
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 26.91 | loss  5.03 | ppl   152.63
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 26.88 | loss  5.22 | ppl   185.77
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 26.86 | loss  4.86 | ppl   129.62
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 27.01 | loss  5.35 | ppl   211.04
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 27.05 | loss  5.02 | ppl   151.20
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 26.00 | loss  5.27 | ppl   193.89
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 25.98 | loss  5.18 | ppl   177.51
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 19.17 | loss  4.87 | ppl   130.10
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 19.12 | loss  5.06 | ppl   156.82
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 19.17 | loss  4.84 | ppl   126.74
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 18.77 | loss  4.85 | ppl   127.64
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 18.20 | loss  5.08 | ppl   160.34
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 16.75 | loss  5.12 | ppl   168.04
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 13.02 | loss  4.90 | ppl   134.39
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 69.74s | valid loss  5.50 | valid ppl   243.92
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.40 | test ppl   220.34
=========================================================================================
