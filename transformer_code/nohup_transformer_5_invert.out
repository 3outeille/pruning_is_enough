nohup: ignoring input
=> Reading YAML config from configs/hypercube/transformer/transformer_invert.yml
Namespace(algo='hc_iter', alpha=1.0, alpha_prime=1.0, arch='transformer', batch_size=20, bias=False, bn_type='NonAffineBatchNorm', bottom_k_on_forward=False, checkpoint_at_prune=False, chg_mask=False, chg_weight=False, ckpt_interval=-1, config='configs/hypercube/transformer/transformer_invert.yml', conv_type='SubnetConv', data='transformer_data/wikitext-2', data_dir='../data', dataset='MNIST', differentiate_clamp=False, dist_backend='nccl', epochs=6, evaluate=False, evaluate_only=False, fine_tune_lr=5.0, fine_tune_lr_policy='multistep_lr', fine_tune_optimizer='sgd', fine_tune_wd=0.0, first_layer_dense=False, first_layer_type=None, fixed_init=False, flips=None, freeze_weights=True, gamma=0.1, gpu=1, hc_period=1, hc_quantized=True, hc_warmup=9999, hidden_size=500, how_to_connect='prob', how_to_prune='random', imp_no_rewind=False, imp_resume_round=-1, imp_rewind_iter=1000, imp_rewind_model='short_imp/Liu_checkpoint_model_correct.pth', init='signed_constant', interpolate='prob', invert_sanity_check=True, iter_period=1, iter_start=0, label_smoothing=None, lam_finetune_loss=-1, last_layer_dense=False, lmbda=1e-05, load_ckpt=None, log_dir=None, log_interval=2, loss='cross-entropy-loss', lr=5.0, lr_adjust=50, lr_gamma=0.25, lr_policy='multistep_lr', max_iter=100000, metric='loss', milestones=[50, 100, 150, 200], mixed_precision=0, mode='fan_in', mode_connect=False, mode_connect_filename=None, momentum=0.0, multiprocessing_distributed=False, name=None, nesterov=False, no_bn_decay=False, no_cuda=False, noise=True, noise_ratio=0, nonlinearity='relu', num_round=1, num_step_finetune=10, num_test=1, num_trial=1, num_workers=4, optimizer='sgd', override_prune_rate=False, plot_hc_convergence=False, pretrained=None, pretrained2=None, print_freq=10, project_freq=1, prune_rate=0.5, prune_type='BottomK', pruning_strategy=None, quantize_threshold=0.5, random_subnet=False, rank=-1, regularization='L2', reinit=False, results_filename=None, resume=None, rewind_score=False, rewind_to_epoch=-1, round='naive', run_idx=None, save_every=-1, save_model=False, save_plot_data=False, scale_fan=False, score_init='unif', score_init_constant=None, seed=42, seed_fixed_init=24, shift=0.0, shuffle=False, skip_fine_tune=False, skip_sanity_checks=True, smart_ratio=-1, start_epoch=None, start_from_nothing=False, subfolder='transformer_5_invert', submask_size=1, target_sparsity=5.0, td=0.99, temp=100000, trainer='default', transformer_bptt=35, transformer_clip=0.25, transformer_dropout=0.2, transformer_emsize=200, transformer_nhead=2, transformer_nhid=200, transformer_nlayers=2, trial_num=1, unflag_before_finetune=True, warmup_length=0, wd=0.0005, weight_training=False, width=1.0, width_mult=1.0, workers=4, world_size=-1, **{'compare-rounding': False})
Seeded everything: 42
Use GPU: 1 for training
transformer_data/wikitext-2/train.txt
transformer_data/wikitext-2/valid.txt
transformer_data/wikitext-2/test.txt
==> Conv Type: SubnetConv
==> BN Type: NonAffineBatchNorm
Computing prune_rate for target_sparsity 5.0 with iter_period 1
Setting prune_rate to 0.4507197283469411
| epoch   0 |   200/ 2983 batches | lr 5.00 | ms/batch 44.12 | loss 15.97 | ppl 8604938.76
| epoch   0 |   400/ 2983 batches | lr 5.00 | ms/batch 36.59 | loss 10.49 | ppl 36126.24
| epoch   0 |   600/ 2983 batches | lr 5.00 | ms/batch 36.29 | loss  8.33 | ppl  4148.66
| epoch   0 |   800/ 2983 batches | lr 5.00 | ms/batch 35.97 | loss  7.99 | ppl  2954.24
| epoch   0 |  1000/ 2983 batches | lr 5.00 | ms/batch 35.93 | loss  7.61 | ppl  2020.09
| epoch   0 |  1200/ 2983 batches | lr 5.00 | ms/batch 37.20 | loss  7.65 | ppl  2091.16
| epoch   0 |  1400/ 2983 batches | lr 5.00 | ms/batch 37.32 | loss  7.44 | ppl  1704.49
| epoch   0 |  1600/ 2983 batches | lr 5.00 | ms/batch 37.17 | loss  7.15 | ppl  1276.59
| epoch   0 |  1800/ 2983 batches | lr 5.00 | ms/batch 36.10 | loss  7.34 | ppl  1541.30
| epoch   0 |  2000/ 2983 batches | lr 5.00 | ms/batch 37.79 | loss  7.18 | ppl  1318.77
| epoch   0 |  2200/ 2983 batches | lr 5.00 | ms/batch 37.94 | loss  7.40 | ppl  1630.97
| epoch   0 |  2400/ 2983 batches | lr 5.00 | ms/batch 37.89 | loss  7.33 | ppl  1530.59
| epoch   0 |  2600/ 2983 batches | lr 5.00 | ms/batch 38.91 | loss  7.36 | ppl  1568.33
| epoch   0 |  2800/ 2983 batches | lr 5.00 | ms/batch 48.36 | loss  7.61 | ppl  2011.40
-----------------------------------------------------------------------------------------
| end of epoch   0 | time: 123.62s | valid loss  7.01 | valid ppl  1112.44
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.00029854458989575505
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   27835 /   40000 ( 69.59%) | total_pruned =   12165 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   27823 /   40000 ( 69.56%) | total_pruned =   12177 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   19827 /   40000 ( 49.57%) | total_pruned =   20173 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   27698 /   40000 ( 69.25%) | total_pruned =   12302 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   27566 /   40000 ( 68.92%) | total_pruned =   12434 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   27899 /   40000 ( 69.75%) | total_pruned =   12101 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   27877 /   40000 ( 69.69%) | total_pruned =   12123 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   20091 /   40000 ( 50.23%) | total_pruned =   19909 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   27526 /   40000 ( 68.81%) | total_pruned =   12474 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   27391 /   40000 ( 68.48%) | total_pruned =   12609 | shape = (200, 200)
alive: 261533, pruned : 138467, total: 400000, ( 65.38% remained)
Model avg sparsity: 0.6538325
| epoch   1 |   200/ 2983 batches | lr 5.00 | ms/batch 49.10 | loss  8.53 | ppl  5056.61
| epoch   1 |   400/ 2983 batches | lr 5.00 | ms/batch 48.99 | loss  8.15 | ppl  3447.18
| epoch   1 |   600/ 2983 batches | lr 5.00 | ms/batch 48.94 | loss  7.75 | ppl  2330.77
| epoch   1 |   800/ 2983 batches | lr 5.00 | ms/batch 48.31 | loss  7.99 | ppl  2941.76
| epoch   1 |  1000/ 2983 batches | lr 5.00 | ms/batch 48.46 | loss  7.74 | ppl  2300.13
| epoch   1 |  1200/ 2983 batches | lr 5.00 | ms/batch 49.43 | loss  7.90 | ppl  2698.11
| epoch   1 |  1400/ 2983 batches | lr 5.00 | ms/batch 49.90 | loss  7.69 | ppl  2177.05
| epoch   1 |  1600/ 2983 batches | lr 5.00 | ms/batch 49.62 | loss  7.42 | ppl  1668.62
| epoch   1 |  1800/ 2983 batches | lr 5.00 | ms/batch 48.53 | loss  7.55 | ppl  1900.64
| epoch   1 |  2000/ 2983 batches | lr 5.00 | ms/batch 50.03 | loss  7.42 | ppl  1668.28
| epoch   1 |  2200/ 2983 batches | lr 5.00 | ms/batch 49.89 | loss  7.59 | ppl  1978.54
| epoch   1 |  2400/ 2983 batches | lr 5.00 | ms/batch 49.01 | loss  7.53 | ppl  1860.03
| epoch   1 |  2600/ 2983 batches | lr 5.00 | ms/batch 49.19 | loss  7.54 | ppl  1872.82
| epoch   1 |  2800/ 2983 batches | lr 5.00 | ms/batch 50.03 | loss  7.72 | ppl  2264.22
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 154.57s | valid loss  7.07 | valid ppl  1180.17
-----------------------------------------------------------------------------------------
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   27835 /   40000 ( 69.59%) | total_pruned =   12165 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   27823 /   40000 ( 69.56%) | total_pruned =   12177 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   19827 /   40000 ( 49.57%) | total_pruned =   20173 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   27698 /   40000 ( 69.25%) | total_pruned =   12302 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   27566 /   40000 ( 68.92%) | total_pruned =   12434 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   27899 /   40000 ( 69.75%) | total_pruned =   12101 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   27877 /   40000 ( 69.69%) | total_pruned =   12123 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   20091 /   40000 ( 50.23%) | total_pruned =   19909 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   27526 /   40000 ( 68.81%) | total_pruned =   12474 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   27391 /   40000 ( 68.48%) | total_pruned =   12609 | shape = (200, 200)
alive: 261533, pruned : 138467, total: 400000, ( 65.38% remained)
Model avg sparsity: 0.6538325
| epoch   2 |   200/ 2983 batches | lr 5.00 | ms/batch 50.54 | loss  7.64 | ppl  2076.68
| epoch   2 |   400/ 2983 batches | lr 5.00 | ms/batch 51.50 | loss  7.56 | ppl  1916.28
| epoch   2 |   600/ 2983 batches | lr 5.00 | ms/batch 51.61 | loss  7.41 | ppl  1653.49
| epoch   2 |   800/ 2983 batches | lr 5.00 | ms/batch 51.20 | loss  7.74 | ppl  2293.72
| epoch   2 |  1000/ 2983 batches | lr 5.00 | ms/batch 49.72 | loss  7.59 | ppl  1981.44
| epoch   2 |  1200/ 2983 batches | lr 5.00 | ms/batch 51.16 | loss  7.78 | ppl  2403.51
| epoch   2 |  1400/ 2983 batches | lr 5.00 | ms/batch 51.03 | loss  7.58 | ppl  1958.25
| epoch   2 |  1600/ 2983 batches | lr 5.00 | ms/batch 50.05 | loss  7.33 | ppl  1528.26
| epoch   2 |  1800/ 2983 batches | lr 5.00 | ms/batch 48.00 | loss  7.49 | ppl  1794.24
| epoch   2 |  2000/ 2983 batches | lr 5.00 | ms/batch 50.80 | loss  7.41 | ppl  1657.53
| epoch   2 |  2200/ 2983 batches | lr 5.00 | ms/batch 50.84 | loss  7.56 | ppl  1918.26
| epoch   2 |  2400/ 2983 batches | lr 5.00 | ms/batch 50.95 | loss  7.48 | ppl  1770.77
| epoch   2 |  2600/ 2983 batches | lr 5.00 | ms/batch 49.22 | loss  7.54 | ppl  1876.79
| epoch   2 |  2800/ 2983 batches | lr 5.00 | ms/batch 51.06 | loss  7.71 | ppl  2223.84
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 158.44s | valid loss  7.06 | valid ppl  1159.80
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 3.50624494716012e-11
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   18271 /   40000 ( 45.68%) | total_pruned =   21729 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   18366 /   40000 ( 45.91%) | total_pruned =   21634 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   13134 /   40000 ( 32.84%) | total_pruned =   26866 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   18528 /   40000 ( 46.32%) | total_pruned =   21472 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   18356 /   40000 ( 45.89%) | total_pruned =   21644 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   18242 /   40000 ( 45.60%) | total_pruned =   21758 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   18393 /   40000 ( 45.98%) | total_pruned =   21607 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   15760 /   40000 ( 39.40%) | total_pruned =   24240 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   18566 /   40000 ( 46.41%) | total_pruned =   21434 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   18296 /   40000 ( 45.74%) | total_pruned =   21704 | shape = (200, 200)
alive: 175912, pruned : 224088, total: 400000, ( 43.98% remained)
Model avg sparsity: 0.43978
| epoch   3 |   200/ 2983 batches | lr 5.00 | ms/batch 51.00 | loss  8.47 | ppl  4755.62
| epoch   3 |   400/ 2983 batches | lr 5.00 | ms/batch 51.48 | loss  8.39 | ppl  4411.47
| epoch   3 |   600/ 2983 batches | lr 5.00 | ms/batch 51.03 | loss  8.21 | ppl  3695.46
| epoch   3 |   800/ 2983 batches | lr 5.00 | ms/batch 50.17 | loss  8.50 | ppl  4895.55
| epoch   3 |  1000/ 2983 batches | lr 5.00 | ms/batch 49.31 | loss  8.29 | ppl  3990.12
| epoch   3 |  1200/ 2983 batches | lr 5.00 | ms/batch 50.99 | loss  8.51 | ppl  4946.56
| epoch   3 |  1400/ 2983 batches | lr 5.00 | ms/batch 51.60 | loss  8.28 | ppl  3950.48
| epoch   3 |  1600/ 2983 batches | lr 5.00 | ms/batch 51.18 | loss  8.01 | ppl  3015.38
| epoch   3 |  1800/ 2983 batches | lr 5.00 | ms/batch 49.65 | loss  8.16 | ppl  3510.73
| epoch   3 |  2000/ 2983 batches | lr 5.00 | ms/batch 49.81 | loss  8.03 | ppl  3071.65
| epoch   3 |  2200/ 2983 batches | lr 5.00 | ms/batch 51.22 | loss  8.27 | ppl  3893.15
| epoch   3 |  2400/ 2983 batches | lr 5.00 | ms/batch 50.05 | loss  8.19 | ppl  3593.83
| epoch   3 |  2600/ 2983 batches | lr 5.00 | ms/batch 49.31 | loss  8.23 | ppl  3741.79
| epoch   3 |  2800/ 2983 batches | lr 5.00 | ms/batch 51.93 | loss  8.34 | ppl  4190.23
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 158.88s | valid loss  7.65 | valid ppl  2095.85
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 9.981457426268357e-15
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   12291 /   40000 ( 30.73%) | total_pruned =   27709 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   12389 /   40000 ( 30.97%) | total_pruned =   27611 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    8765 /   40000 ( 21.91%) | total_pruned =   31235 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   12705 /   40000 ( 31.76%) | total_pruned =   27295 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   12585 /   40000 ( 31.46%) | total_pruned =   27415 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   12292 /   40000 ( 30.73%) | total_pruned =   27708 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   12381 /   40000 ( 30.95%) | total_pruned =   27619 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   14777 /   40000 ( 36.94%) | total_pruned =   25223 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   12968 /   40000 ( 32.42%) | total_pruned =   27032 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   12662 /   40000 ( 31.66%) | total_pruned =   27338 | shape = (200, 200)
alive: 123815, pruned : 276185, total: 400000, ( 30.95% remained)
Model avg sparsity: 0.3095375
| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 51.24 | loss  8.37 | ppl  4302.82
| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 51.21 | loss  8.38 | ppl  4342.16
| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 51.66 | loss  8.16 | ppl  3513.57
| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 50.43 | loss  8.46 | ppl  4705.27
| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 49.26 | loss  8.33 | ppl  4154.18
| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 51.27 | loss  8.53 | ppl  5063.93
| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch 50.54 | loss  8.35 | ppl  4223.66
| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch 51.25 | loss  8.11 | ppl  3316.32
| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch 50.78 | loss  8.27 | ppl  3901.92
| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch 52.46 | loss  8.14 | ppl  3429.06
| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch 53.64 | loss  8.40 | ppl  4429.12
| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch 53.83 | loss  8.29 | ppl  3973.93
| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch 52.42 | loss  8.34 | ppl  4177.97
| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch 53.97 | loss  8.46 | ppl  4736.92
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 162.46s | valid loss  7.86 | valid ppl  2590.17
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 2.9579862899610512e-18
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =    8666 /   40000 ( 21.66%) | total_pruned =   31334 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    8679 /   40000 ( 21.70%) | total_pruned =   31321 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    8599 /   40000 ( 21.50%) | total_pruned =   31401 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    9271 /   40000 ( 23.18%) | total_pruned =   30729 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    9122 /   40000 ( 22.80%) | total_pruned =   30878 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    8668 /   40000 ( 21.67%) | total_pruned =   31332 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    8694 /   40000 ( 21.73%) | total_pruned =   31306 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   14518 /   40000 ( 36.30%) | total_pruned =   25482 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    9525 /   40000 ( 23.81%) | total_pruned =   30475 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    9218 /   40000 ( 23.05%) | total_pruned =   30782 | shape = (200, 200)
alive: 94960, pruned : 305040, total: 400000, ( 23.74% remained)
Model avg sparsity: 0.2374
| epoch   5 |   200/ 2983 batches | lr 5.00 | ms/batch 53.20 | loss  8.36 | ppl  4274.62
| epoch   5 |   400/ 2983 batches | lr 5.00 | ms/batch 50.42 | loss  8.40 | ppl  4450.79
| epoch   5 |   600/ 2983 batches | lr 5.00 | ms/batch 50.74 | loss  8.12 | ppl  3352.28
| epoch   5 |   800/ 2983 batches | lr 5.00 | ms/batch 49.94 | loss  8.50 | ppl  4912.88
| epoch   5 |  1000/ 2983 batches | lr 5.00 | ms/batch 49.30 | loss  8.36 | ppl  4263.45
| epoch   5 |  1200/ 2983 batches | lr 5.00 | ms/batch 49.56 | loss  8.56 | ppl  5233.62
| epoch   5 |  1400/ 2983 batches | lr 5.00 | ms/batch 53.65 | loss  8.35 | ppl  4241.16
| epoch   5 |  1600/ 2983 batches | lr 5.00 | ms/batch 51.16 | loss  8.13 | ppl  3380.03
| epoch   5 |  1800/ 2983 batches | lr 5.00 | ms/batch 49.53 | loss  8.26 | ppl  3866.14
| epoch   5 |  2000/ 2983 batches | lr 5.00 | ms/batch 50.54 | loss  8.14 | ppl  3436.88
| epoch   5 |  2200/ 2983 batches | lr 5.00 | ms/batch 53.04 | loss  8.40 | ppl  4425.85
| epoch   5 |  2400/ 2983 batches | lr 5.00 | ms/batch 51.42 | loss  8.31 | ppl  4074.90
| epoch   5 |  2600/ 2983 batches | lr 5.00 | ms/batch 50.11 | loss  8.36 | ppl  4291.67
| epoch   5 |  2800/ 2983 batches | lr 5.00 | ms/batch 51.63 | loss  8.44 | ppl  4649.04
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 160.41s | valid loss  7.86 | valid ppl  2601.88
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 9.633828878242378e-22
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =    6630 /   40000 ( 16.57%) | total_pruned =   33370 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    6712 /   40000 ( 16.78%) | total_pruned =   33288 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    8592 /   40000 ( 21.48%) | total_pruned =   31408 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    7273 /   40000 ( 18.18%) | total_pruned =   32727 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    7205 /   40000 ( 18.01%) | total_pruned =   32795 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    6621 /   40000 ( 16.55%) | total_pruned =   33379 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    6714 /   40000 ( 16.79%) | total_pruned =   33286 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   14498 /   40000 ( 36.24%) | total_pruned =   25502 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    7659 /   40000 ( 19.15%) | total_pruned =   32341 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    7341 /   40000 ( 18.35%) | total_pruned =   32659 | shape = (200, 200)
alive: 79245, pruned : 320755, total: 400000, ( 19.81% remained)
Model avg sparsity: 0.1981125
=========================================================================================
| End of training | test loss  7.79 | test ppl  2420.63
=========================================================================================
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =    6630 /   40000 ( 16.57%) | total_pruned =   33370 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    6712 /   40000 ( 16.78%) | total_pruned =   33288 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    8592 /   40000 ( 21.48%) | total_pruned =   31408 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    7273 /   40000 ( 18.18%) | total_pruned =   32727 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    7205 /   40000 ( 18.01%) | total_pruned =   32795 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    6621 /   40000 ( 16.55%) | total_pruned =   33379 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    6714 /   40000 ( 16.79%) | total_pruned =   33286 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   14498 /   40000 ( 36.24%) | total_pruned =   25502 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    7659 /   40000 ( 19.15%) | total_pruned =   32341 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    7341 /   40000 ( 18.35%) | total_pruned =   32659 | shape = (200, 200)
alive: 79245, pruned : 320755, total: 400000, ( 19.81% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 51.39 | loss  7.32 | ppl  1508.76
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 51.22 | loss  6.97 | ppl  1064.63
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 50.72 | loss  6.38 | ppl   589.25
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 49.02 | loss  6.71 | ppl   820.00
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 48.94 | loss  6.40 | ppl   599.97
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 51.70 | loss  6.45 | ppl   630.08
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 50.64 | loss  6.47 | ppl   644.34
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 48.77 | loss  6.05 | ppl   424.32
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 49.76 | loss  6.11 | ppl   450.64
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 51.76 | loss  5.93 | ppl   375.13
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 51.43 | loss  5.92 | ppl   372.94
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 61.58 | loss  6.05 | ppl   424.81
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 53.77 | loss  5.92 | ppl   371.79
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 51.55 | loss  5.88 | ppl   358.77
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 173.88s | valid loss  5.81 | valid ppl   334.16
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 57.07 | loss  5.80 | ppl   329.92
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 56.38 | loss  5.94 | ppl   381.11
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 56.04 | loss  5.46 | ppl   234.89
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 52.75 | loss  5.91 | ppl   367.28
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 53.77 | loss  5.68 | ppl   291.73
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 55.66 | loss  5.82 | ppl   338.66
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 50.30 | loss  5.86 | ppl   350.35
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 48.42 | loss  5.50 | ppl   245.22
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 49.92 | loss  5.52 | ppl   250.49
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 52.00 | loss  5.38 | ppl   217.89
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 52.79 | loss  5.39 | ppl   219.63
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 54.50 | loss  5.55 | ppl   256.60
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 53.44 | loss  5.61 | ppl   274.21
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 50.43 | loss  5.40 | ppl   220.92
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 176.78s | valid loss  5.59 | valid ppl   268.45
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 54.58 | loss  5.35 | ppl   211.30
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 54.59 | loss  5.55 | ppl   256.84
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 53.81 | loss  5.10 | ppl   164.80
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 51.22 | loss  5.59 | ppl   266.58
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 52.33 | loss  5.25 | ppl   189.93
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 54.52 | loss  5.51 | ppl   247.99
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 53.11 | loss  5.59 | ppl   268.00
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 51.33 | loss  5.16 | ppl   173.97
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 53.14 | loss  5.27 | ppl   194.13
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 54.86 | loss  5.18 | ppl   177.22
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 54.68 | loss  5.08 | ppl   160.25
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 55.82 | loss  5.27 | ppl   193.60
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 54.93 | loss  5.36 | ppl   213.46
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 55.98 | loss  5.10 | ppl   164.06
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 179.08s | valid loss  5.51 | valid ppl   246.91
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 47.61 | loss  5.07 | ppl   159.00
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 45.96 | loss  5.33 | ppl   206.59
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 45.07 | loss  4.84 | ppl   127.07
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 43.77 | loss  5.32 | ppl   203.69
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 43.15 | loss  5.01 | ppl   149.34
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 44.37 | loss  5.26 | ppl   192.31
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 44.00 | loss  5.17 | ppl   176.69
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 42.43 | loss  4.76 | ppl   117.13
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 42.44 | loss  4.97 | ppl   143.45
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 38.54 | loss  4.79 | ppl   120.46
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 34.16 | loss  4.84 | ppl   126.27
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 34.02 | loss  4.95 | ppl   141.82
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 33.79 | loss  5.00 | ppl   148.92
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 32.83 | loss  4.71 | ppl   110.65
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 132.43s | valid loss  5.36 | valid ppl   212.78
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 33.70 | loss  4.90 | ppl   134.82
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 33.72 | loss  5.16 | ppl   174.52
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 33.33 | loss  4.69 | ppl   108.54
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 31.69 | loss  5.22 | ppl   185.69
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 31.58 | loss  4.86 | ppl   128.99
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 33.09 | loss  5.19 | ppl   179.01
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 33.50 | loss  5.08 | ppl   160.12
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 33.52 | loss  4.68 | ppl   107.30
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 32.94 | loss  4.87 | ppl   130.00
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 33.19 | loss  4.75 | ppl   115.28
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 33.01 | loss  4.71 | ppl   110.70
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 32.84 | loss  4.85 | ppl   127.71
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 32.23 | loss  4.89 | ppl   132.82
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 31.53 | loss  4.64 | ppl   103.70
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 109.66s | valid loss  5.35 | valid ppl   209.63
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 33.13 | loss  4.80 | ppl   121.51
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 32.76 | loss  5.13 | ppl   168.92
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 32.78 | loss  4.65 | ppl   104.86
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 31.44 | loss  5.10 | ppl   164.33
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 30.47 | loss  4.72 | ppl   112.19
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 31.44 | loss  5.06 | ppl   158.28
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 24.35 | loss  5.00 | ppl   148.55
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 24.29 | loss  4.61 | ppl   100.02
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 24.08 | loss  4.78 | ppl   119.03
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 24.10 | loss  4.70 | ppl   110.08
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 24.21 | loss  4.67 | ppl   107.02
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 24.20 | loss  4.87 | ppl   130.07
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 23.60 | loss  4.86 | ppl   129.65
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 21.98 | loss  4.59 | ppl    98.74
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 89.60s | valid loss  5.34 | valid ppl   208.68
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.25 | test ppl   191.39
=========================================================================================
