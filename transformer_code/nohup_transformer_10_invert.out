nohup: ignoring input
=> Reading YAML config from configs/hypercube/transformer/transformer_invert.yml
Namespace(algo='hc_iter', alpha=1.0, alpha_prime=1.0, arch='transformer', batch_size=20, bias=False, bn_type='NonAffineBatchNorm', bottom_k_on_forward=False, checkpoint_at_prune=False, chg_mask=False, chg_weight=False, ckpt_interval=-1, config='configs/hypercube/transformer/transformer_invert.yml', conv_type='SubnetConv', data='transformer_data/wikitext-2', data_dir='../data', dataset='MNIST', differentiate_clamp=False, dist_backend='nccl', epochs=6, evaluate=False, evaluate_only=False, fine_tune_lr=5.0, fine_tune_lr_policy='multistep_lr', fine_tune_optimizer='sgd', fine_tune_wd=0.0, first_layer_dense=False, first_layer_type=None, fixed_init=False, flips=None, freeze_weights=True, gamma=0.1, gpu=2, hc_period=1, hc_quantized=True, hc_warmup=9999, hidden_size=500, how_to_connect='prob', how_to_prune='random', imp_no_rewind=False, imp_resume_round=-1, imp_rewind_iter=1000, imp_rewind_model='short_imp/Liu_checkpoint_model_correct.pth', init='signed_constant', interpolate='prob', invert_sanity_check=True, iter_period=1, iter_start=0, label_smoothing=None, lam_finetune_loss=-1, last_layer_dense=False, lmbda=1e-05, load_ckpt=None, log_dir=None, log_interval=2, loss='cross-entropy-loss', lr=5.0, lr_adjust=50, lr_gamma=0.25, lr_policy='multistep_lr', max_iter=100000, metric='loss', milestones=[50, 100, 150, 200], mixed_precision=0, mode='fan_in', mode_connect=False, mode_connect_filename=None, momentum=0.0, multiprocessing_distributed=False, name=None, nesterov=False, no_bn_decay=False, no_cuda=False, noise=True, noise_ratio=0, nonlinearity='relu', num_round=1, num_step_finetune=10, num_test=1, num_trial=1, num_workers=4, optimizer='sgd', override_prune_rate=False, plot_hc_convergence=False, pretrained=None, pretrained2=None, print_freq=10, project_freq=1, prune_rate=0.5, prune_type='BottomK', pruning_strategy=None, quantize_threshold=0.5, random_subnet=False, rank=-1, regularization='L2', reinit=False, results_filename=None, resume=None, rewind_score=False, rewind_to_epoch=-1, round='naive', run_idx=None, save_every=-1, save_model=False, save_plot_data=False, scale_fan=False, score_init='unif', score_init_constant=None, seed=42, seed_fixed_init=24, shift=0.0, shuffle=False, skip_fine_tune=False, skip_sanity_checks=True, smart_ratio=-1, start_epoch=None, start_from_nothing=False, subfolder='transformer_10_invert', submask_size=1, target_sparsity=10.0, td=0.99, temp=100000, trainer='default', transformer_bptt=35, transformer_clip=0.25, transformer_dropout=0.2, transformer_emsize=200, transformer_nhead=2, transformer_nhid=200, transformer_nlayers=2, trial_num=1, unflag_before_finetune=True, warmup_length=0, wd=0.0005, weight_training=False, width=1.0, width_mult=1.0, workers=4, world_size=-1, **{'compare-rounding': False})
Seeded everything: 42
Use GPU: 2 for training
transformer_data/wikitext-2/train.txt
transformer_data/wikitext-2/valid.txt
transformer_data/wikitext-2/test.txt
==> Conv Type: SubnetConv
==> BN Type: NonAffineBatchNorm
Computing prune_rate for target_sparsity 10.0 with iter_period 1
Setting prune_rate to 0.36904265551980675
| epoch   0 |   200/ 2983 batches | lr 5.00 | ms/batch 54.45 | loss 15.97 | ppl 8604938.76
| epoch   0 |   400/ 2983 batches | lr 5.00 | ms/batch 51.78 | loss 10.49 | ppl 36126.24
| epoch   0 |   600/ 2983 batches | lr 5.00 | ms/batch 55.53 | loss  8.33 | ppl  4148.66
| epoch   0 |   800/ 2983 batches | lr 5.00 | ms/batch 63.46 | loss  7.99 | ppl  2954.24
| epoch   0 |  1000/ 2983 batches | lr 5.00 | ms/batch 62.18 | loss  7.61 | ppl  2020.09
| epoch   0 |  1200/ 2983 batches | lr 5.00 | ms/batch 61.59 | loss  7.65 | ppl  2091.16
| epoch   0 |  1400/ 2983 batches | lr 5.00 | ms/batch 62.84 | loss  7.44 | ppl  1704.49
| epoch   0 |  1600/ 2983 batches | lr 5.00 | ms/batch 61.29 | loss  7.15 | ppl  1276.59
| epoch   0 |  1800/ 2983 batches | lr 5.00 | ms/batch 62.74 | loss  7.34 | ppl  1541.30
| epoch   0 |  2000/ 2983 batches | lr 5.00 | ms/batch 62.28 | loss  7.18 | ppl  1318.77
| epoch   0 |  2200/ 2983 batches | lr 5.00 | ms/batch 60.87 | loss  7.40 | ppl  1630.97
| epoch   0 |  2400/ 2983 batches | lr 5.00 | ms/batch 62.32 | loss  7.33 | ppl  1530.59
| epoch   0 |  2600/ 2983 batches | lr 5.00 | ms/batch 60.56 | loss  7.36 | ppl  1568.33
| epoch   0 |  2800/ 2983 batches | lr 5.00 | ms/batch 62.18 | loss  7.61 | ppl  2011.40
-----------------------------------------------------------------------------------------
| end of epoch   0 | time: 186.87s | valid loss  7.01 | valid ppl  1112.44
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.000342959858244285
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   32007 /   40000 ( 80.02%) | total_pruned =    7993 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   32030 /   40000 ( 80.08%) | total_pruned =    7970 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   19847 /   40000 ( 49.62%) | total_pruned =   20153 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   31654 /   40000 ( 79.14%) | total_pruned =    8346 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   31557 /   40000 ( 78.89%) | total_pruned =    8443 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   32036 /   40000 ( 80.09%) | total_pruned =    7964 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   32022 /   40000 ( 80.06%) | total_pruned =    7978 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   20186 /   40000 ( 50.47%) | total_pruned =   19814 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   31377 /   40000 ( 78.44%) | total_pruned =    8623 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   31355 /   40000 ( 78.39%) | total_pruned =    8645 | shape = (200, 200)
alive: 294071, pruned : 105929, total: 400000, ( 73.52% remained)
Model avg sparsity: 0.7351775
| epoch   1 |   200/ 2983 batches | lr 5.00 | ms/batch 63.68 | loss  8.53 | ppl  5056.64
| epoch   1 |   400/ 2983 batches | lr 5.00 | ms/batch 63.40 | loss  8.14 | ppl  3441.56
| epoch   1 |   600/ 2983 batches | lr 5.00 | ms/batch 62.96 | loss  7.75 | ppl  2327.05
| epoch   1 |   800/ 2983 batches | lr 5.00 | ms/batch 62.44 | loss  7.99 | ppl  2950.55
| epoch   1 |  1000/ 2983 batches | lr 5.00 | ms/batch 60.82 | loss  7.74 | ppl  2305.80
| epoch   1 |  1200/ 2983 batches | lr 5.00 | ms/batch 60.94 | loss  7.90 | ppl  2686.16
| epoch   1 |  1400/ 2983 batches | lr 5.00 | ms/batch 61.84 | loss  7.68 | ppl  2155.16
| epoch   1 |  1600/ 2983 batches | lr 5.00 | ms/batch 61.94 | loss  7.39 | ppl  1627.78
| epoch   1 |  1800/ 2983 batches | lr 5.00 | ms/batch 62.52 | loss  7.55 | ppl  1908.52
| epoch   1 |  2000/ 2983 batches | lr 5.00 | ms/batch 61.07 | loss  7.41 | ppl  1644.81
| epoch   1 |  2200/ 2983 batches | lr 5.00 | ms/batch 60.96 | loss  7.59 | ppl  1975.58
| epoch   1 |  2400/ 2983 batches | lr 5.00 | ms/batch 62.45 | loss  7.53 | ppl  1857.94
| epoch   1 |  2600/ 2983 batches | lr 5.00 | ms/batch 62.67 | loss  7.53 | ppl  1868.99
| epoch   1 |  2800/ 2983 batches | lr 5.00 | ms/batch 62.76 | loss  7.73 | ppl  2283.07
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 193.22s | valid loss  7.07 | valid ppl  1179.21
-----------------------------------------------------------------------------------------
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   32007 /   40000 ( 80.02%) | total_pruned =    7993 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   32030 /   40000 ( 80.08%) | total_pruned =    7970 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   19847 /   40000 ( 49.62%) | total_pruned =   20153 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   31654 /   40000 ( 79.14%) | total_pruned =    8346 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   31557 /   40000 ( 78.89%) | total_pruned =    8443 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   32036 /   40000 ( 80.09%) | total_pruned =    7964 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   32022 /   40000 ( 80.06%) | total_pruned =    7978 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   20186 /   40000 ( 50.47%) | total_pruned =   19814 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   31377 /   40000 ( 78.44%) | total_pruned =    8623 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   31355 /   40000 ( 78.39%) | total_pruned =    8645 | shape = (200, 200)
alive: 294071, pruned : 105929, total: 400000, ( 73.52% remained)
Model avg sparsity: 0.7351775
| epoch   2 |   200/ 2983 batches | lr 5.00 | ms/batch 62.57 | loss  7.63 | ppl  2063.99
| epoch   2 |   400/ 2983 batches | lr 5.00 | ms/batch 62.75 | loss  7.56 | ppl  1912.24
| epoch   2 |   600/ 2983 batches | lr 5.00 | ms/batch 62.68 | loss  7.40 | ppl  1641.37
| epoch   2 |   800/ 2983 batches | lr 5.00 | ms/batch 60.57 | loss  7.73 | ppl  2271.07
| epoch   2 |  1000/ 2983 batches | lr 5.00 | ms/batch 62.18 | loss  7.60 | ppl  1988.36
| epoch   2 |  1200/ 2983 batches | lr 5.00 | ms/batch 62.43 | loss  7.78 | ppl  2401.80
| epoch   2 |  1400/ 2983 batches | lr 5.00 | ms/batch 62.75 | loss  7.57 | ppl  1936.63
| epoch   2 |  1600/ 2983 batches | lr 5.00 | ms/batch 62.98 | loss  7.32 | ppl  1513.18
| epoch   2 |  1800/ 2983 batches | lr 5.00 | ms/batch 59.90 | loss  7.49 | ppl  1797.76
| epoch   2 |  2000/ 2983 batches | lr 5.00 | ms/batch 61.70 | loss  7.39 | ppl  1627.29
| epoch   2 |  2200/ 2983 batches | lr 5.00 | ms/batch 62.41 | loss  7.57 | ppl  1936.50
| epoch   2 |  2400/ 2983 batches | lr 5.00 | ms/batch 62.98 | loss  7.48 | ppl  1769.06
| epoch   2 |  2600/ 2983 batches | lr 5.00 | ms/batch 62.76 | loss  7.52 | ppl  1835.93
| epoch   2 |  2800/ 2983 batches | lr 5.00 | ms/batch 60.47 | loss  7.70 | ppl  2215.70
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 194.98s | valid loss  7.12 | valid ppl  1231.01
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 4.5431887418789785e-11
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   23658 /   40000 ( 59.15%) | total_pruned =   16342 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   23538 /   40000 ( 58.84%) | total_pruned =   16462 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   13170 /   40000 ( 32.92%) | total_pruned =   26830 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   23658 /   40000 ( 59.15%) | total_pruned =   16342 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   23598 /   40000 ( 58.99%) | total_pruned =   16402 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   23694 /   40000 ( 59.23%) | total_pruned =   16306 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   23545 /   40000 ( 58.86%) | total_pruned =   16455 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   15861 /   40000 ( 39.65%) | total_pruned =   24139 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   23540 /   40000 ( 58.85%) | total_pruned =   16460 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   23525 /   40000 ( 58.81%) | total_pruned =   16475 | shape = (200, 200)
alive: 217787, pruned : 182213, total: 400000, ( 54.45% remained)
Model avg sparsity: 0.5444675
| epoch   3 |   200/ 2983 batches | lr 5.00 | ms/batch 62.79 | loss  8.45 | ppl  4675.09
| epoch   3 |   400/ 2983 batches | lr 5.00 | ms/batch 62.68 | loss  8.40 | ppl  4432.45
| epoch   3 |   600/ 2983 batches | lr 5.00 | ms/batch 58.91 | loss  8.21 | ppl  3691.05
| epoch   3 |   800/ 2983 batches | lr 5.00 | ms/batch 62.93 | loss  8.48 | ppl  4796.38
| epoch   3 |  1000/ 2983 batches | lr 5.00 | ms/batch 62.84 | loss  8.31 | ppl  4044.50
| epoch   3 |  1200/ 2983 batches | lr 5.00 | ms/batch 62.75 | loss  8.52 | ppl  5023.51
| epoch   3 |  1400/ 2983 batches | lr 5.00 | ms/batch 62.61 | loss  8.28 | ppl  3957.61
| epoch   3 |  1600/ 2983 batches | lr 5.00 | ms/batch 60.42 | loss  8.00 | ppl  2981.66
| epoch   3 |  1800/ 2983 batches | lr 5.00 | ms/batch 62.58 | loss  8.17 | ppl  3536.66
| epoch   3 |  2000/ 2983 batches | lr 5.00 | ms/batch 63.14 | loss  8.05 | ppl  3123.53
| epoch   3 |  2200/ 2983 batches | lr 5.00 | ms/batch 62.66 | loss  8.28 | ppl  3954.03
| epoch   3 |  2400/ 2983 batches | lr 5.00 | ms/batch 62.52 | loss  8.18 | ppl  3583.48
| epoch   3 |  2600/ 2983 batches | lr 5.00 | ms/batch 60.92 | loss  8.20 | ppl  3641.71
| epoch   3 |  2800/ 2983 batches | lr 5.00 | ms/batch 62.91 | loss  8.34 | ppl  4186.49
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 195.15s | valid loss  7.65 | valid ppl  2106.44
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 1.4203038295164741e-14
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   17484 /   40000 ( 43.71%) | total_pruned =   22516 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   17607 /   40000 ( 44.02%) | total_pruned =   22393 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    9354 /   40000 ( 23.39%) | total_pruned =   30646 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   17701 /   40000 ( 44.25%) | total_pruned =   22299 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   17574 /   40000 ( 43.94%) | total_pruned =   22426 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   17511 /   40000 ( 43.78%) | total_pruned =   22489 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   17639 /   40000 ( 44.10%) | total_pruned =   22361 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   14785 /   40000 ( 36.96%) | total_pruned =   25215 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   17839 /   40000 ( 44.60%) | total_pruned =   22161 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   17549 /   40000 ( 43.87%) | total_pruned =   22451 | shape = (200, 200)
alive: 165043, pruned : 234957, total: 400000, ( 41.26% remained)
Model avg sparsity: 0.4126075
| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 61.15 | loss  8.36 | ppl  4286.77
| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 60.87 | loss  8.38 | ppl  4366.62
| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 62.40 | loss  8.16 | ppl  3494.89
| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 62.82 | loss  8.44 | ppl  4648.55
| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 63.97 | loss  8.35 | ppl  4209.45
| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 61.39 | loss  8.54 | ppl  5118.76
| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch 61.35 | loss  8.36 | ppl  4251.72
| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch 62.76 | loss  8.11 | ppl  3329.77
| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch 63.70 | loss  8.27 | ppl  3898.55
| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch 62.85 | loss  8.14 | ppl  3445.59
| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch 62.01 | loss  8.39 | ppl  4406.11
| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch 61.44 | loss  8.29 | ppl  3999.38
| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch 62.72 | loss  8.35 | ppl  4213.59
| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch 63.22 | loss  8.46 | ppl  4741.86
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 195.31s | valid loss  7.88 | valid ppl  2635.62
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 4.54940031122274e-18
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   13240 /   40000 ( 33.10%) | total_pruned =   26760 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   13336 /   40000 ( 33.34%) | total_pruned =   26664 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    9170 /   40000 ( 22.93%) | total_pruned =   30830 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   13662 /   40000 ( 34.16%) | total_pruned =   26338 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   13514 /   40000 ( 33.78%) | total_pruned =   26486 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   13238 /   40000 ( 33.09%) | total_pruned =   26762 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   13311 /   40000 ( 33.28%) | total_pruned =   26689 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   14536 /   40000 ( 36.34%) | total_pruned =   25464 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   13866 /   40000 ( 34.66%) | total_pruned =   26134 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   13596 /   40000 ( 33.99%) | total_pruned =   26404 | shape = (200, 200)
alive: 131469, pruned : 268531, total: 400000, ( 32.87% remained)
Model avg sparsity: 0.3286725
| epoch   5 |   200/ 2983 batches | lr 5.00 | ms/batch 61.89 | loss  8.36 | ppl  4284.23
| epoch   5 |   400/ 2983 batches | lr 5.00 | ms/batch 62.74 | loss  8.40 | ppl  4466.77
| epoch   5 |   600/ 2983 batches | lr 5.00 | ms/batch 62.02 | loss  8.11 | ppl  3344.03
| epoch   5 |   800/ 2983 batches | lr 5.00 | ms/batch 61.78 | loss  8.47 | ppl  4774.05
| epoch   5 |  1000/ 2983 batches | lr 5.00 | ms/batch 63.07 | loss  8.35 | ppl  4233.35
| epoch   5 |  1200/ 2983 batches | lr 5.00 | ms/batch 61.28 | loss  8.58 | ppl  5334.89
| epoch   5 |  1400/ 2983 batches | lr 5.00 | ms/batch 62.94 | loss  8.35 | ppl  4221.15
| epoch   5 |  1600/ 2983 batches | lr 5.00 | ms/batch 63.13 | loss  8.12 | ppl  3372.38
| epoch   5 |  1800/ 2983 batches | lr 5.00 | ms/batch 61.65 | loss  8.25 | ppl  3823.67
| epoch   5 |  2000/ 2983 batches | lr 5.00 | ms/batch 62.97 | loss  8.13 | ppl  3386.67
| epoch   5 |  2200/ 2983 batches | lr 5.00 | ms/batch 61.53 | loss  8.38 | ppl  4340.49
| epoch   5 |  2400/ 2983 batches | lr 5.00 | ms/batch 63.79 | loss  8.30 | ppl  4035.17
| epoch   5 |  2600/ 2983 batches | lr 5.00 | ms/batch 63.06 | loss  8.36 | ppl  4257.73
| epoch   5 |  2800/ 2983 batches | lr 5.00 | ms/batch 61.42 | loss  8.45 | ppl  4696.30
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 195.24s | valid loss  7.86 | valid ppl  2597.09
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 1.530028971430142e-21
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   10560 /   40000 ( 26.40%) | total_pruned =   29440 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   10593 /   40000 ( 26.48%) | total_pruned =   29407 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    9140 /   40000 ( 22.85%) | total_pruned =   30860 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   11062 /   40000 ( 27.66%) | total_pruned =   28938 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   10945 /   40000 ( 27.36%) | total_pruned =   29055 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   10565 /   40000 ( 26.41%) | total_pruned =   29435 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   10606 /   40000 ( 26.52%) | total_pruned =   29394 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   14495 /   40000 ( 36.24%) | total_pruned =   25505 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   11345 /   40000 ( 28.36%) | total_pruned =   28655 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   11004 /   40000 ( 27.51%) | total_pruned =   28996 | shape = (200, 200)
alive: 110315, pruned : 289685, total: 400000, ( 27.58% remained)
Model avg sparsity: 0.2757875
=========================================================================================
| End of training | test loss  7.79 | test ppl  2425.09
=========================================================================================
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =   10560 /   40000 ( 26.40%) | total_pruned =   29440 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   10593 /   40000 ( 26.48%) | total_pruned =   29407 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    9140 /   40000 ( 22.85%) | total_pruned =   30860 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   11062 /   40000 ( 27.66%) | total_pruned =   28938 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   10945 /   40000 ( 27.36%) | total_pruned =   29055 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   10565 /   40000 ( 26.41%) | total_pruned =   29435 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   10606 /   40000 ( 26.52%) | total_pruned =   29394 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   14495 /   40000 ( 36.24%) | total_pruned =   25505 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   11345 /   40000 ( 28.36%) | total_pruned =   28655 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   11004 /   40000 ( 27.51%) | total_pruned =   28996 | shape = (200, 200)
alive: 110315, pruned : 289685, total: 400000, ( 27.58% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 55.70 | loss  7.12 | ppl  1236.40
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 53.74 | loss  6.77 | ppl   871.14
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 55.52 | loss  6.25 | ppl   518.13
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 54.88 | loss  6.69 | ppl   806.75
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 56.15 | loss  6.39 | ppl   596.48
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 56.55 | loss  6.49 | ppl   657.28
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 55.01 | loss  6.46 | ppl   640.60
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 56.18 | loss  6.01 | ppl   408.82
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 56.24 | loss  6.11 | ppl   448.58
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 54.82 | loss  5.94 | ppl   381.26
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 56.36 | loss  5.94 | ppl   378.34
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 56.20 | loss  6.09 | ppl   439.96
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 54.95 | loss  5.92 | ppl   374.23
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 56.47 | loss  5.88 | ppl   356.14
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 186.71s | valid loss  5.81 | valid ppl   332.07
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 56.06 | loss  5.71 | ppl   301.00
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 53.48 | loss  5.92 | ppl   371.53
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 55.48 | loss  5.45 | ppl   233.25
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 55.52 | loss  5.95 | ppl   385.62
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 54.44 | loss  5.63 | ppl   277.62
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 55.79 | loss  5.84 | ppl   342.78
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 55.63 | loss  5.86 | ppl   349.39
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 54.40 | loss  5.48 | ppl   239.66
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 55.60 | loss  5.52 | ppl   249.16
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 55.54 | loss  5.43 | ppl   227.28
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 54.09 | loss  5.41 | ppl   222.90
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 55.36 | loss  5.58 | ppl   265.01
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 54.39 | loss  5.58 | ppl   264.57
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 54.80 | loss  5.41 | ppl   224.03
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 182.18s | valid loss  5.60 | valid ppl   271.10
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 55.05 | loss  5.31 | ppl   202.09
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 53.57 | loss  5.58 | ppl   263.86
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 53.54 | loss  5.11 | ppl   164.94
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 54.68 | loss  5.56 | ppl   258.63
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 55.41 | loss  5.23 | ppl   187.33
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 54.00 | loss  5.52 | ppl   249.88
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 55.24 | loss  5.54 | ppl   255.16
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 53.45 | loss  5.22 | ppl   184.07
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 55.39 | loss  5.25 | ppl   190.44
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 55.33 | loss  5.11 | ppl   165.00
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 55.31 | loss  5.09 | ppl   162.30
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 54.09 | loss  5.33 | ppl   206.43
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 54.78 | loss  5.40 | ppl   220.43
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 54.28 | loss  5.09 | ppl   161.68
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 179.85s | valid loss  5.51 | valid ppl   246.94
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 54.72 | loss  5.07 | ppl   158.85
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 56.55 | loss  5.37 | ppl   213.83
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 54.32 | loss  4.87 | ppl   129.87
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 55.72 | loss  5.32 | ppl   203.91
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 55.29 | loss  4.92 | ppl   137.23
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 55.27 | loss  5.23 | ppl   187.67
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 53.96 | loss  5.17 | ppl   175.80
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 53.95 | loss  4.75 | ppl   115.56
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 55.27 | loss  4.92 | ppl   136.70
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 55.31 | loss  4.76 | ppl   116.81
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 55.32 | loss  4.83 | ppl   125.23
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 55.25 | loss  4.90 | ppl   134.37
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 53.91 | loss  4.99 | ppl   146.96
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 53.55 | loss  4.68 | ppl   107.64
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 177.55s | valid loss  5.35 | valid ppl   210.32
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 55.94 | loss  4.91 | ppl   136.02
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 55.28 | loss  5.19 | ppl   178.65
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 53.99 | loss  4.73 | ppl   112.88
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 51.74 | loss  5.19 | ppl   179.78
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 47.20 | loss  4.84 | ppl   126.37
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 48.58 | loss  5.16 | ppl   173.49
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 55.11 | loss  5.03 | ppl   153.19
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 53.66 | loss  4.65 | ppl   104.87
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 53.98 | loss  4.83 | ppl   125.62
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 54.88 | loss  4.69 | ppl   108.57
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 55.18 | loss  4.72 | ppl   112.19
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 54.97 | loss  4.85 | ppl   128.16
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 55.12 | loss  4.92 | ppl   136.64
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 52.07 | loss  4.62 | ppl   101.91
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 177.84s | valid loss  5.34 | valid ppl   207.96
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 55.74 | loss  4.81 | ppl   122.47
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 55.14 | loss  5.09 | ppl   162.80
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 52.07 | loss  4.65 | ppl   104.09
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 55.10 | loss  5.07 | ppl   158.81
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 55.26 | loss  4.65 | ppl   105.11
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 55.49 | loss  5.09 | ppl   161.62
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 57.44 | loss  5.01 | ppl   149.37
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 55.24 | loss  4.60 | ppl    99.70
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 52.71 | loss  4.77 | ppl   117.92
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 55.37 | loss  4.64 | ppl   103.80
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 55.45 | loss  4.65 | ppl   104.74
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 56.19 | loss  4.86 | ppl   128.88
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 55.37 | loss  4.85 | ppl   127.98
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 54.76 | loss  4.59 | ppl    98.55
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 184.35s | valid loss  5.33 | valid ppl   206.88
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.24 | test ppl   188.79
=========================================================================================
