nohup: ignoring input
=> Reading YAML config from configs/hypercube/transformer/transformer_base.yml
Namespace(algo='hc_iter', alpha=1.0, alpha_prime=1.0, arch='transformer', batch_size=20, bias=False, bn_type='NonAffineBatchNorm', bottom_k_on_forward=False, checkpoint_at_prune=False, chg_mask=False, chg_weight=False, ckpt_interval=-1, config='configs/hypercube/transformer/transformer_base.yml', conv_type='SubnetConv', data='transformer_data/wikitext-2', data_dir='../data', dataset='MNIST', differentiate_clamp=False, dist_backend='nccl', epochs=6, evaluate=False, evaluate_only=False, fine_tune_lr=5.0, fine_tune_lr_policy='multistep_lr', fine_tune_optimizer='sgd', fine_tune_wd=0.0, first_layer_dense=False, first_layer_type=None, fixed_init=False, flips=None, freeze_weights=True, gamma=0.1, gpu=2, hc_period=1, hc_quantized=True, hc_warmup=9999, hidden_size=500, how_to_connect='prob', how_to_prune='random', imp_no_rewind=False, imp_resume_round=-1, imp_rewind_iter=1000, imp_rewind_model='short_imp/Liu_checkpoint_model_correct.pth', init='signed_constant', interpolate='prob', invert_sanity_check=False, iter_period=1, iter_start=0, label_smoothing=None, lam_finetune_loss=-1, last_layer_dense=False, lmbda=1e-05, load_ckpt=None, log_dir=None, log_interval=2, loss='cross-entropy-loss', lr=5.0, lr_adjust=50, lr_gamma=0.25, lr_policy='multistep_lr', max_iter=100000, metric='loss', milestones=[50, 100, 150, 200], mixed_precision=0, mode='fan_in', mode_connect=False, mode_connect_filename=None, momentum=0.0, multiprocessing_distributed=False, name=None, nesterov=False, no_bn_decay=False, no_cuda=False, noise=True, noise_ratio=0, nonlinearity='relu', num_round=1, num_step_finetune=10, num_test=1, num_trial=1, num_workers=4, optimizer='sgd', override_prune_rate=False, plot_hc_convergence=False, pretrained=None, pretrained2=None, print_freq=10, project_freq=1, prune_rate=0.5, prune_type='BottomK', pruning_strategy=None, quantize_threshold=0.5, random_subnet=False, rank=-1, regularization='L2', reinit=False, results_filename=None, resume=None, rewind_score=False, rewind_to_epoch=-1, round='naive', run_idx=None, save_every=-1, save_model=False, save_plot_data=False, scale_fan=False, score_init='unif', score_init_constant=None, seed=42, seed_fixed_init=24, shift=0.0, shuffle=False, skip_fine_tune=True, skip_sanity_checks=False, smart_ratio=-1, start_epoch=None, start_from_nothing=False, subfolder='transformer_20_sanity', submask_size=1, target_sparsity=20.0, td=0.99, temp=100000, trainer='default', transformer_bptt=35, transformer_clip=0.25, transformer_dropout=0.2, transformer_emsize=200, transformer_nhead=2, transformer_nhid=200, transformer_nlayers=2, trial_num=1, unflag_before_finetune=True, warmup_length=0, wd=0.0005, weight_training=False, width=1.0, width_mult=1.0, workers=4, world_size=-1, **{'compare-rounding': False})
Seeded everything: 42
Use GPU: 2 for training
transformer_data/wikitext-2/train.txt
transformer_data/wikitext-2/valid.txt
transformer_data/wikitext-2/test.txt
==> Conv Type: SubnetConv
==> BN Type: NonAffineBatchNorm
Computing prune_rate for target_sparsity 20.0 with iter_period 1
Setting prune_rate to 0.27522033632230447
| epoch   0 |   200/ 2983 batches | lr 5.00 | ms/batch 80.15 | loss 15.97 | ppl 8604938.76
| epoch   0 |   400/ 2983 batches | lr 5.00 | ms/batch 60.96 | loss 10.49 | ppl 36126.24
| epoch   0 |   600/ 2983 batches | lr 5.00 | ms/batch 62.84 | loss  8.33 | ppl  4148.66
| epoch   0 |   800/ 2983 batches | lr 5.00 | ms/batch 63.47 | loss  7.99 | ppl  2954.24
| epoch   0 |  1000/ 2983 batches | lr 5.00 | ms/batch 63.39 | loss  7.61 | ppl  2020.09
| epoch   0 |  1200/ 2983 batches | lr 5.00 | ms/batch 63.05 | loss  7.65 | ppl  2091.16
| epoch   0 |  1400/ 2983 batches | lr 5.00 | ms/batch 59.25 | loss  7.44 | ppl  1704.49
| epoch   0 |  1600/ 2983 batches | lr 5.00 | ms/batch 63.08 | loss  7.15 | ppl  1276.59
| epoch   0 |  1800/ 2983 batches | lr 5.00 | ms/batch 63.30 | loss  7.34 | ppl  1541.30
| epoch   0 |  2000/ 2983 batches | lr 5.00 | ms/batch 63.03 | loss  7.18 | ppl  1318.77
| epoch   0 |  2200/ 2983 batches | lr 5.00 | ms/batch 63.34 | loss  7.40 | ppl  1630.97
| epoch   0 |  2400/ 2983 batches | lr 5.00 | ms/batch 59.64 | loss  7.33 | ppl  1530.59
| epoch   0 |  2600/ 2983 batches | lr 5.00 | ms/batch 62.17 | loss  7.36 | ppl  1568.33
| epoch   0 |  2800/ 2983 batches | lr 5.00 | ms/batch 62.88 | loss  7.61 | ppl  2011.40
-----------------------------------------------------------------------------------------
| end of epoch   0 | time: 196.46s | valid loss  7.01 | valid ppl  1112.44
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.00014945091970730573
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   26110 /   40000 ( 65.28%) | total_pruned =   13890 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   26008 /   40000 ( 65.02%) | total_pruned =   13992 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   20246 /   40000 ( 50.62%) | total_pruned =   19754 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   25683 /   40000 ( 64.21%) | total_pruned =   14317 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   25839 /   40000 ( 64.60%) | total_pruned =   14161 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   26093 /   40000 ( 65.23%) | total_pruned =   13907 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   25990 /   40000 ( 64.97%) | total_pruned =   14010 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   20266 /   40000 ( 50.66%) | total_pruned =   19734 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   25512 /   40000 ( 63.78%) | total_pruned =   14488 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   25778 /   40000 ( 64.44%) | total_pruned =   14222 | shape = (200, 200)
alive: 247525, pruned : 152475, total: 400000, ( 61.88% remained)
Model avg sparsity: 0.6188125
| epoch   1 |   200/ 2983 batches | lr 5.00 | ms/batch 61.17 | loss  7.43 | ppl  1684.53
| epoch   1 |   400/ 2983 batches | lr 5.00 | ms/batch 63.68 | loss  7.39 | ppl  1623.10
| epoch   1 |   600/ 2983 batches | lr 5.00 | ms/batch 63.91 | loss  7.15 | ppl  1278.73
| epoch   1 |   800/ 2983 batches | lr 5.00 | ms/batch 63.72 | loss  7.62 | ppl  2030.00
| epoch   1 |  1000/ 2983 batches | lr 5.00 | ms/batch 63.88 | loss  7.43 | ppl  1693.70
| epoch   1 |  1200/ 2983 batches | lr 5.00 | ms/batch 60.52 | loss  7.60 | ppl  1997.23
| epoch   1 |  1400/ 2983 batches | lr 5.00 | ms/batch 63.02 | loss  7.44 | ppl  1699.46
| epoch   1 |  1600/ 2983 batches | lr 5.00 | ms/batch 62.84 | loss  7.15 | ppl  1277.83
| epoch   1 |  1800/ 2983 batches | lr 5.00 | ms/batch 59.16 | loss  7.28 | ppl  1452.41
| epoch   1 |  2000/ 2983 batches | lr 5.00 | ms/batch 53.38 | loss  7.23 | ppl  1385.01
| epoch   1 |  2200/ 2983 batches | lr 5.00 | ms/batch 50.38 | loss  7.40 | ppl  1632.74
| epoch   1 |  2400/ 2983 batches | lr 5.00 | ms/batch 53.20 | loss  7.35 | ppl  1558.91
| epoch   1 |  2600/ 2983 batches | lr 5.00 | ms/batch 48.85 | loss  7.35 | ppl  1551.07
| epoch   1 |  2800/ 2983 batches | lr 5.00 | ms/batch 44.92 | loss  7.57 | ppl  1942.02
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 176.78s | valid loss  7.02 | valid ppl  1113.35
-----------------------------------------------------------------------------------------
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   26110 /   40000 ( 65.28%) | total_pruned =   13890 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   26008 /   40000 ( 65.02%) | total_pruned =   13992 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   20246 /   40000 ( 50.62%) | total_pruned =   19754 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   25683 /   40000 ( 64.21%) | total_pruned =   14317 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   25839 /   40000 ( 64.60%) | total_pruned =   14161 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   26093 /   40000 ( 65.23%) | total_pruned =   13907 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   25990 /   40000 ( 64.97%) | total_pruned =   14010 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   20266 /   40000 ( 50.66%) | total_pruned =   19734 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   25512 /   40000 ( 63.78%) | total_pruned =   14488 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   25778 /   40000 ( 64.44%) | total_pruned =   14222 | shape = (200, 200)
alive: 247525, pruned : 152475, total: 400000, ( 61.88% remained)
Model avg sparsity: 0.6188125
| epoch   2 |   200/ 2983 batches | lr 5.00 | ms/batch 44.87 | loss  7.46 | ppl  1734.88
| epoch   2 |   400/ 2983 batches | lr 5.00 | ms/batch 44.96 | loss  7.38 | ppl  1598.76
| epoch   2 |   600/ 2983 batches | lr 5.00 | ms/batch 44.47 | loss  7.20 | ppl  1343.72
| epoch   2 |   800/ 2983 batches | lr 5.00 | ms/batch 42.71 | loss  7.53 | ppl  1862.70
| epoch   2 |  1000/ 2983 batches | lr 5.00 | ms/batch 43.12 | loss  7.44 | ppl  1698.23
| epoch   2 |  1200/ 2983 batches | lr 5.00 | ms/batch 44.25 | loss  7.59 | ppl  1978.48
| epoch   2 |  1400/ 2983 batches | lr 5.00 | ms/batch 44.15 | loss  7.41 | ppl  1655.47
| epoch   2 |  1600/ 2983 batches | lr 5.00 | ms/batch 43.61 | loss  7.14 | ppl  1267.31
| epoch   2 |  1800/ 2983 batches | lr 5.00 | ms/batch 43.34 | loss  7.34 | ppl  1533.19
| epoch   2 |  2000/ 2983 batches | lr 5.00 | ms/batch 38.15 | loss  7.21 | ppl  1354.89
| epoch   2 |  2200/ 2983 batches | lr 5.00 | ms/batch 29.16 | loss  7.39 | ppl  1614.30
| epoch   2 |  2400/ 2983 batches | lr 5.00 | ms/batch 29.75 | loss  7.34 | ppl  1546.19
| epoch   2 |  2600/ 2983 batches | lr 5.00 | ms/batch 26.14 | loss  7.38 | ppl  1603.70
| epoch   2 |  2800/ 2983 batches | lr 5.00 | ms/batch 29.55 | loss  7.56 | ppl  1922.67
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 119.09s | valid loss  7.00 | valid ppl  1101.87
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 4.3554621714791963e-11
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   17333 /   40000 ( 43.33%) | total_pruned =   22667 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   17420 /   40000 ( 43.55%) | total_pruned =   22580 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   19154 /   40000 ( 47.88%) | total_pruned =   20846 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   17280 /   40000 ( 43.20%) | total_pruned =   22720 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   17320 /   40000 ( 43.30%) | total_pruned =   22680 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   17317 /   40000 ( 43.29%) | total_pruned =   22683 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   17403 /   40000 ( 43.51%) | total_pruned =   22597 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   19106 /   40000 ( 47.77%) | total_pruned =   20894 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   17366 /   40000 ( 43.41%) | total_pruned =   22634 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   17386 /   40000 ( 43.47%) | total_pruned =   22614 | shape = (200, 200)
alive: 177085, pruned : 222915, total: 400000, ( 44.27% remained)
Model avg sparsity: 0.4427125
| epoch   3 |   200/ 2983 batches | lr 5.00 | ms/batch 26.95 | loss  7.45 | ppl  1712.76
| epoch   3 |   400/ 2983 batches | lr 5.00 | ms/batch 29.29 | loss  7.36 | ppl  1570.00
| epoch   3 |   600/ 2983 batches | lr 5.00 | ms/batch 29.10 | loss  7.21 | ppl  1355.44
| epoch   3 |   800/ 2983 batches | lr 5.00 | ms/batch 29.67 | loss  7.57 | ppl  1938.65
| epoch   3 |  1000/ 2983 batches | lr 5.00 | ms/batch 28.57 | loss  7.43 | ppl  1682.06
| epoch   3 |  1200/ 2983 batches | lr 5.00 | ms/batch 27.80 | loss  7.60 | ppl  2005.38
| epoch   3 |  1400/ 2983 batches | lr 5.00 | ms/batch 29.52 | loss  7.42 | ppl  1664.87
| epoch   3 |  1600/ 2983 batches | lr 5.00 | ms/batch 29.64 | loss  7.12 | ppl  1235.00
| epoch   3 |  1800/ 2983 batches | lr 5.00 | ms/batch 29.65 | loss  7.31 | ppl  1501.59
| epoch   3 |  2000/ 2983 batches | lr 5.00 | ms/batch 26.16 | loss  7.23 | ppl  1378.85
| epoch   3 |  2200/ 2983 batches | lr 5.00 | ms/batch 29.18 | loss  7.41 | ppl  1646.12
| epoch   3 |  2400/ 2983 batches | lr 5.00 | ms/batch 29.52 | loss  7.33 | ppl  1531.88
| epoch   3 |  2600/ 2983 batches | lr 5.00 | ms/batch 29.44 | loss  7.34 | ppl  1542.95
| epoch   3 |  2800/ 2983 batches | lr 5.00 | ms/batch 27.88 | loss  7.54 | ppl  1890.34
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 89.64s | valid loss  7.02 | valid ppl  1118.54
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 2.346795243486806e-14
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   11104 /   40000 ( 27.76%) | total_pruned =   28896 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   11075 /   40000 ( 27.69%) | total_pruned =   28925 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   18961 /   40000 ( 47.40%) | total_pruned =   21039 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   11303 /   40000 ( 28.26%) | total_pruned =   28697 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   11416 /   40000 ( 28.54%) | total_pruned =   28584 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   11077 /   40000 ( 27.69%) | total_pruned =   28923 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   11036 /   40000 ( 27.59%) | total_pruned =   28964 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   18874 /   40000 ( 47.19%) | total_pruned =   21126 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   11493 /   40000 ( 28.73%) | total_pruned =   28507 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   11566 /   40000 ( 28.91%) | total_pruned =   28434 | shape = (200, 200)
alive: 127905, pruned : 272095, total: 400000, ( 31.98% remained)
Model avg sparsity: 0.3197625
| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 29.52 | loss  7.51 | ppl  1827.41
| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 28.16 | loss  7.33 | ppl  1529.14
| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 27.40 | loss  7.17 | ppl  1296.56
| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 29.49 | loss  7.58 | ppl  1949.63
| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 29.31 | loss  7.40 | ppl  1638.52
| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 29.48 | loss  7.61 | ppl  2009.48
| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch 26.55 | loss  7.38 | ppl  1611.55
| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch 29.59 | loss  7.13 | ppl  1245.77
| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch 29.55 | loss  7.33 | ppl  1528.13
| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch 29.45 | loss  7.19 | ppl  1331.79
| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch 28.20 | loss  7.39 | ppl  1613.98
| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch 27.75 | loss  7.31 | ppl  1499.69
| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch 29.18 | loss  7.38 | ppl  1598.49
| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch 29.39 | loss  7.54 | ppl  1888.10
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 89.97s | valid loss  6.98 | valid ppl  1071.62
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 1.1480369411271404e-17
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =    6573 /   40000 ( 16.43%) | total_pruned =   33427 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    6560 /   40000 ( 16.40%) | total_pruned =   33440 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   18894 /   40000 ( 47.23%) | total_pruned =   21106 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    7019 /   40000 ( 17.55%) | total_pruned =   32981 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    7062 /   40000 ( 17.66%) | total_pruned =   32938 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    6544 /   40000 ( 16.36%) | total_pruned =   33456 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    6539 /   40000 ( 16.35%) | total_pruned =   33461 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   18816 /   40000 ( 47.04%) | total_pruned =   21184 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    7313 /   40000 ( 18.28%) | total_pruned =   32687 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    7254 /   40000 ( 18.14%) | total_pruned =   32746 | shape = (200, 200)
alive: 92574, pruned : 307426, total: 400000, ( 23.14% remained)
Model avg sparsity: 0.231435
| epoch   5 |   200/ 2983 batches | lr 5.00 | ms/batch 29.61 | loss  7.46 | ppl  1742.67
| epoch   5 |   400/ 2983 batches | lr 5.00 | ms/batch 29.51 | loss  7.39 | ppl  1618.95
| epoch   5 |   600/ 2983 batches | lr 5.00 | ms/batch 29.82 | loss  7.16 | ppl  1285.98
| epoch   5 |   800/ 2983 batches | lr 5.00 | ms/batch 26.54 | loss  7.58 | ppl  1963.72
| epoch   5 |  1000/ 2983 batches | lr 5.00 | ms/batch 29.63 | loss  7.44 | ppl  1697.25
| epoch   5 |  1200/ 2983 batches | lr 5.00 | ms/batch 29.14 | loss  7.58 | ppl  1955.70
| epoch   5 |  1400/ 2983 batches | lr 5.00 | ms/batch 29.84 | loss  7.38 | ppl  1604.24
| epoch   5 |  1600/ 2983 batches | lr 5.00 | ms/batch 28.33 | loss  7.12 | ppl  1241.15
| epoch   5 |  1800/ 2983 batches | lr 5.00 | ms/batch 27.71 | loss  7.33 | ppl  1517.88
| epoch   5 |  2000/ 2983 batches | lr 5.00 | ms/batch 29.21 | loss  7.21 | ppl  1358.31
| epoch   5 |  2200/ 2983 batches | lr 5.00 | ms/batch 29.47 | loss  7.41 | ppl  1658.53
| epoch   5 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.20 | loss  7.33 | ppl  1520.93
| epoch   5 |  2600/ 2983 batches | lr 5.00 | ms/batch 26.47 | loss  7.35 | ppl  1552.24
| epoch   5 |  2800/ 2983 batches | lr 5.00 | ms/batch 29.56 | loss  7.54 | ppl  1890.66
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 90.47s | valid loss  7.00 | valid ppl  1095.02
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 5.334189290631489e-21
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =    3247 /   40000 (  8.12%) | total_pruned =   36753 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    3251 /   40000 (  8.13%) | total_pruned =   36749 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   18801 /   40000 ( 47.00%) | total_pruned =   21199 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    3947 /   40000 (  9.87%) | total_pruned =   36053 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    3956 /   40000 (  9.89%) | total_pruned =   36044 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    3230 /   40000 (  8.07%) | total_pruned =   36770 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    3215 /   40000 (  8.04%) | total_pruned =   36785 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   18736 /   40000 ( 46.84%) | total_pruned =   21264 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    4315 /   40000 ( 10.79%) | total_pruned =   35685 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    4211 /   40000 ( 10.53%) | total_pruned =   35789 | shape = (200, 200)
alive: 66909, pruned : 333091, total: 400000, ( 16.73% remained)
Model avg sparsity: 0.1672725
=========================================================================================
| End of training | test loss  6.92 | test ppl  1014.57
=========================================================================================
Beginning Sanity Checks:
Sanity Check 1: Weight Reinit
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =    3247 /   40000 (  8.12%) | total_pruned =   36753 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    3251 /   40000 (  8.13%) | total_pruned =   36749 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   18801 /   40000 ( 47.00%) | total_pruned =   21199 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    3947 /   40000 (  9.87%) | total_pruned =   36053 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    3956 /   40000 (  9.89%) | total_pruned =   36044 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    3230 /   40000 (  8.07%) | total_pruned =   36770 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    3215 /   40000 (  8.04%) | total_pruned =   36785 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   18736 /   40000 ( 46.84%) | total_pruned =   21264 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    4315 /   40000 ( 10.79%) | total_pruned =   35685 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    4211 /   40000 ( 10.53%) | total_pruned =   35789 | shape = (200, 200)
alive: 66909, pruned : 333091, total: 400000, ( 16.73% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 25.50 | loss  7.18 | ppl  1311.62
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 25.53 | loss  6.94 | ppl  1030.44
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 25.48 | loss  6.49 | ppl   656.55
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 25.26 | loss  6.75 | ppl   855.16
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 23.49 | loss  6.46 | ppl   638.09
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 23.42 | loss  6.43 | ppl   619.22
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 25.59 | loss  6.44 | ppl   626.98
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 25.84 | loss  6.05 | ppl   422.14
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 25.31 | loss  6.06 | ppl   428.74
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 25.36 | loss  5.97 | ppl   392.17
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 21.57 | loss  5.97 | ppl   390.97
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 25.39 | loss  6.07 | ppl   434.28
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 25.44 | loss  5.88 | ppl   358.93
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 25.58 | loss  5.92 | ppl   373.93
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 82.86s | valid loss  5.77 | valid ppl   319.57
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 25.54 | loss  5.63 | ppl   278.58
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 25.67 | loss  6.01 | ppl   406.20
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 25.78 | loss  5.42 | ppl   226.37
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 24.87 | loss  5.94 | ppl   378.08
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 21.67 | loss  5.65 | ppl   284.60
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 25.39 | loss  5.79 | ppl   326.01
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 25.38 | loss  5.77 | ppl   320.71
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 25.79 | loss  5.54 | ppl   255.25
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 25.43 | loss  5.55 | ppl   258.29
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 21.57 | loss  5.44 | ppl   231.19
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 25.49 | loss  5.37 | ppl   215.75
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 25.53 | loss  5.54 | ppl   255.28
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 25.45 | loss  5.63 | ppl   279.40
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 25.70 | loss  5.44 | ppl   231.22
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 82.57s | valid loss  5.60 | valid ppl   270.71
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 25.41 | loss  5.25 | ppl   191.33
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 25.57 | loss  5.51 | ppl   245.99
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 25.48 | loss  5.14 | ppl   170.94
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 21.88 | loss  5.56 | ppl   260.12
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 13.78 | loss  5.31 | ppl   202.62
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 11.89 | loss  5.51 | ppl   248.19
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 12.13 | loss  5.54 | ppl   254.71
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 15.38 | loss  5.22 | ppl   184.64
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 23.99 | loss  5.30 | ppl   201.01
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 24.13 | loss  5.08 | ppl   160.33
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 24.53 | loss  5.12 | ppl   168.06
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 24.33 | loss  5.29 | ppl   197.86
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 24.11 | loss  5.36 | ppl   211.86
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 20.02 | loss  5.13 | ppl   169.69
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 70.80s | valid loss  5.55 | valid ppl   256.60
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 24.10 | loss  5.00 | ppl   148.43
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 24.00 | loss  5.28 | ppl   197.15
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 23.95 | loss  4.81 | ppl   122.86
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 20.13 | loss  5.24 | ppl   188.77
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 21.60 | loss  5.00 | ppl   147.68
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 23.90 | loss  5.18 | ppl   177.28
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 23.87 | loss  5.16 | ppl   173.46
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 24.06 | loss  4.78 | ppl   119.44
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 23.93 | loss  4.91 | ppl   136.16
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 23.93 | loss  4.77 | ppl   118.13
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 18.25 | loss  4.80 | ppl   121.78
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 23.88 | loss  4.93 | ppl   138.70
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 23.96 | loss  4.98 | ppl   145.11
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 23.98 | loss  4.67 | ppl   107.22
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 77.37s | valid loss  5.39 | valid ppl   219.21
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 19.71 | loss  4.88 | ppl   131.93
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 23.60 | loss  5.20 | ppl   180.43
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 24.03 | loss  4.69 | ppl   108.35
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 24.13 | loss  5.25 | ppl   191.51
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 24.13 | loss  4.89 | ppl   132.67
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 23.82 | loss  5.07 | ppl   158.67
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 21.27 | loss  5.03 | ppl   152.39
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 20.66 | loss  4.71 | ppl   111.46
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 23.94 | loss  4.79 | ppl   120.23
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 23.79 | loss  4.65 | ppl   104.94
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 23.77 | loss  4.71 | ppl   111.33
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 23.81 | loss  4.83 | ppl   125.61
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 23.87 | loss  4.92 | ppl   136.78
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 20.03 | loss  4.65 | ppl   104.17
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 76.32s | valid loss  5.39 | valid ppl   218.53
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 23.96 | loss  4.81 | ppl   122.33
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 24.15 | loss  5.13 | ppl   169.29
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 23.70 | loss  4.61 | ppl   100.77
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 19.79 | loss  5.11 | ppl   165.73
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 23.01 | loss  4.76 | ppl   116.98
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 23.67 | loss  4.95 | ppl   141.88
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 23.78 | loss  4.97 | ppl   143.87
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 23.68 | loss  4.61 | ppl   100.75
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 23.80 | loss  4.79 | ppl   120.08
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 22.20 | loss  4.64 | ppl   103.32
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 19.65 | loss  4.62 | ppl   101.97
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 23.61 | loss  4.74 | ppl   114.71
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 23.92 | loss  4.83 | ppl   125.09
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 23.73 | loss  4.56 | ppl    95.60
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 77.04s | valid loss  5.38 | valid ppl   216.84
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.29 | test ppl   198.60
=========================================================================================
Sanity Check 2: Mask Reshuffle
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =    3247 /   40000 (  8.12%) | total_pruned =   36753 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    3251 /   40000 (  8.13%) | total_pruned =   36749 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   18801 /   40000 ( 47.00%) | total_pruned =   21199 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    3947 /   40000 (  9.87%) | total_pruned =   36053 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    3956 /   40000 (  9.89%) | total_pruned =   36044 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    3230 /   40000 (  8.07%) | total_pruned =   36770 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    3215 /   40000 (  8.04%) | total_pruned =   36785 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   18736 /   40000 ( 46.84%) | total_pruned =   21264 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    4315 /   40000 ( 10.79%) | total_pruned =   35685 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    4211 /   40000 ( 10.53%) | total_pruned =   35789 | shape = (200, 200)
alive: 66909, pruned : 333091, total: 400000, ( 16.73% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 23.84 | loss  7.16 | ppl  1281.62
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 23.93 | loss  7.10 | ppl  1206.82
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 23.76 | loss  6.35 | ppl   575.20
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 23.80 | loss  6.66 | ppl   779.70
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 23.87 | loss  6.39 | ppl   593.93
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 20.40 | loss  6.43 | ppl   622.31
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 21.32 | loss  6.35 | ppl   574.92
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 23.82 | loss  6.06 | ppl   430.10
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 23.89 | loss  6.17 | ppl   477.75
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 23.90 | loss  5.90 | ppl   364.35
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 23.70 | loss  5.96 | ppl   387.73
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 23.82 | loss  6.00 | ppl   403.78
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 19.48 | loss  5.91 | ppl   370.39
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 21.87 | loss  5.98 | ppl   394.66
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 76.92s | valid loss  5.79 | valid ppl   327.16
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 23.87 | loss  5.73 | ppl   306.67
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 23.60 | loss  5.99 | ppl   399.63
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 19.61 | loss  5.43 | ppl   227.61
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 21.78 | loss  5.93 | ppl   377.71
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 23.84 | loss  5.63 | ppl   279.15
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 23.58 | loss  5.79 | ppl   328.64
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 23.78 | loss  5.80 | ppl   331.80
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 23.60 | loss  5.48 | ppl   238.97
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 22.90 | loss  5.60 | ppl   270.76
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 19.03 | loss  5.35 | ppl   210.73
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 24.15 | loss  5.34 | ppl   207.73
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 23.74 | loss  5.55 | ppl   256.10
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 23.84 | loss  5.57 | ppl   263.47
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 23.57 | loss  5.42 | ppl   225.73
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 76.19s | valid loss  5.58 | valid ppl   265.96
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 23.62 | loss  5.33 | ppl   205.48
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 23.80 | loss  5.53 | ppl   253.19
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 23.70 | loss  5.06 | ppl   157.83
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 23.62 | loss  5.58 | ppl   264.92
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 23.80 | loss  5.25 | ppl   190.96
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 21.17 | loss  5.58 | ppl   266.15
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 20.58 | loss  5.50 | ppl   245.39
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 23.70 | loss  5.13 | ppl   168.37
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 23.96 | loss  5.32 | ppl   203.75
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 23.61 | loss  5.11 | ppl   166.09
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 23.73 | loss  5.10 | ppl   164.20
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 23.69 | loss  5.22 | ppl   185.24
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 19.52 | loss  5.40 | ppl   220.50
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 21.74 | loss  5.16 | ppl   174.21
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 76.67s | valid loss  5.55 | valid ppl   257.26
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 23.89 | loss  5.06 | ppl   157.89
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 23.87 | loss  5.26 | ppl   192.12
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 20.78 | loss  4.79 | ppl   120.20
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 21.67 | loss  5.31 | ppl   202.33
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 23.77 | loss  4.92 | ppl   136.67
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 23.73 | loss  5.24 | ppl   189.17
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 24.30 | loss  5.14 | ppl   170.40
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 23.63 | loss  4.75 | ppl   115.43
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 23.25 | loss  4.95 | ppl   141.41
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 11.59 | loss  4.75 | ppl   116.12
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 11.57 | loss  4.74 | ppl   113.91
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 14.82 | loss  4.89 | ppl   132.52
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 20.12 | loss  4.98 | ppl   145.22
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 23.73 | loss  4.74 | ppl   114.90
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 70.76s | valid loss  5.40 | valid ppl   220.41
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 23.90 | loss  4.94 | ppl   139.19
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 20.87 | loss  5.10 | ppl   163.47
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 19.62 | loss  4.66 | ppl   105.88
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 23.54 | loss  5.19 | ppl   179.34
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 23.42 | loss  4.79 | ppl   120.87
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 23.66 | loss  5.13 | ppl   168.54
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 23.54 | loss  5.03 | ppl   153.14
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 23.68 | loss  4.63 | ppl   102.26
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 19.64 | loss  4.82 | ppl   124.48
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 20.15 | loss  4.72 | ppl   112.31
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 23.48 | loss  4.71 | ppl   110.91
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 23.46 | loss  4.83 | ppl   124.66
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 23.50 | loss  4.98 | ppl   145.37
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 23.47 | loss  4.64 | ppl   103.33
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 74.70s | valid loss  5.40 | valid ppl   220.49
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 21.59 | loss  4.84 | ppl   126.11
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 23.94 | loss  5.06 | ppl   157.23
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 24.14 | loss  4.63 | ppl   102.16
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 24.33 | loss  5.06 | ppl   158.37
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 24.12 | loss  4.73 | ppl   113.26
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 19.21 | loss  5.10 | ppl   164.57
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 24.20 | loss  4.96 | ppl   142.82
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 24.08 | loss  4.53 | ppl    93.21
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 24.11 | loss  4.79 | ppl   120.89
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 24.17 | loss  4.64 | ppl   103.25
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 24.33 | loss  4.60 | ppl    99.72
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 19.38 | loss  4.79 | ppl   120.78
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 24.30 | loss  4.85 | ppl   128.22
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 24.38 | loss  4.61 | ppl   100.60
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 78.17s | valid loss  5.39 | valid ppl   219.24
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.30 | test ppl   200.45
=========================================================================================
