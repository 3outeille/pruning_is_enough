nohup: ignoring input
=> Reading YAML config from configs/hypercube/transformer/transformer_base.yml
Namespace(algo='hc_iter', alpha=1.0, alpha_prime=1.0, arch='transformer', batch_size=20, bias=False, bn_type='NonAffineBatchNorm', bottom_k_on_forward=False, checkpoint_at_prune=False, chg_mask=False, chg_weight=False, ckpt_interval=-1, config='configs/hypercube/transformer/transformer_base.yml', conv_type='SubnetConv', data='transformer_data/wikitext-2', data_dir='../data', dataset='MNIST', differentiate_clamp=False, dist_backend='nccl', epochs=6, evaluate=False, evaluate_only=False, fine_tune_lr=5.0, fine_tune_lr_policy='multistep_lr', fine_tune_optimizer='sgd', fine_tune_wd=0.0, first_layer_dense=False, first_layer_type=None, fixed_init=False, flips=None, freeze_weights=True, gamma=0.1, gpu=1, hc_period=1, hc_quantized=True, hc_warmup=9999, hidden_size=500, how_to_connect='prob', how_to_prune='random', imp_no_rewind=False, imp_resume_round=-1, imp_rewind_iter=1000, imp_rewind_model='short_imp/Liu_checkpoint_model_correct.pth', init='signed_constant', interpolate='prob', invert_sanity_check=False, iter_period=1, iter_start=0, label_smoothing=None, lam_finetune_loss=-1, last_layer_dense=False, lmbda=1e-05, load_ckpt=None, log_dir=None, log_interval=2, loss='cross-entropy-loss', lr=5.0, lr_adjust=50, lr_gamma=0.25, lr_policy='multistep_lr', max_iter=100000, metric='loss', milestones=[50, 100, 150, 200], mixed_precision=0, mode='fan_in', mode_connect=False, mode_connect_filename=None, momentum=0.0, multiprocessing_distributed=False, name=None, nesterov=False, no_bn_decay=False, no_cuda=False, noise=True, noise_ratio=0, nonlinearity='relu', num_round=1, num_step_finetune=10, num_test=1, num_trial=1, num_workers=4, optimizer='sgd', override_prune_rate=False, plot_hc_convergence=False, pretrained=None, pretrained2=None, print_freq=10, project_freq=1, prune_rate=0.5, prune_type='BottomK', pruning_strategy=None, quantize_threshold=0.5, random_subnet=False, rank=-1, regularization='L2', reinit=False, results_filename=None, resume=None, rewind_score=False, rewind_to_epoch=-1, round='naive', run_idx=None, save_every=-1, save_model=False, save_plot_data=False, scale_fan=False, score_init='unif', score_init_constant=None, seed=42, seed_fixed_init=24, shift=0.0, shuffle=False, skip_fine_tune=True, skip_sanity_checks=False, smart_ratio=-1, start_epoch=None, start_from_nothing=False, subfolder='transformer_0_5_sanity', submask_size=1, target_sparsity=0.5, td=0.99, temp=100000, trainer='default', transformer_bptt=35, transformer_clip=0.25, transformer_dropout=0.2, transformer_emsize=200, transformer_nhead=2, transformer_nhid=200, transformer_nlayers=2, trial_num=1, unflag_before_finetune=True, warmup_length=0, wd=0.0005, weight_training=False, width=1.0, width_mult=1.0, workers=4, world_size=-1, **{'compare-rounding': False})
Seeded everything: 42
Use GPU: 1 for training
transformer_data/wikitext-2/train.txt
transformer_data/wikitext-2/valid.txt
transformer_data/wikitext-2/test.txt
==> Conv Type: SubnetConv
==> BN Type: NonAffineBatchNorm
Computing prune_rate for target_sparsity 0.5 with iter_period 1
Setting prune_rate to 0.6534275784224268
| epoch   0 |   200/ 2983 batches | lr 5.00 | ms/batch 21.58 | loss 15.97 | ppl 8604938.76
| epoch   0 |   400/ 2983 batches | lr 5.00 | ms/batch 18.19 | loss 10.49 | ppl 36126.24
| epoch   0 |   600/ 2983 batches | lr 5.00 | ms/batch 17.84 | loss  8.33 | ppl  4148.66
| epoch   0 |   800/ 2983 batches | lr 5.00 | ms/batch 18.18 | loss  7.99 | ppl  2954.24
| epoch   0 |  1000/ 2983 batches | lr 5.00 | ms/batch 18.71 | loss  7.61 | ppl  2020.09
| epoch   0 |  1200/ 2983 batches | lr 5.00 | ms/batch 18.34 | loss  7.65 | ppl  2091.16
| epoch   0 |  1400/ 2983 batches | lr 5.00 | ms/batch 18.97 | loss  7.44 | ppl  1704.49
| epoch   0 |  1600/ 2983 batches | lr 5.00 | ms/batch 18.62 | loss  7.15 | ppl  1276.59
| epoch   0 |  1800/ 2983 batches | lr 5.00 | ms/batch 17.98 | loss  7.34 | ppl  1541.30
| epoch   0 |  2000/ 2983 batches | lr 5.00 | ms/batch 19.28 | loss  7.18 | ppl  1318.77
| epoch   0 |  2200/ 2983 batches | lr 5.00 | ms/batch 18.15 | loss  7.40 | ppl  1630.97
| epoch   0 |  2400/ 2983 batches | lr 5.00 | ms/batch 18.39 | loss  7.33 | ppl  1530.59
| epoch   0 |  2600/ 2983 batches | lr 5.00 | ms/batch 18.75 | loss  7.36 | ppl  1568.33
| epoch   0 |  2800/ 2983 batches | lr 5.00 | ms/batch 19.49 | loss  7.61 | ppl  2011.40
-----------------------------------------------------------------------------------------
| end of epoch   0 | time: 59.06s | valid loss  7.01 | valid ppl  1112.44
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.00035528340958990157
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =    6824 /   40000 ( 17.06%) | total_pruned =   33176 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    6832 /   40000 ( 17.08%) | total_pruned =   33168 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   20143 /   40000 ( 50.36%) | total_pruned =   19857 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    7265 /   40000 ( 18.16%) | total_pruned =   32735 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    7350 /   40000 ( 18.38%) | total_pruned =   32650 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    6837 /   40000 ( 17.09%) | total_pruned =   33163 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    6811 /   40000 ( 17.03%) | total_pruned =   33189 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   19782 /   40000 ( 49.45%) | total_pruned =   20218 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    7593 /   40000 ( 18.98%) | total_pruned =   32407 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    7541 /   40000 ( 18.85%) | total_pruned =   32459 | shape = (200, 200)
alive: 96978, pruned : 303022, total: 400000, ( 24.24% remained)
Model avg sparsity: 0.242445
| epoch   1 |   200/ 2983 batches | lr 5.00 | ms/batch 18.58 | loss  7.43 | ppl  1684.50
| epoch   1 |   400/ 2983 batches | lr 5.00 | ms/batch 18.97 | loss  7.39 | ppl  1625.87
| epoch   1 |   600/ 2983 batches | lr 5.00 | ms/batch 19.34 | loss  7.20 | ppl  1344.18
| epoch   1 |   800/ 2983 batches | lr 5.00 | ms/batch 18.74 | loss  7.61 | ppl  2014.45
| epoch   1 |  1000/ 2983 batches | lr 5.00 | ms/batch 19.19 | loss  7.44 | ppl  1706.91
| epoch   1 |  1200/ 2983 batches | lr 5.00 | ms/batch 18.88 | loss  7.56 | ppl  1924.62
| epoch   1 |  1400/ 2983 batches | lr 5.00 | ms/batch 18.87 | loss  7.45 | ppl  1713.71
| epoch   1 |  1600/ 2983 batches | lr 5.00 | ms/batch 19.20 | loss  7.16 | ppl  1281.70
| epoch   1 |  1800/ 2983 batches | lr 5.00 | ms/batch 18.67 | loss  7.28 | ppl  1452.54
| epoch   1 |  2000/ 2983 batches | lr 5.00 | ms/batch 19.99 | loss  7.21 | ppl  1359.39
| epoch   1 |  2200/ 2983 batches | lr 5.00 | ms/batch 19.44 | loss  7.40 | ppl  1632.09
| epoch   1 |  2400/ 2983 batches | lr 5.00 | ms/batch 19.40 | loss  7.36 | ppl  1574.98
| epoch   1 |  2600/ 2983 batches | lr 5.00 | ms/batch 18.89 | loss  7.33 | ppl  1519.18
| epoch   1 |  2800/ 2983 batches | lr 5.00 | ms/batch 18.65 | loss  7.56 | ppl  1919.21
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 60.23s | valid loss  7.00 | valid ppl  1099.30
-----------------------------------------------------------------------------------------
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =    6824 /   40000 ( 17.06%) | total_pruned =   33176 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    6832 /   40000 ( 17.08%) | total_pruned =   33168 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   20143 /   40000 ( 50.36%) | total_pruned =   19857 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    7265 /   40000 ( 18.16%) | total_pruned =   32735 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    7350 /   40000 ( 18.38%) | total_pruned =   32650 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    6837 /   40000 ( 17.09%) | total_pruned =   33163 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    6811 /   40000 ( 17.03%) | total_pruned =   33189 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   19782 /   40000 ( 49.45%) | total_pruned =   20218 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    7593 /   40000 ( 18.98%) | total_pruned =   32407 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    7541 /   40000 ( 18.85%) | total_pruned =   32459 | shape = (200, 200)
alive: 96978, pruned : 303022, total: 400000, ( 24.24% remained)
Model avg sparsity: 0.242445
| epoch   2 |   200/ 2983 batches | lr 5.00 | ms/batch 19.37 | loss  7.48 | ppl  1771.78
| epoch   2 |   400/ 2983 batches | lr 5.00 | ms/batch 19.04 | loss  7.39 | ppl  1613.43
| epoch   2 |   600/ 2983 batches | lr 5.00 | ms/batch 19.22 | loss  7.24 | ppl  1393.02
| epoch   2 |   800/ 2983 batches | lr 5.00 | ms/batch 19.22 | loss  7.53 | ppl  1870.80
| epoch   2 |  1000/ 2983 batches | lr 5.00 | ms/batch 19.85 | loss  7.43 | ppl  1685.10
| epoch   2 |  1200/ 2983 batches | lr 5.00 | ms/batch 19.53 | loss  7.58 | ppl  1966.85
| epoch   2 |  1400/ 2983 batches | lr 5.00 | ms/batch 19.44 | loss  7.40 | ppl  1636.28
| epoch   2 |  1600/ 2983 batches | lr 5.00 | ms/batch 19.88 | loss  7.12 | ppl  1236.45
| epoch   2 |  1800/ 2983 batches | lr 5.00 | ms/batch 19.50 | loss  7.32 | ppl  1507.27
| epoch   2 |  2000/ 2983 batches | lr 5.00 | ms/batch 19.20 | loss  7.21 | ppl  1349.20
| epoch   2 |  2200/ 2983 batches | lr 5.00 | ms/batch 19.62 | loss  7.37 | ppl  1585.28
| epoch   2 |  2400/ 2983 batches | lr 5.00 | ms/batch 18.87 | loss  7.34 | ppl  1540.67
| epoch   2 |  2600/ 2983 batches | lr 5.00 | ms/batch 19.27 | loss  7.35 | ppl  1549.91
| epoch   2 |  2800/ 2983 batches | lr 5.00 | ms/batch 19.30 | loss  7.57 | ppl  1939.50
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 61.57s | valid loss  6.99 | valid ppl  1082.69
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.0033657867461442947
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   17729 /   40000 ( 44.32%) | total_pruned =   22271 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     685 /   40000 (  1.71%) | total_pruned =   39315 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     684 /   40000 (  1.71%) | total_pruned =   39316 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   11535 /   40000 ( 28.84%) | total_pruned =   28465 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    1277 /   40000 (  3.19%) | total_pruned =   38723 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     840 /   40000 (  2.10%) | total_pruned =   39160 | shape = (200, 200)
alive: 32750, pruned : 367250, total: 400000, (  8.19% remained)
Model avg sparsity: 0.081875
| epoch   3 |   200/ 2983 batches | lr 5.00 | ms/batch 19.73 | loss  7.46 | ppl  1735.47
| epoch   3 |   400/ 2983 batches | lr 5.00 | ms/batch 19.43 | loss  7.37 | ppl  1589.26
| epoch   3 |   600/ 2983 batches | lr 5.00 | ms/batch 19.06 | loss  7.19 | ppl  1330.25
| epoch   3 |   800/ 2983 batches | lr 5.00 | ms/batch 19.60 | loss  7.56 | ppl  1915.81
| epoch   3 |  1000/ 2983 batches | lr 5.00 | ms/batch 19.63 | loss  7.43 | ppl  1690.02
| epoch   3 |  1200/ 2983 batches | lr 5.00 | ms/batch 19.28 | loss  7.59 | ppl  1976.07
| epoch   3 |  1400/ 2983 batches | lr 5.00 | ms/batch 19.39 | loss  7.42 | ppl  1674.71
| epoch   3 |  1600/ 2983 batches | lr 5.00 | ms/batch 19.77 | loss  7.12 | ppl  1236.86
| epoch   3 |  1800/ 2983 batches | lr 5.00 | ms/batch 20.36 | loss  7.31 | ppl  1488.43
| epoch   3 |  2000/ 2983 batches | lr 5.00 | ms/batch 20.36 | loss  7.24 | ppl  1391.51
| epoch   3 |  2200/ 2983 batches | lr 5.00 | ms/batch 19.74 | loss  7.39 | ppl  1613.77
| epoch   3 |  2400/ 2983 batches | lr 5.00 | ms/batch 19.89 | loss  7.34 | ppl  1535.14
| epoch   3 |  2600/ 2983 batches | lr 5.00 | ms/batch 20.11 | loss  7.33 | ppl  1522.89
| epoch   3 |  2800/ 2983 batches | lr 5.00 | ms/batch 20.23 | loss  7.55 | ppl  1905.36
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 62.62s | valid loss  6.97 | valid ppl  1068.31
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.03750195726752281
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    4840 /   40000 ( 12.10%) | total_pruned =   35160 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     665 /   40000 (  1.66%) | total_pruned =   39335 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     551 /   40000 (  1.38%) | total_pruned =   39449 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =    3388 /   40000 (  8.47%) | total_pruned =   36612 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    1108 /   40000 (  2.77%) | total_pruned =   38892 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     788 /   40000 (  1.97%) | total_pruned =   39212 | shape = (200, 200)
alive: 11340, pruned : 388660, total: 400000, (  2.83% remained)
Model avg sparsity: 0.02835
| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 20.03 | loss  7.45 | ppl  1715.04
| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 19.78 | loss  7.36 | ppl  1571.58
| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 20.02 | loss  7.18 | ppl  1313.95
| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 19.97 | loss  7.54 | ppl  1876.13
| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 19.94 | loss  7.41 | ppl  1652.08
| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 19.53 | loss  7.57 | ppl  1945.84
| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch 20.01 | loss  7.37 | ppl  1590.40
| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch 20.58 | loss  7.13 | ppl  1248.21
| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch 19.95 | loss  7.33 | ppl  1522.47
| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch 19.97 | loss  7.25 | ppl  1404.16
| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch 20.22 | loss  7.36 | ppl  1575.80
| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch 19.84 | loss  7.32 | ppl  1515.86
| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch 20.15 | loss  7.34 | ppl  1540.66
| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch 19.56 | loss  7.53 | ppl  1864.91
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 63.00s | valid loss  6.99 | valid ppl  1081.50
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.14972461760044098
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    1589 /   40000 (  3.97%) | total_pruned =   38411 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     554 /   40000 (  1.39%) | total_pruned =   39446 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     228 /   40000 (  0.57%) | total_pruned =   39772 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =     805 /   40000 (  2.01%) | total_pruned =   39195 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =     232 /   40000 (  0.58%) | total_pruned =   39768 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     522 /   40000 (  1.30%) | total_pruned =   39478 | shape = (200, 200)
alive: 3930, pruned : 396070, total: 400000, (  0.98% remained)
Model avg sparsity: 0.009825
| epoch   5 |   200/ 2983 batches | lr 5.00 | ms/batch 19.74 | loss  7.43 | ppl  1688.33
| epoch   5 |   400/ 2983 batches | lr 5.00 | ms/batch 19.72 | loss  7.40 | ppl  1643.52
| epoch   5 |   600/ 2983 batches | lr 5.00 | ms/batch 19.71 | loss  7.16 | ppl  1289.67
| epoch   5 |   800/ 2983 batches | lr 5.00 | ms/batch 19.83 | loss  7.58 | ppl  1950.10
| epoch   5 |  1000/ 2983 batches | lr 5.00 | ms/batch 19.91 | loss  7.44 | ppl  1698.88
| epoch   5 |  1200/ 2983 batches | lr 5.00 | ms/batch 20.09 | loss  7.58 | ppl  1955.54
| epoch   5 |  1400/ 2983 batches | lr 5.00 | ms/batch 20.21 | loss  7.35 | ppl  1562.92
| epoch   5 |  1600/ 2983 batches | lr 5.00 | ms/batch 19.88 | loss  7.14 | ppl  1265.34
| epoch   5 |  1800/ 2983 batches | lr 5.00 | ms/batch 19.60 | loss  7.34 | ppl  1546.19
| epoch   5 |  2000/ 2983 batches | lr 5.00 | ms/batch 19.79 | loss  7.20 | ppl  1344.03
| epoch   5 |  2200/ 2983 batches | lr 5.00 | ms/batch 19.90 | loss  7.41 | ppl  1651.94
| epoch   5 |  2400/ 2983 batches | lr 5.00 | ms/batch 19.64 | loss  7.32 | ppl  1503.50
| epoch   5 |  2600/ 2983 batches | lr 5.00 | ms/batch 20.19 | loss  7.33 | ppl  1525.45
| epoch   5 |  2800/ 2983 batches | lr 5.00 | ms/batch 19.94 | loss  7.54 | ppl  1873.91
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 62.88s | valid loss  6.98 | valid ppl  1078.69
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.3633585572242737
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =     351 /   40000 (  0.88%) | total_pruned =   39649 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     103 /   40000 (  0.26%) | total_pruned =   39897 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     131 /   40000 (  0.33%) | total_pruned =   39869 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =     345 /   40000 (  0.86%) | total_pruned =   39655 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =     187 /   40000 (  0.47%) | total_pruned =   39813 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     245 /   40000 (  0.61%) | total_pruned =   39755 | shape = (200, 200)
alive: 1362, pruned : 398638, total: 400000, (  0.34% remained)
Model avg sparsity: 0.003405
=========================================================================================
| End of training | test loss  6.91 | test ppl  1001.49
=========================================================================================
Beginning Sanity Checks:
Sanity Check 1: Weight Reinit
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =     351 /   40000 (  0.88%) | total_pruned =   39649 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     103 /   40000 (  0.26%) | total_pruned =   39897 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     131 /   40000 (  0.33%) | total_pruned =   39869 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =     345 /   40000 (  0.86%) | total_pruned =   39655 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =     187 /   40000 (  0.47%) | total_pruned =   39813 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     245 /   40000 (  0.61%) | total_pruned =   39755 | shape = (200, 200)
alive: 1362, pruned : 398638, total: 400000, (  0.34% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 16.81 | loss  7.05 | ppl  1152.68
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 16.36 | loss  6.83 | ppl   923.64
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 15.94 | loss  6.38 | ppl   587.42
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 16.65 | loss  6.77 | ppl   868.15
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 16.45 | loss  6.40 | ppl   604.48
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 16.42 | loss  6.51 | ppl   674.76
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 16.93 | loss  6.44 | ppl   626.52
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 16.99 | loss  6.15 | ppl   467.91
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 16.74 | loss  6.12 | ppl   454.31
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 17.00 | loss  6.19 | ppl   488.01
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 17.03 | loss  6.08 | ppl   439.08
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 16.70 | loss  6.23 | ppl   509.55
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 17.06 | loss  6.06 | ppl   427.81
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 17.21 | loss  6.19 | ppl   490.17
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 57.33s | valid loss  6.01 | valid ppl   407.64
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 16.81 | loss  5.79 | ppl   326.25
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 16.96 | loss  6.11 | ppl   452.12
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 16.67 | loss  5.65 | ppl   284.76
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 17.19 | loss  6.08 | ppl   438.86
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 17.16 | loss  5.85 | ppl   348.37
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 17.16 | loss  5.99 | ppl   400.63
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 16.81 | loss  6.13 | ppl   461.17
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 17.41 | loss  5.74 | ppl   311.81
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 17.22 | loss  5.74 | ppl   312.47
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 17.19 | loss  5.72 | ppl   305.86
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 16.90 | loss  5.55 | ppl   258.49
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 16.87 | loss  5.84 | ppl   344.25
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 17.50 | loss  5.88 | ppl   359.59
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 19.05 | loss  5.75 | ppl   314.12
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 58.92s | valid loss  5.83 | valid ppl   339.12
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 16.54 | loss  5.51 | ppl   247.53
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 16.77 | loss  5.76 | ppl   317.31
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 17.30 | loss  5.39 | ppl   218.26
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 16.93 | loss  5.75 | ppl   315.67
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 17.75 | loss  5.63 | ppl   277.28
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 16.58 | loss  5.77 | ppl   319.25
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 16.99 | loss  5.84 | ppl   344.67
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 16.53 | loss  5.51 | ppl   246.83
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 17.30 | loss  5.52 | ppl   250.02
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 17.13 | loss  5.52 | ppl   248.98
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 17.07 | loss  5.38 | ppl   218.05
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 18.23 | loss  5.63 | ppl   280.00
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 17.99 | loss  5.64 | ppl   282.49
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 18.56 | loss  5.61 | ppl   273.28
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 59.01s | valid loss  5.82 | valid ppl   335.90
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 16.88 | loss  5.27 | ppl   195.05
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 16.57 | loss  5.52 | ppl   249.26
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 16.66 | loss  5.14 | ppl   170.61
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 16.96 | loss  5.53 | ppl   252.53
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 18.27 | loss  5.30 | ppl   200.04
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 18.13 | loss  5.43 | ppl   227.78
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 17.71 | loss  5.46 | ppl   234.92
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 17.93 | loss  5.14 | ppl   171.09
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 18.13 | loss  5.17 | ppl   175.60
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 17.88 | loss  5.10 | ppl   164.19
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 17.57 | loss  5.11 | ppl   165.09
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 17.89 | loss  5.31 | ppl   202.61
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 17.72 | loss  5.27 | ppl   193.94
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 17.81 | loss  5.20 | ppl   180.85
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 59.82s | valid loss  5.55 | valid ppl   256.83
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 16.35 | loss  5.19 | ppl   179.19
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 17.44 | loss  5.37 | ppl   215.45
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 17.47 | loss  5.03 | ppl   153.57
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 17.55 | loss  5.50 | ppl   244.31
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 16.99 | loss  5.22 | ppl   184.07
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 17.42 | loss  5.36 | ppl   212.97
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 17.49 | loss  5.38 | ppl   216.99
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 17.43 | loss  5.07 | ppl   159.47
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 17.29 | loss  5.15 | ppl   172.09
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 16.93 | loss  5.07 | ppl   158.68
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 17.37 | loss  4.99 | ppl   146.65
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 17.02 | loss  5.28 | ppl   196.17
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 16.95 | loss  5.26 | ppl   191.73
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 16.87 | loss  5.20 | ppl   181.11
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 58.35s | valid loss  5.54 | valid ppl   253.74
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 16.33 | loss  5.14 | ppl   170.36
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 16.37 | loss  5.32 | ppl   205.12
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 16.78 | loss  4.95 | ppl   141.00
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 17.31 | loss  5.44 | ppl   231.47
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 16.75 | loss  5.10 | ppl   164.75
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 16.88 | loss  5.28 | ppl   195.48
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 16.96 | loss  5.39 | ppl   220.22
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 16.57 | loss  4.97 | ppl   144.36
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 16.81 | loss  5.13 | ppl   168.47
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 16.93 | loss  5.02 | ppl   151.94
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 16.96 | loss  4.99 | ppl   146.48
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 17.07 | loss  5.23 | ppl   186.69
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 16.69 | loss  5.20 | ppl   180.96
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 16.60 | loss  5.13 | ppl   169.67
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 57.32s | valid loss  5.53 | valid ppl   253.07
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.44 | test ppl   229.53
=========================================================================================
Sanity Check 2: Mask Reshuffle
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =     351 /   40000 (  0.88%) | total_pruned =   39649 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     103 /   40000 (  0.26%) | total_pruned =   39897 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     131 /   40000 (  0.33%) | total_pruned =   39869 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =     345 /   40000 (  0.86%) | total_pruned =   39655 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =     187 /   40000 (  0.47%) | total_pruned =   39813 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     245 /   40000 (  0.61%) | total_pruned =   39755 | shape = (200, 200)
alive: 1362, pruned : 398638, total: 400000, (  0.34% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 16.11 | loss  6.74 | ppl   847.14
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 16.18 | loss  6.84 | ppl   936.67
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 16.24 | loss  6.36 | ppl   577.16
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 16.75 | loss  6.63 | ppl   757.31
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 16.64 | loss  6.37 | ppl   584.85
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 16.83 | loss  6.63 | ppl   758.13
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 16.59 | loss  6.54 | ppl   692.45
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 16.28 | loss  6.23 | ppl   510.17
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 16.85 | loss  6.17 | ppl   477.17
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 16.35 | loss  6.21 | ppl   497.52
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 16.89 | loss  6.06 | ppl   427.31
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 16.92 | loss  6.28 | ppl   534.89
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 17.12 | loss  6.09 | ppl   442.46
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 17.17 | loss  6.22 | ppl   503.22
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 56.90s | valid loss  5.99 | valid ppl   400.22
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 16.39 | loss  5.85 | ppl   347.97
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 16.65 | loss  6.19 | ppl   489.66
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 16.40 | loss  5.70 | ppl   299.39
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 16.57 | loss  6.22 | ppl   503.69
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 16.50 | loss  5.88 | ppl   357.11
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 16.80 | loss  6.09 | ppl   443.24
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 16.86 | loss  6.08 | ppl   436.40
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 16.59 | loss  5.81 | ppl   333.85
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 16.73 | loss  5.89 | ppl   360.49
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 16.66 | loss  5.71 | ppl   301.71
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 16.61 | loss  5.56 | ppl   260.95
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 16.26 | loss  5.95 | ppl   385.39
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 16.56 | loss  5.84 | ppl   343.30
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 16.59 | loss  5.88 | ppl   356.81
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 56.74s | valid loss  5.90 | valid ppl   365.50
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 15.98 | loss  5.54 | ppl   255.66
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 17.00 | loss  5.92 | ppl   373.36
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 16.41 | loss  5.40 | ppl   220.97
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 16.77 | loss  5.93 | ppl   374.93
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 16.91 | loss  5.64 | ppl   280.48
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 16.95 | loss  5.75 | ppl   313.94
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 17.03 | loss  5.86 | ppl   349.49
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 16.71 | loss  5.59 | ppl   267.96
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 16.43 | loss  5.53 | ppl   251.21
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 16.53 | loss  5.54 | ppl   254.02
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 16.63 | loss  5.37 | ppl   214.77
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 16.92 | loss  5.77 | ppl   319.21
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 17.12 | loss  5.65 | ppl   284.10
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 16.83 | loss  5.67 | ppl   291.13
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 56.85s | valid loss  5.82 | valid ppl   336.07
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 16.37 | loss  5.29 | ppl   199.26
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 16.28 | loss  5.51 | ppl   248.24
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 16.50 | loss  5.14 | ppl   170.69
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 16.71 | loss  5.67 | ppl   290.50
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 16.54 | loss  5.28 | ppl   195.87
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 16.76 | loss  5.48 | ppl   240.71
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 16.57 | loss  5.47 | ppl   238.44
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 16.59 | loss  5.14 | ppl   170.00
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 16.19 | loss  5.19 | ppl   179.24
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 17.14 | loss  5.10 | ppl   164.77
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 16.31 | loss  5.08 | ppl   161.08
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 16.35 | loss  5.37 | ppl   215.24
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 16.53 | loss  5.34 | ppl   209.20
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 16.53 | loss  5.26 | ppl   193.06
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 56.47s | valid loss  5.55 | valid ppl   257.93
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 16.50 | loss  5.21 | ppl   182.69
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 16.70 | loss  5.39 | ppl   220.01
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 16.91 | loss  5.06 | ppl   158.22
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 16.58 | loss  5.59 | ppl   266.45
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 16.66 | loss  5.20 | ppl   181.36
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 17.07 | loss  5.36 | ppl   212.09
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 17.52 | loss  5.44 | ppl   229.44
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 16.53 | loss  5.08 | ppl   161.56
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 16.88 | loss  5.17 | ppl   175.65
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 16.79 | loss  5.07 | ppl   159.66
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 16.82 | loss  5.02 | ppl   150.79
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 17.04 | loss  5.31 | ppl   202.44
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 17.02 | loss  5.28 | ppl   195.97
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 16.66 | loss  5.20 | ppl   181.68
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 57.53s | valid loss  5.54 | valid ppl   253.83
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 16.49 | loss  5.14 | ppl   171.29
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 16.68 | loss  5.36 | ppl   212.27
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 16.76 | loss  4.96 | ppl   143.03
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 17.04 | loss  5.51 | ppl   246.60
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 16.92 | loss  5.19 | ppl   179.23
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 16.91 | loss  5.38 | ppl   216.76
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 16.88 | loss  5.35 | ppl   211.48
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 16.68 | loss  5.01 | ppl   149.90
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 16.81 | loss  5.15 | ppl   172.96
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 16.92 | loss  4.99 | ppl   147.04
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 16.83 | loss  5.00 | ppl   148.58
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 16.88 | loss  5.27 | ppl   194.76
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 16.30 | loss  5.22 | ppl   184.95
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 16.69 | loss  5.18 | ppl   177.38
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 57.09s | valid loss  5.53 | valid ppl   251.32
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.43 | test ppl   228.65
=========================================================================================
