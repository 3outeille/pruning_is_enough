nohup: ignoring input
=> Reading YAML config from configs/hypercube/transformer/transformer_invert.yml
Namespace(algo='hc_iter', alpha=1.0, alpha_prime=1.0, arch='transformer', batch_size=20, bias=False, bn_type='NonAffineBatchNorm', bottom_k_on_forward=False, checkpoint_at_prune=False, chg_mask=False, chg_weight=False, ckpt_interval=-1, config='configs/hypercube/transformer/transformer_invert.yml', conv_type='SubnetConv', data='transformer_data/wikitext-2', data_dir='../data', dataset='MNIST', differentiate_clamp=False, dist_backend='nccl', epochs=6, evaluate=False, evaluate_only=False, fine_tune_lr=5.0, fine_tune_lr_policy='multistep_lr', fine_tune_optimizer='sgd', fine_tune_wd=0.0, first_layer_dense=False, first_layer_type=None, fixed_init=False, flips=None, freeze_weights=True, gamma=0.1, gpu=0, hc_period=1, hc_quantized=True, hc_warmup=9999, hidden_size=500, how_to_connect='prob', how_to_prune='random', imp_no_rewind=False, imp_resume_round=-1, imp_rewind_iter=1000, imp_rewind_model='short_imp/Liu_checkpoint_model_correct.pth', init='signed_constant', interpolate='prob', invert_sanity_check=True, iter_period=1, iter_start=0, label_smoothing=None, lam_finetune_loss=-1, last_layer_dense=False, lmbda=1e-05, load_ckpt=None, log_dir=None, log_interval=2, loss='cross-entropy-loss', lr=5.0, lr_adjust=50, lr_gamma=0.25, lr_policy='multistep_lr', max_iter=100000, metric='loss', milestones=[50, 100, 150, 200], mixed_precision=0, mode='fan_in', mode_connect=False, mode_connect_filename=None, momentum=0.0, multiprocessing_distributed=False, name=None, nesterov=False, no_bn_decay=False, no_cuda=False, noise=True, noise_ratio=0, nonlinearity='relu', num_round=1, num_step_finetune=10, num_test=1, num_trial=1, num_workers=4, optimizer='sgd', override_prune_rate=False, plot_hc_convergence=False, pretrained=None, pretrained2=None, print_freq=10, project_freq=1, prune_rate=0.5, prune_type='BottomK', pruning_strategy=None, quantize_threshold=0.5, random_subnet=False, rank=-1, regularization='L2', reinit=False, results_filename=None, resume=None, rewind_score=False, rewind_to_epoch=-1, round='naive', run_idx=None, save_every=-1, save_model=False, save_plot_data=False, scale_fan=False, score_init='unif', score_init_constant=None, seed=42, seed_fixed_init=24, shift=0.0, shuffle=False, skip_fine_tune=False, skip_sanity_checks=True, smart_ratio=-1, start_epoch=None, start_from_nothing=False, subfolder='transformer_1_8_invert', submask_size=1, target_sparsity=1.8, td=0.99, temp=100000, trainer='default', transformer_bptt=35, transformer_clip=0.25, transformer_dropout=0.2, transformer_emsize=200, transformer_nhead=2, transformer_nhid=200, transformer_nlayers=2, trial_num=1, unflag_before_finetune=True, warmup_length=0, wd=0.0005, weight_training=False, width=1.0, width_mult=1.0, workers=4, world_size=-1, **{'compare-rounding': False})
Seeded everything: 42
Use GPU: 0 for training
transformer_data/wikitext-2/train.txt
transformer_data/wikitext-2/valid.txt
transformer_data/wikitext-2/test.txt
==> Conv Type: SubnetConv
==> BN Type: NonAffineBatchNorm
Computing prune_rate for target_sparsity 1.8 with iter_period 1
Setting prune_rate to 0.5522305073059569
| epoch   0 |   200/ 2983 batches | lr 5.00 | ms/batch 25.58 | loss 15.97 | ppl 8604938.76
| epoch   0 |   400/ 2983 batches | lr 5.00 | ms/batch 21.90 | loss 10.49 | ppl 36126.24
| epoch   0 |   600/ 2983 batches | lr 5.00 | ms/batch 25.18 | loss  8.33 | ppl  4148.66
| epoch   0 |   800/ 2983 batches | lr 5.00 | ms/batch 30.13 | loss  7.99 | ppl  2954.24
| epoch   0 |  1000/ 2983 batches | lr 5.00 | ms/batch 29.73 | loss  7.61 | ppl  2020.09
| epoch   0 |  1200/ 2983 batches | lr 5.00 | ms/batch 29.93 | loss  7.65 | ppl  2091.16
| epoch   0 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.70 | loss  7.44 | ppl  1704.49
| epoch   0 |  1600/ 2983 batches | lr 5.00 | ms/batch 36.56 | loss  7.15 | ppl  1276.59
| epoch   0 |  1800/ 2983 batches | lr 5.00 | ms/batch 38.36 | loss  7.34 | ppl  1541.30
| epoch   0 |  2000/ 2983 batches | lr 5.00 | ms/batch 38.40 | loss  7.18 | ppl  1318.77
| epoch   0 |  2200/ 2983 batches | lr 5.00 | ms/batch 38.91 | loss  7.40 | ppl  1630.97
| epoch   0 |  2400/ 2983 batches | lr 5.00 | ms/batch 39.35 | loss  7.33 | ppl  1530.59
| epoch   0 |  2600/ 2983 batches | lr 5.00 | ms/batch 39.59 | loss  7.36 | ppl  1568.33
| epoch   0 |  2800/ 2983 batches | lr 5.00 | ms/batch 38.71 | loss  7.61 | ppl  2011.40
-----------------------------------------------------------------------------------------
| end of epoch   0 | time: 104.37s | valid loss  7.01 | valid ppl  1112.44
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.00024383747950196266
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   22703 /   40000 ( 56.76%) | total_pruned =   17297 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   22619 /   40000 ( 56.55%) | total_pruned =   17381 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   19791 /   40000 ( 49.48%) | total_pruned =   20209 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   22753 /   40000 ( 56.88%) | total_pruned =   17247 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   22687 /   40000 ( 56.72%) | total_pruned =   17313 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   22716 /   40000 ( 56.79%) | total_pruned =   17284 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   22621 /   40000 ( 56.55%) | total_pruned =   17379 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   19958 /   40000 ( 49.90%) | total_pruned =   20042 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   22653 /   40000 ( 56.63%) | total_pruned =   17347 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   22631 /   40000 ( 56.58%) | total_pruned =   17369 | shape = (200, 200)
alive: 221132, pruned : 178868, total: 400000, ( 55.28% remained)
Model avg sparsity: 0.55283
| epoch   1 |   200/ 2983 batches | lr 5.00 | ms/batch 39.39 | loss  8.53 | ppl  5056.56
| epoch   1 |   400/ 2983 batches | lr 5.00 | ms/batch 40.23 | loss  8.14 | ppl  3445.54
| epoch   1 |   600/ 2983 batches | lr 5.00 | ms/batch 39.39 | loss  7.75 | ppl  2313.56
| epoch   1 |   800/ 2983 batches | lr 5.00 | ms/batch 38.86 | loss  8.00 | ppl  2968.73
| epoch   1 |  1000/ 2983 batches | lr 5.00 | ms/batch 38.58 | loss  7.75 | ppl  2312.30
| epoch   1 |  1200/ 2983 batches | lr 5.00 | ms/batch 38.77 | loss  7.91 | ppl  2718.40
| epoch   1 |  1400/ 2983 batches | lr 5.00 | ms/batch 40.17 | loss  7.68 | ppl  2171.66
| epoch   1 |  1600/ 2983 batches | lr 5.00 | ms/batch 39.45 | loss  7.40 | ppl  1629.94
| epoch   1 |  1800/ 2983 batches | lr 5.00 | ms/batch 38.89 | loss  7.54 | ppl  1886.81
| epoch   1 |  2000/ 2983 batches | lr 5.00 | ms/batch 39.06 | loss  7.42 | ppl  1662.13
| epoch   1 |  2200/ 2983 batches | lr 5.00 | ms/batch 38.65 | loss  7.59 | ppl  1969.44
| epoch   1 |  2400/ 2983 batches | lr 5.00 | ms/batch 39.53 | loss  7.54 | ppl  1883.14
| epoch   1 |  2600/ 2983 batches | lr 5.00 | ms/batch 39.71 | loss  7.53 | ppl  1859.73
| epoch   1 |  2800/ 2983 batches | lr 5.00 | ms/batch 39.39 | loss  7.72 | ppl  2254.35
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 121.77s | valid loss  7.10 | valid ppl  1211.20
-----------------------------------------------------------------------------------------
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   22703 /   40000 ( 56.76%) | total_pruned =   17297 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   22619 /   40000 ( 56.55%) | total_pruned =   17381 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   19791 /   40000 ( 49.48%) | total_pruned =   20209 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   22753 /   40000 ( 56.88%) | total_pruned =   17247 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   22687 /   40000 ( 56.72%) | total_pruned =   17313 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   22716 /   40000 ( 56.79%) | total_pruned =   17284 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   22621 /   40000 ( 56.55%) | total_pruned =   17379 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   19958 /   40000 ( 49.90%) | total_pruned =   20042 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   22653 /   40000 ( 56.63%) | total_pruned =   17347 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   22631 /   40000 ( 56.58%) | total_pruned =   17369 | shape = (200, 200)
alive: 221132, pruned : 178868, total: 400000, ( 55.28% remained)
Model avg sparsity: 0.55283
| epoch   2 |   200/ 2983 batches | lr 5.00 | ms/batch 39.18 | loss  7.64 | ppl  2081.13
| epoch   2 |   400/ 2983 batches | lr 5.00 | ms/batch 39.28 | loss  7.56 | ppl  1924.40
| epoch   2 |   600/ 2983 batches | lr 5.00 | ms/batch 40.36 | loss  7.44 | ppl  1701.21
| epoch   2 |   800/ 2983 batches | lr 5.00 | ms/batch 38.91 | loss  7.74 | ppl  2305.54
| epoch   2 |  1000/ 2983 batches | lr 5.00 | ms/batch 39.72 | loss  7.59 | ppl  1975.78
| epoch   2 |  1200/ 2983 batches | lr 5.00 | ms/batch 38.39 | loss  7.78 | ppl  2384.45
| epoch   2 |  1400/ 2983 batches | lr 5.00 | ms/batch 39.55 | loss  7.57 | ppl  1945.24
| epoch   2 |  1600/ 2983 batches | lr 5.00 | ms/batch 39.52 | loss  7.33 | ppl  1527.09
| epoch   2 |  1800/ 2983 batches | lr 5.00 | ms/batch 39.42 | loss  7.50 | ppl  1807.59
| epoch   2 |  2000/ 2983 batches | lr 5.00 | ms/batch 38.81 | loss  7.41 | ppl  1651.97
| epoch   2 |  2200/ 2983 batches | lr 5.00 | ms/batch 39.20 | loss  7.55 | ppl  1902.97
| epoch   2 |  2400/ 2983 batches | lr 5.00 | ms/batch 39.19 | loss  7.48 | ppl  1766.60
| epoch   2 |  2600/ 2983 batches | lr 5.00 | ms/batch 40.14 | loss  7.54 | ppl  1872.46
| epoch   2 |  2800/ 2983 batches | lr 5.00 | ms/batch 38.73 | loss  7.71 | ppl  2234.06
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 121.85s | valid loss  7.08 | valid ppl  1187.24
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 2.4169940354701325e-11
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   12578 /   40000 ( 31.45%) | total_pruned =   27422 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   12673 /   40000 ( 31.68%) | total_pruned =   27327 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   13092 /   40000 ( 32.73%) | total_pruned =   26908 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   13029 /   40000 ( 32.57%) | total_pruned =   26971 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   12888 /   40000 ( 32.22%) | total_pruned =   27112 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   12591 /   40000 ( 31.48%) | total_pruned =   27409 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   12667 /   40000 ( 31.67%) | total_pruned =   27333 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   15768 /   40000 ( 39.42%) | total_pruned =   24232 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   13221 /   40000 ( 33.05%) | total_pruned =   26779 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   12948 /   40000 ( 32.37%) | total_pruned =   27052 | shape = (200, 200)
alive: 131455, pruned : 268545, total: 400000, ( 32.86% remained)
Model avg sparsity: 0.3286375
| epoch   3 |   200/ 2983 batches | lr 5.00 | ms/batch 39.31 | loss  8.45 | ppl  4690.17
| epoch   3 |   400/ 2983 batches | lr 5.00 | ms/batch 39.80 | loss  8.41 | ppl  4487.68
| epoch   3 |   600/ 2983 batches | lr 5.00 | ms/batch 39.40 | loss  8.23 | ppl  3769.64
| epoch   3 |   800/ 2983 batches | lr 5.00 | ms/batch 39.00 | loss  8.48 | ppl  4834.91
| epoch   3 |  1000/ 2983 batches | lr 5.00 | ms/batch 39.21 | loss  8.30 | ppl  4028.39
| epoch   3 |  1200/ 2983 batches | lr 5.00 | ms/batch 39.09 | loss  8.52 | ppl  5031.34
| epoch   3 |  1400/ 2983 batches | lr 5.00 | ms/batch 39.42 | loss  8.29 | ppl  3990.42
| epoch   3 |  1600/ 2983 batches | lr 5.00 | ms/batch 39.94 | loss  8.01 | ppl  3024.65
| epoch   3 |  1800/ 2983 batches | lr 5.00 | ms/batch 38.99 | loss  8.19 | ppl  3594.49
| epoch   3 |  2000/ 2983 batches | lr 5.00 | ms/batch 39.52 | loss  8.03 | ppl  3070.29
| epoch   3 |  2200/ 2983 batches | lr 5.00 | ms/batch 39.55 | loss  8.28 | ppl  3944.14
| epoch   3 |  2400/ 2983 batches | lr 5.00 | ms/batch 39.14 | loss  8.20 | ppl  3623.14
| epoch   3 |  2600/ 2983 batches | lr 5.00 | ms/batch 39.23 | loss  8.23 | ppl  3756.98
| epoch   3 |  2800/ 2983 batches | lr 5.00 | ms/batch 39.22 | loss  8.34 | ppl  4195.07
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 121.86s | valid loss  7.68 | valid ppl  2160.25
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 6.07127000922165e-15
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =    7505 /   40000 ( 18.76%) | total_pruned =   32495 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    7580 /   40000 ( 18.95%) | total_pruned =   32420 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    8607 /   40000 ( 21.52%) | total_pruned =   31393 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    8091 /   40000 ( 20.23%) | total_pruned =   31909 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    7989 /   40000 ( 19.97%) | total_pruned =   32011 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    7499 /   40000 ( 18.75%) | total_pruned =   32501 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    7573 /   40000 ( 18.93%) | total_pruned =   32427 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   14564 /   40000 ( 36.41%) | total_pruned =   25436 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    8441 /   40000 ( 21.10%) | total_pruned =   31559 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    8109 /   40000 ( 20.27%) | total_pruned =   31891 | shape = (200, 200)
alive: 85958, pruned : 314042, total: 400000, ( 21.49% remained)
Model avg sparsity: 0.214895
| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 39.57 | loss  8.37 | ppl  4332.37
| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 39.82 | loss  8.38 | ppl  4347.00
| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 40.60 | loss  8.15 | ppl  3457.22
| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 38.88 | loss  8.44 | ppl  4646.11
| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 39.07 | loss  8.34 | ppl  4204.04
| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 38.89 | loss  8.54 | ppl  5128.66
| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch 39.65 | loss  8.35 | ppl  4215.69
| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch 40.57 | loss  8.10 | ppl  3293.29
| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch 39.13 | loss  8.28 | ppl  3962.68
| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch 39.66 | loss  8.14 | ppl  3414.68
| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch 39.37 | loss  8.40 | ppl  4453.09
| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch 39.08 | loss  8.29 | ppl  3982.11
| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch 40.19 | loss  8.35 | ppl  4221.17
| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch 39.67 | loss  8.45 | ppl  4679.36
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 122.80s | valid loss  7.87 | valid ppl  2622.49
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 1.6766505372894858e-18
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =    4887 /   40000 ( 12.22%) | total_pruned =   35113 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    4971 /   40000 ( 12.43%) | total_pruned =   35029 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    8198 /   40000 ( 20.50%) | total_pruned =   31802 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    5619 /   40000 ( 14.05%) | total_pruned =   34381 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    5616 /   40000 ( 14.04%) | total_pruned =   34384 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    4880 /   40000 ( 12.20%) | total_pruned =   35120 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    4924 /   40000 ( 12.31%) | total_pruned =   35076 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   14370 /   40000 ( 35.92%) | total_pruned =   25630 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    6030 /   40000 ( 15.07%) | total_pruned =   33970 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    5759 /   40000 ( 14.40%) | total_pruned =   34241 | shape = (200, 200)
alive: 65254, pruned : 334746, total: 400000, ( 16.31% remained)
Model avg sparsity: 0.163135
| epoch   5 |   200/ 2983 batches | lr 5.00 | ms/batch 39.81 | loss  8.35 | ppl  4216.41
| epoch   5 |   400/ 2983 batches | lr 5.00 | ms/batch 40.12 | loss  8.40 | ppl  4447.23
| epoch   5 |   600/ 2983 batches | lr 5.00 | ms/batch 40.19 | loss  8.12 | ppl  3367.30
| epoch   5 |   800/ 2983 batches | lr 5.00 | ms/batch 38.91 | loss  8.48 | ppl  4808.33
| epoch   5 |  1000/ 2983 batches | lr 5.00 | ms/batch 39.22 | loss  8.34 | ppl  4179.61
| epoch   5 |  1200/ 2983 batches | lr 5.00 | ms/batch 39.01 | loss  8.57 | ppl  5285.07
| epoch   5 |  1400/ 2983 batches | lr 5.00 | ms/batch 38.81 | loss  8.35 | ppl  4213.16
| epoch   5 |  1600/ 2983 batches | lr 5.00 | ms/batch 40.51 | loss  8.12 | ppl  3375.25
| epoch   5 |  1800/ 2983 batches | lr 5.00 | ms/batch 38.77 | loss  8.26 | ppl  3879.11
| epoch   5 |  2000/ 2983 batches | lr 5.00 | ms/batch 39.66 | loss  8.15 | ppl  3459.45
| epoch   5 |  2200/ 2983 batches | lr 5.00 | ms/batch 39.04 | loss  8.39 | ppl  4413.69
| epoch   5 |  2400/ 2983 batches | lr 5.00 | ms/batch 39.32 | loss  8.30 | ppl  4020.86
| epoch   5 |  2600/ 2983 batches | lr 5.00 | ms/batch 40.13 | loss  8.36 | ppl  4265.07
| epoch   5 |  2800/ 2983 batches | lr 5.00 | ms/batch 40.36 | loss  8.45 | ppl  4663.57
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 122.18s | valid loss  7.90 | valid ppl  2707.04
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 5.432374882132484e-22
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =    3735 /   40000 (  9.34%) | total_pruned =   36265 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    3768 /   40000 (  9.42%) | total_pruned =   36232 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    8182 /   40000 ( 20.45%) | total_pruned =   31818 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    4502 /   40000 ( 11.26%) | total_pruned =   35498 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    4513 /   40000 ( 11.28%) | total_pruned =   35487 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    3747 /   40000 (  9.37%) | total_pruned =   36253 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    3731 /   40000 (  9.33%) | total_pruned =   36269 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   14339 /   40000 ( 35.85%) | total_pruned =   25661 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    4919 /   40000 ( 12.30%) | total_pruned =   35081 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    4682 /   40000 ( 11.71%) | total_pruned =   35318 | shape = (200, 200)
alive: 56118, pruned : 343882, total: 400000, ( 14.03% remained)
Model avg sparsity: 0.140295
=========================================================================================
| End of training | test loss  7.84 | test ppl  2533.40
=========================================================================================
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =    3735 /   40000 (  9.34%) | total_pruned =   36265 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    3768 /   40000 (  9.42%) | total_pruned =   36232 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    8182 /   40000 ( 20.45%) | total_pruned =   31818 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    4502 /   40000 ( 11.26%) | total_pruned =   35498 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    4513 /   40000 ( 11.28%) | total_pruned =   35487 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    3747 /   40000 (  9.37%) | total_pruned =   36253 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    3731 /   40000 (  9.33%) | total_pruned =   36269 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   14339 /   40000 ( 35.85%) | total_pruned =   25661 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    4919 /   40000 ( 12.30%) | total_pruned =   35081 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    4682 /   40000 ( 11.71%) | total_pruned =   35318 | shape = (200, 200)
alive: 56118, pruned : 343882, total: 400000, ( 14.03% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 37.09 | loss  7.33 | ppl  1518.08
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 37.73 | loss  7.06 | ppl  1169.53
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 38.97 | loss  6.30 | ppl   543.69
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 36.83 | loss  6.67 | ppl   787.12
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 36.25 | loss  6.42 | ppl   611.01
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 35.35 | loss  6.55 | ppl   697.62
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 34.68 | loss  6.45 | ppl   631.88
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 35.38 | loss  6.09 | ppl   439.34
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 35.76 | loss  6.05 | ppl   423.09
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 35.56 | loss  5.98 | ppl   393.50
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 35.30 | loss  5.93 | ppl   377.32
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 35.04 | loss  6.10 | ppl   443.94
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 33.99 | loss  5.94 | ppl   378.65
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 33.26 | loss  5.91 | ppl   368.66
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 118.24s | valid loss  5.83 | valid ppl   340.29
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 35.80 | loss  5.80 | ppl   331.27
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 35.76 | loss  6.02 | ppl   412.45
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 35.02 | loss  5.49 | ppl   241.09
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 35.73 | loss  5.92 | ppl   370.78
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 35.53 | loss  5.69 | ppl   297.16
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 35.28 | loss  5.87 | ppl   354.45
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 35.30 | loss  5.86 | ppl   351.32
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 35.16 | loss  5.53 | ppl   252.32
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 35.01 | loss  5.54 | ppl   254.79
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 35.60 | loss  5.45 | ppl   232.61
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 35.57 | loss  5.41 | ppl   223.22
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 35.68 | loss  5.61 | ppl   273.97
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 34.83 | loss  5.61 | ppl   272.98
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 35.32 | loss  5.43 | ppl   227.12
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 116.57s | valid loss  5.63 | valid ppl   279.97
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 35.58 | loss  5.38 | ppl   216.28
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 35.90 | loss  5.59 | ppl   267.37
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 34.82 | loss  5.14 | ppl   170.64
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 35.02 | loss  5.58 | ppl   263.82
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 35.77 | loss  5.33 | ppl   207.41
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 35.68 | loss  5.56 | ppl   259.85
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 35.15 | loss  5.58 | ppl   266.40
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 35.25 | loss  5.20 | ppl   181.09
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 34.37 | loss  5.29 | ppl   199.11
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 35.32 | loss  5.16 | ppl   173.83
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 35.48 | loss  5.10 | ppl   163.78
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 35.90 | loss  5.29 | ppl   198.40
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 35.83 | loss  5.42 | ppl   225.93
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 34.41 | loss  5.12 | ppl   166.75
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 116.89s | valid loss  5.53 | valid ppl   251.10
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 35.78 | loss  5.06 | ppl   158.08
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 35.73 | loss  5.36 | ppl   211.87
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 35.49 | loss  4.85 | ppl   127.93
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 34.67 | loss  5.30 | ppl   201.29
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 35.93 | loss  5.02 | ppl   151.91
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 35.85 | loss  5.20 | ppl   181.92
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 35.32 | loss  5.14 | ppl   171.28
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 34.74 | loss  4.81 | ppl   122.78
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 34.56 | loss  4.95 | ppl   140.54
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 35.73 | loss  4.84 | ppl   127.03
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 35.77 | loss  4.85 | ppl   127.79
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 35.51 | loss  4.94 | ppl   140.12
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 35.20 | loss  5.05 | ppl   155.58
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 34.92 | loss  4.72 | ppl   112.52
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 117.24s | valid loss  5.38 | valid ppl   216.10
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 35.71 | loss  4.95 | ppl   140.60
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 35.53 | loss  5.17 | ppl   175.63
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 34.97 | loss  4.74 | ppl   113.93
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 34.75 | loss  5.24 | ppl   188.49
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 35.39 | loss  4.92 | ppl   137.21
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 35.26 | loss  5.14 | ppl   169.87
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 35.67 | loss  5.07 | ppl   159.04
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 35.81 | loss  4.71 | ppl   111.05
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 34.87 | loss  4.87 | ppl   129.95
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 35.36 | loss  4.76 | ppl   116.18
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 35.77 | loss  4.71 | ppl   111.07
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 35.44 | loss  4.87 | ppl   130.38
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 35.69 | loss  4.94 | ppl   139.38
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 34.76 | loss  4.68 | ppl   107.96
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 116.99s | valid loss  5.36 | valid ppl   213.27
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 36.07 | loss  4.83 | ppl   125.23
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 36.05 | loss  5.12 | ppl   168.11
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 35.82 | loss  4.65 | ppl   104.63
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 34.82 | loss  5.13 | ppl   169.34
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 35.80 | loss  4.77 | ppl   118.37
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 35.69 | loss  5.07 | ppl   158.68
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 35.19 | loss  4.98 | ppl   145.09
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 35.21 | loss  4.60 | ppl    99.93
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 35.46 | loss  4.79 | ppl   119.74
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 35.75 | loss  4.70 | ppl   109.44
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 35.56 | loss  4.69 | ppl   109.40
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 35.67 | loss  4.89 | ppl   133.27
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 36.02 | loss  4.96 | ppl   142.37
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 34.80 | loss  4.59 | ppl    98.82
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 116.53s | valid loss  5.36 | valid ppl   212.77
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.27 | test ppl   194.88
=========================================================================================
