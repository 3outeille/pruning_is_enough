nohup: ignoring input
=> Reading YAML config from configs/hypercube/transformer/transformer_base.yml
Namespace(algo='hc_iter', alpha=1.0, alpha_prime=1.0, arch='transformer', batch_size=20, bias=False, bn_type='NonAffineBatchNorm', bottom_k_on_forward=False, checkpoint_at_prune=False, chg_mask=False, chg_weight=False, ckpt_interval=-1, config='configs/hypercube/transformer/transformer_base.yml', conv_type='SubnetConv', data='transformer_data/wikitext-2', data_dir='../data', dataset='MNIST', differentiate_clamp=False, dist_backend='nccl', epochs=6, evaluate=False, evaluate_only=False, fine_tune_lr=5.0, fine_tune_lr_policy='multistep_lr', fine_tune_optimizer='sgd', fine_tune_wd=0.0, first_layer_dense=False, first_layer_type=None, fixed_init=False, flips=None, freeze_weights=True, gamma=0.1, gpu=0, hc_period=1, hc_quantized=True, hc_warmup=9999, hidden_size=500, how_to_connect='prob', how_to_prune='random', imp_no_rewind=False, imp_resume_round=-1, imp_rewind_iter=1000, imp_rewind_model='short_imp/Liu_checkpoint_model_correct.pth', init='signed_constant', interpolate='prob', invert_sanity_check=False, iter_period=1, iter_start=0, label_smoothing=None, lam_finetune_loss=-1, last_layer_dense=False, lmbda=1e-05, load_ckpt=None, log_dir=None, log_interval=2, loss='cross-entropy-loss', lr=5.0, lr_adjust=50, lr_gamma=0.25, lr_policy='multistep_lr', max_iter=100000, metric='loss', milestones=[50, 100, 150, 200], mixed_precision=0, mode='fan_in', mode_connect=False, mode_connect_filename=None, momentum=0.0, multiprocessing_distributed=False, name=None, nesterov=False, no_bn_decay=False, no_cuda=False, noise=True, noise_ratio=0, nonlinearity='relu', num_round=1, num_step_finetune=10, num_test=1, num_trial=1, num_workers=4, optimizer='sgd', override_prune_rate=False, plot_hc_convergence=False, pretrained=None, pretrained2=None, print_freq=10, project_freq=1, prune_rate=0.5, prune_type='BottomK', pruning_strategy=None, quantize_threshold=0.5, random_subnet=False, rank=-1, regularization='L2', reinit=False, results_filename=None, resume=None, rewind_score=False, rewind_to_epoch=-1, round='naive', run_idx=None, save_every=-1, save_model=False, save_plot_data=False, scale_fan=False, score_init='unif', score_init_constant=None, seed=42, seed_fixed_init=24, shift=0.0, shuffle=False, skip_fine_tune=True, skip_sanity_checks=False, smart_ratio=-1, start_epoch=None, start_from_nothing=False, subfolder='transformer_5_sanity', submask_size=1, target_sparsity=5.0, td=0.99, temp=100000, trainer='default', transformer_bptt=35, transformer_clip=0.25, transformer_dropout=0.2, transformer_emsize=200, transformer_nhead=2, transformer_nhid=200, transformer_nlayers=2, trial_num=1, unflag_before_finetune=True, warmup_length=0, wd=0.0005, weight_training=False, width=1.0, width_mult=1.0, workers=4, world_size=-1, **{'compare-rounding': False})
Seeded everything: 42
Use GPU: 0 for training
transformer_data/wikitext-2/train.txt
transformer_data/wikitext-2/valid.txt
transformer_data/wikitext-2/test.txt
==> Conv Type: SubnetConv
==> BN Type: NonAffineBatchNorm
Computing prune_rate for target_sparsity 5.0 with iter_period 1
Setting prune_rate to 0.4507197283469411
| epoch   0 |   200/ 2983 batches | lr 5.00 | ms/batch 26.95 | loss 15.97 | ppl 8604938.76
| epoch   0 |   400/ 2983 batches | lr 5.00 | ms/batch 21.81 | loss 10.49 | ppl 36126.24
| epoch   0 |   600/ 2983 batches | lr 5.00 | ms/batch 22.58 | loss  8.33 | ppl  4148.66
| epoch   0 |   800/ 2983 batches | lr 5.00 | ms/batch 25.78 | loss  7.99 | ppl  2954.24
| epoch   0 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.38 | loss  7.61 | ppl  2020.09
| epoch   0 |  1200/ 2983 batches | lr 5.00 | ms/batch 29.95 | loss  7.65 | ppl  2091.16
| epoch   0 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.51 | loss  7.44 | ppl  1704.49
| epoch   0 |  1600/ 2983 batches | lr 5.00 | ms/batch 29.98 | loss  7.15 | ppl  1276.59
| epoch   0 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.09 | loss  7.34 | ppl  1541.30
| epoch   0 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.02 | loss  7.18 | ppl  1318.77
| epoch   0 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.31 | loss  7.40 | ppl  1630.97
| epoch   0 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.35 | loss  7.33 | ppl  1530.59
| epoch   0 |  2600/ 2983 batches | lr 5.00 | ms/batch 30.21 | loss  7.36 | ppl  1568.33
| epoch   0 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.13 | loss  7.61 | ppl  2011.40
-----------------------------------------------------------------------------------------
| end of epoch   0 | time: 89.23s | valid loss  7.01 | valid ppl  1112.44
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.0002454716304782778
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   17151 /   40000 ( 42.88%) | total_pruned =   22849 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   17225 /   40000 ( 43.06%) | total_pruned =   22775 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   20209 /   40000 ( 50.52%) | total_pruned =   19791 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   17113 /   40000 ( 42.78%) | total_pruned =   22887 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   17159 /   40000 ( 42.90%) | total_pruned =   22841 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   17141 /   40000 ( 42.85%) | total_pruned =   22859 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   17213 /   40000 ( 43.03%) | total_pruned =   22787 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   20041 /   40000 ( 50.10%) | total_pruned =   19959 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   17200 /   40000 ( 43.00%) | total_pruned =   22800 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   17237 /   40000 ( 43.09%) | total_pruned =   22763 | shape = (200, 200)
alive: 177689, pruned : 222311, total: 400000, ( 44.42% remained)
Model avg sparsity: 0.4442225
| epoch   1 |   200/ 2983 batches | lr 5.00 | ms/batch 30.15 | loss  7.43 | ppl  1684.53
| epoch   1 |   400/ 2983 batches | lr 5.00 | ms/batch 30.29 | loss  7.39 | ppl  1623.10
| epoch   1 |   600/ 2983 batches | lr 5.00 | ms/batch 30.36 | loss  7.15 | ppl  1278.73
| epoch   1 |   800/ 2983 batches | lr 5.00 | ms/batch 30.53 | loss  7.62 | ppl  2030.00
| epoch   1 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.30 | loss  7.43 | ppl  1693.70
| epoch   1 |  1200/ 2983 batches | lr 5.00 | ms/batch 30.29 | loss  7.60 | ppl  1997.23
| epoch   1 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.50 | loss  7.44 | ppl  1699.46
| epoch   1 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.34 | loss  7.15 | ppl  1277.83
| epoch   1 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.43 | loss  7.28 | ppl  1446.81
| epoch   1 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.38 | loss  7.21 | ppl  1353.58
| epoch   1 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.16 | loss  7.41 | ppl  1659.99
| epoch   1 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.59 | loss  7.36 | ppl  1573.47
| epoch   1 |  2600/ 2983 batches | lr 5.00 | ms/batch 30.30 | loss  7.36 | ppl  1566.30
| epoch   1 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.21 | loss  7.56 | ppl  1925.90
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 94.52s | valid loss  6.97 | valid ppl  1068.82
-----------------------------------------------------------------------------------------
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =   17151 /   40000 ( 42.88%) | total_pruned =   22849 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =   17225 /   40000 ( 43.06%) | total_pruned =   22775 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   20209 /   40000 ( 50.52%) | total_pruned =   19791 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =   17113 /   40000 ( 42.78%) | total_pruned =   22887 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =   17159 /   40000 ( 42.90%) | total_pruned =   22841 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =   17141 /   40000 ( 42.85%) | total_pruned =   22859 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =   17213 /   40000 ( 43.03%) | total_pruned =   22787 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   20041 /   40000 ( 50.10%) | total_pruned =   19959 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =   17200 /   40000 ( 43.00%) | total_pruned =   22800 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =   17237 /   40000 ( 43.09%) | total_pruned =   22763 | shape = (200, 200)
alive: 177689, pruned : 222311, total: 400000, ( 44.42% remained)
Model avg sparsity: 0.4442225
| epoch   2 |   200/ 2983 batches | lr 5.00 | ms/batch 30.67 | loss  7.46 | ppl  1730.19
| epoch   2 |   400/ 2983 batches | lr 5.00 | ms/batch 30.44 | loss  7.38 | ppl  1603.10
| epoch   2 |   600/ 2983 batches | lr 5.00 | ms/batch 30.39 | loss  7.22 | ppl  1360.98
| epoch   2 |   800/ 2983 batches | lr 5.00 | ms/batch 29.92 | loss  7.53 | ppl  1864.67
| epoch   2 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.48 | loss  7.43 | ppl  1686.83
| epoch   2 |  1200/ 2983 batches | lr 5.00 | ms/batch 30.28 | loss  7.59 | ppl  1977.15
| epoch   2 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.37 | loss  7.43 | ppl  1680.78
| epoch   2 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.46 | loss  7.13 | ppl  1248.64
| epoch   2 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.37 | loss  7.31 | ppl  1498.99
| epoch   2 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.45 | loss  7.21 | ppl  1353.68
| epoch   2 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.44 | loss  7.37 | ppl  1593.43
| epoch   2 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.63 | loss  7.35 | ppl  1557.79
| epoch   2 |  2600/ 2983 batches | lr 5.00 | ms/batch 30.41 | loss  7.35 | ppl  1551.20
| epoch   2 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.50 | loss  7.56 | ppl  1921.50
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 94.59s | valid loss  6.97 | valid ppl  1068.33
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 6.348573750036834e-11
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =    6854 /   40000 ( 17.14%) | total_pruned =   33146 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    6880 /   40000 ( 17.20%) | total_pruned =   33120 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   19168 /   40000 ( 47.92%) | total_pruned =   20832 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    7307 /   40000 ( 18.27%) | total_pruned =   32693 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    7362 /   40000 ( 18.41%) | total_pruned =   32638 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    6866 /   40000 ( 17.16%) | total_pruned =   33134 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    6845 /   40000 ( 17.11%) | total_pruned =   33155 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   19043 /   40000 ( 47.61%) | total_pruned =   20957 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    7624 /   40000 ( 19.06%) | total_pruned =   32376 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    7554 /   40000 ( 18.89%) | total_pruned =   32446 | shape = (200, 200)
alive: 95503, pruned : 304497, total: 400000, ( 23.88% remained)
Model avg sparsity: 0.2387575
| epoch   3 |   200/ 2983 batches | lr 5.00 | ms/batch 30.26 | loss  7.46 | ppl  1737.59
| epoch   3 |   400/ 2983 batches | lr 5.00 | ms/batch 30.38 | loss  7.41 | ppl  1652.11
| epoch   3 |   600/ 2983 batches | lr 5.00 | ms/batch 29.75 | loss  7.20 | ppl  1332.97
| epoch   3 |   800/ 2983 batches | lr 5.00 | ms/batch 30.72 | loss  7.57 | ppl  1934.48
| epoch   3 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.45 | loss  7.43 | ppl  1688.68
| epoch   3 |  1200/ 2983 batches | lr 5.00 | ms/batch 30.34 | loss  7.60 | ppl  2000.62
| epoch   3 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.42 | loss  7.41 | ppl  1657.10
| epoch   3 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.27 | loss  7.12 | ppl  1240.65
| epoch   3 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.89 | loss  7.30 | ppl  1484.10
| epoch   3 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.65 | loss  7.20 | ppl  1336.80
| epoch   3 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.50 | loss  7.40 | ppl  1640.55
| epoch   3 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.12 | loss  7.34 | ppl  1535.04
| epoch   3 |  2600/ 2983 batches | lr 5.00 | ms/batch 30.11 | loss  7.35 | ppl  1549.75
| epoch   3 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.27 | loss  7.57 | ppl  1941.13
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 94.48s | valid loss  6.98 | valid ppl  1079.32
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 3.143564239211499e-14
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =    1248 /   40000 (  3.12%) | total_pruned =   38752 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =    1206 /   40000 (  3.02%) | total_pruned =   38794 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   18969 /   40000 ( 47.42%) | total_pruned =   21031 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =    2204 /   40000 (  5.51%) | total_pruned =   37796 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =    2207 /   40000 (  5.52%) | total_pruned =   37793 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =    1211 /   40000 (  3.03%) | total_pruned =   38789 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =    1117 /   40000 (  2.79%) | total_pruned =   38883 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =   18839 /   40000 ( 47.10%) | total_pruned =   21161 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    2589 /   40000 (  6.47%) | total_pruned =   37411 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =    2443 /   40000 (  6.11%) | total_pruned =   37557 | shape = (200, 200)
alive: 52033, pruned : 347967, total: 400000, ( 13.01% remained)
Model avg sparsity: 0.1300825
| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 30.43 | loss  7.51 | ppl  1831.09
| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 30.20 | loss  7.34 | ppl  1542.96
| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 29.77 | loss  7.17 | ppl  1298.89
| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 31.18 | loss  7.53 | ppl  1859.02
| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.24 | loss  7.40 | ppl  1640.14
| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 30.25 | loss  7.60 | ppl  1991.15
| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch 30.37 | loss  7.37 | ppl  1587.51
| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.08 | loss  7.14 | ppl  1266.19
| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch 29.96 | loss  7.34 | ppl  1539.21
| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.10 | loss  7.19 | ppl  1329.44
| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.08 | loss  7.39 | ppl  1615.06
| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch 30.21 | loss  7.32 | ppl  1514.00
| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch 30.36 | loss  7.35 | ppl  1561.80
| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch 30.31 | loss  7.54 | ppl  1884.06
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 94.01s | valid loss  7.03 | valid ppl  1133.49
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.006324671674519777
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =   15817 /   40000 ( 39.54%) | total_pruned =   24183 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     685 /   40000 (  1.71%) | total_pruned =   39315 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     679 /   40000 (  1.70%) | total_pruned =   39321 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =    9316 /   40000 ( 23.29%) | total_pruned =   30684 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    1259 /   40000 (  3.15%) | total_pruned =   38741 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     820 /   40000 (  2.05%) | total_pruned =   39180 | shape = (200, 200)
alive: 28576, pruned : 371424, total: 400000, (  7.14% remained)
Model avg sparsity: 0.07144
| epoch   5 |   200/ 2983 batches | lr 5.00 | ms/batch 30.53 | loss  7.45 | ppl  1717.92
| epoch   5 |   400/ 2983 batches | lr 5.00 | ms/batch 30.10 | loss  7.40 | ppl  1630.71
| epoch   5 |   600/ 2983 batches | lr 5.00 | ms/batch 29.89 | loss  7.16 | ppl  1285.09
| epoch   5 |   800/ 2983 batches | lr 5.00 | ms/batch 30.28 | loss  7.59 | ppl  1984.16
| epoch   5 |  1000/ 2983 batches | lr 5.00 | ms/batch 30.21 | loss  7.42 | ppl  1677.17
| epoch   5 |  1200/ 2983 batches | lr 5.00 | ms/batch 29.90 | loss  7.57 | ppl  1936.86
| epoch   5 |  1400/ 2983 batches | lr 5.00 | ms/batch 29.93 | loss  7.39 | ppl  1618.21
| epoch   5 |  1600/ 2983 batches | lr 5.00 | ms/batch 30.12 | loss  7.12 | ppl  1231.68
| epoch   5 |  1800/ 2983 batches | lr 5.00 | ms/batch 30.14 | loss  7.33 | ppl  1521.37
| epoch   5 |  2000/ 2983 batches | lr 5.00 | ms/batch 30.25 | loss  7.20 | ppl  1335.67
| epoch   5 |  2200/ 2983 batches | lr 5.00 | ms/batch 30.91 | loss  7.43 | ppl  1679.75
| epoch   5 |  2400/ 2983 batches | lr 5.00 | ms/batch 29.61 | loss  7.35 | ppl  1561.71
| epoch   5 |  2600/ 2983 batches | lr 5.00 | ms/batch 29.25 | loss  7.33 | ppl  1527.58
| epoch   5 |  2800/ 2983 batches | lr 5.00 | ms/batch 28.65 | loss  7.53 | ppl  1857.59
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 92.69s | valid loss  6.98 | valid ppl  1077.29
-----------------------------------------------------------------------------------------
Pruning Model:
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 0.02236262895166874
Rounding model with scheme: naive
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    7576 /   40000 ( 18.94%) | total_pruned =   32424 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     675 /   40000 (  1.69%) | total_pruned =   39325 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     612 /   40000 (  1.53%) | total_pruned =   39388 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =    4864 /   40000 ( 12.16%) | total_pruned =   35136 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    1167 /   40000 (  2.92%) | total_pruned =   38833 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     802 /   40000 (  2.00%) | total_pruned =   39198 | shape = (200, 200)
alive: 15696, pruned : 384304, total: 400000, (  3.92% remained)
Model avg sparsity: 0.03924
=========================================================================================
| End of training | test loss  6.91 | test ppl   998.09
=========================================================================================
Beginning Sanity Checks:
Sanity Check 1: Weight Reinit
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    7576 /   40000 ( 18.94%) | total_pruned =   32424 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     675 /   40000 (  1.69%) | total_pruned =   39325 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     612 /   40000 (  1.53%) | total_pruned =   39388 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =    4864 /   40000 ( 12.16%) | total_pruned =   35136 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    1167 /   40000 (  2.92%) | total_pruned =   38833 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     802 /   40000 (  2.00%) | total_pruned =   39198 | shape = (200, 200)
alive: 15696, pruned : 384304, total: 400000, (  3.92% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 27.99 | loss  7.06 | ppl  1168.65
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 27.44 | loss  6.87 | ppl   963.74
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 26.86 | loss  6.27 | ppl   530.10
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 27.04 | loss  6.75 | ppl   849.99
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 26.99 | loss  6.40 | ppl   602.37
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 27.08 | loss  6.44 | ppl   627.89
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 27.01 | loss  6.34 | ppl   567.95
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 26.98 | loss  6.05 | ppl   422.73
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 27.07 | loss  6.13 | ppl   459.14
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 26.88 | loss  6.12 | ppl   456.47
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 26.58 | loss  6.05 | ppl   424.14
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 27.30 | loss  6.16 | ppl   473.80
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 27.01 | loss  6.04 | ppl   419.94
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 27.30 | loss  6.09 | ppl   439.75
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 89.36s | valid loss  5.89 | valid ppl   362.59
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 27.08 | loss  5.76 | ppl   318.79
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 27.00 | loss  6.08 | ppl   437.16
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 26.97 | loss  5.58 | ppl   264.40
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 26.88 | loss  6.01 | ppl   408.72
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 26.23 | loss  5.74 | ppl   309.59
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 27.09 | loss  5.89 | ppl   360.97
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 27.09 | loss  5.96 | ppl   389.39
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 27.09 | loss  5.64 | ppl   282.18
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 27.11 | loss  5.69 | ppl   296.18
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 26.97 | loss  5.60 | ppl   270.17
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 26.32 | loss  5.54 | ppl   255.71
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 27.25 | loss  5.76 | ppl   318.41
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 27.15 | loss  5.80 | ppl   330.36
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 27.12 | loss  5.68 | ppl   292.80
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 88.93s | valid loss  5.77 | valid ppl   320.49
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 27.35 | loss  5.52 | ppl   248.46
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 26.89 | loss  5.71 | ppl   302.62
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 26.78 | loss  5.31 | ppl   203.09
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 26.99 | loss  5.74 | ppl   312.51
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 26.57 | loss  5.48 | ppl   239.77
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 27.04 | loss  5.64 | ppl   280.61
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 27.10 | loss  5.83 | ppl   340.04
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 26.98 | loss  5.43 | ppl   229.03
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 27.07 | loss  5.48 | ppl   239.72
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 26.97 | loss  5.30 | ppl   199.37
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 26.24 | loss  5.31 | ppl   202.93
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 27.26 | loss  5.55 | ppl   258.04
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 27.03 | loss  5.50 | ppl   244.68
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 27.14 | loss  5.39 | ppl   219.19
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 89.00s | valid loss  5.74 | valid ppl   310.90
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 27.12 | loss  5.19 | ppl   180.06
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 27.00 | loss  5.46 | ppl   234.14
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 27.04 | loss  5.03 | ppl   152.83
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 26.89 | loss  5.49 | ppl   242.07
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 26.10 | loss  5.19 | ppl   178.69
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 26.98 | loss  5.36 | ppl   213.37
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 26.96 | loss  5.37 | ppl   214.75
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 26.94 | loss  5.04 | ppl   154.72
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 26.79 | loss  5.11 | ppl   165.61
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 26.96 | loss  4.99 | ppl   147.65
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 26.26 | loss  4.98 | ppl   145.80
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 26.95 | loss  5.23 | ppl   186.95
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 27.05 | loss  5.17 | ppl   176.23
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 27.12 | loss  5.03 | ppl   152.22
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 88.75s | valid loss  5.51 | valid ppl   246.96
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 27.04 | loss  5.12 | ppl   166.60
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 27.06 | loss  5.32 | ppl   204.50
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 27.05 | loss  4.94 | ppl   139.64
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 26.82 | loss  5.42 | ppl   225.43
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 26.18 | loss  5.10 | ppl   164.12
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 27.08 | loss  5.28 | ppl   195.89
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 26.98 | loss  5.29 | ppl   199.20
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 27.06 | loss  4.97 | ppl   143.34
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 26.81 | loss  5.07 | ppl   159.83
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 26.92 | loss  4.97 | ppl   144.10
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 26.54 | loss  4.94 | ppl   139.09
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 27.15 | loss  5.16 | ppl   174.18
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 27.23 | loss  5.16 | ppl   173.34
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 27.03 | loss  5.04 | ppl   154.09
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 88.90s | valid loss  5.50 | valid ppl   243.93
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 27.08 | loss  5.04 | ppl   154.89
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 27.20 | loss  5.29 | ppl   199.11
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 26.94 | loss  4.86 | ppl   128.38
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 26.94 | loss  5.36 | ppl   213.34
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 26.30 | loss  4.95 | ppl   141.51
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 26.86 | loss  5.19 | ppl   179.91
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 27.00 | loss  5.25 | ppl   189.70
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 26.96 | loss  4.86 | ppl   129.24
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 27.17 | loss  5.04 | ppl   155.16
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 26.80 | loss  4.91 | ppl   135.82
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 26.16 | loss  4.93 | ppl   137.70
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 26.83 | loss  5.05 | ppl   155.68
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 26.92 | loss  5.09 | ppl   162.42
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 26.69 | loss  4.99 | ppl   147.42
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 88.55s | valid loss  5.49 | valid ppl   243.41
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.39 | test ppl   219.60
=========================================================================================
Sanity Check 2: Mask Reshuffle
Switching to weight training by switching off requires_grad for scores and switching it on for weights.
transformer_encoder.layers.0.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.0.attn.value.flag | nonzeros =    7576 /   40000 ( 18.94%) | total_pruned =   32424 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc1.flag | nonzeros =     675 /   40000 (  1.69%) | total_pruned =   39325 | shape = (200, 200)
transformer_encoder.layers.0.mlp.fc2.flag | nonzeros =     612 /   40000 (  1.53%) | total_pruned =   39388 | shape = (200, 200)
transformer_encoder.layers.1.attn.query.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.key.flag | nonzeros =       0 /   40000 (  0.00%) | total_pruned =   40000 | shape = (200, 200)
transformer_encoder.layers.1.attn.value.flag | nonzeros =    4864 /   40000 ( 12.16%) | total_pruned =   35136 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc1.flag | nonzeros =    1167 /   40000 (  2.92%) | total_pruned =   38833 | shape = (200, 200)
transformer_encoder.layers.1.mlp.fc2.flag | nonzeros =     802 /   40000 (  2.00%) | total_pruned =   39198 | shape = (200, 200)
alive: 15696, pruned : 384304, total: 400000, (  3.92% remained)
Rounding model with scheme: all_ones
| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 27.06 | loss  7.02 | ppl  1113.83
| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 27.11 | loss  6.69 | ppl   802.66
| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 26.80 | loss  6.36 | ppl   576.44
| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 25.97 | loss  6.73 | ppl   841.12
| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 27.03 | loss  6.51 | ppl   671.31
| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 26.94 | loss  6.56 | ppl   706.25
| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 26.92 | loss  6.50 | ppl   661.97
| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 26.83 | loss  6.17 | ppl   477.45
| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 26.87 | loss  6.14 | ppl   465.35
| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 27.08 | loss  6.13 | ppl   461.58
| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 26.28 | loss  6.05 | ppl   423.48
| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 26.96 | loss  6.21 | ppl   500.13
| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 26.99 | loss  6.06 | ppl   426.77
| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 26.88 | loss  6.12 | ppl   456.98
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 88.63s | valid loss  5.97 | valid ppl   393.26
-----------------------------------------------------------------------------------------
| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch 26.89 | loss  5.89 | ppl   362.27
| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch 26.96 | loss  5.99 | ppl   399.03
| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch 26.84 | loss  5.62 | ppl   275.51
| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch 27.18 | loss  6.06 | ppl   427.72
| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch 26.19 | loss  5.84 | ppl   342.56
| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch 26.85 | loss  5.94 | ppl   379.02
| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch 27.03 | loss  6.00 | ppl   401.44
| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch 26.96 | loss  5.69 | ppl   295.95
| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch 26.94 | loss  5.78 | ppl   323.51
| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch 26.89 | loss  5.63 | ppl   279.23
| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch 26.15 | loss  5.55 | ppl   257.49
| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch 26.86 | loss  5.77 | ppl   321.34
| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch 27.01 | loss  5.78 | ppl   322.55
| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch 26.92 | loss  5.63 | ppl   278.99
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 88.57s | valid loss  5.77 | valid ppl   320.60
-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch 27.17 | loss  5.57 | ppl   262.48
| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch 26.80 | loss  5.71 | ppl   302.29
| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch 26.82 | loss  5.34 | ppl   208.21
| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch 27.35 | loss  5.69 | ppl   296.77
| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch 26.10 | loss  5.48 | ppl   238.87
| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch 26.75 | loss  5.70 | ppl   300.24
| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch 26.72 | loss  5.70 | ppl   297.48
| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch 27.01 | loss  5.45 | ppl   232.99
| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch 26.95 | loss  5.50 | ppl   244.97
| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch 27.14 | loss  5.36 | ppl   212.82
| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch 26.22 | loss  5.27 | ppl   194.88
| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch 26.78 | loss  5.56 | ppl   259.57
| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch 26.76 | loss  5.60 | ppl   269.16
| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch 26.97 | loss  5.42 | ppl   224.85
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 88.62s | valid loss  5.72 | valid ppl   304.07
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch 27.03 | loss  5.20 | ppl   181.52
| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch 26.74 | loss  5.43 | ppl   227.10
| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch 27.00 | loss  5.05 | ppl   155.25
| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch 27.06 | loss  5.51 | ppl   248.13
| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch 26.12 | loss  5.19 | ppl   178.63
| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch 26.81 | loss  5.33 | ppl   205.81
| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch 26.98 | loss  5.34 | ppl   209.54
| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch 26.76 | loss  4.99 | ppl   146.70
| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch 27.10 | loss  5.10 | ppl   164.55
| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch 27.10 | loss  4.98 | ppl   145.61
| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch 26.35 | loss  4.98 | ppl   145.65
| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch 26.85 | loss  5.17 | ppl   176.58
| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch 26.91 | loss  5.27 | ppl   194.21
| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch 27.20 | loss  5.02 | ppl   152.05
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 88.94s | valid loss  5.52 | valid ppl   250.77
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch 27.02 | loss  5.11 | ppl   165.03
| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch 26.91 | loss  5.33 | ppl   206.66
| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch 26.85 | loss  4.97 | ppl   144.70
| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch 27.35 | loss  5.43 | ppl   228.42
| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch 26.46 | loss  5.12 | ppl   167.46
| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch 26.85 | loss  5.26 | ppl   192.55
| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch 27.01 | loss  5.28 | ppl   197.15
| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch 26.90 | loss  4.92 | ppl   136.63
| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch 27.05 | loss  5.04 | ppl   154.08
| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch 26.82 | loss  4.95 | ppl   140.70
| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch 26.25 | loss  4.92 | ppl   136.70
| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch 26.78 | loss  5.14 | ppl   170.56
| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch 26.76 | loss  5.18 | ppl   178.30
| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch 26.94 | loss  4.97 | ppl   143.58
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 88.65s | valid loss  5.52 | valid ppl   249.11
-----------------------------------------------------------------------------------------
| epoch  11 |   200/ 2983 batches | lr 1.25 | ms/batch 27.10 | loss  5.04 | ppl   155.11
| epoch  11 |   400/ 2983 batches | lr 1.25 | ms/batch 26.94 | loss  5.28 | ppl   195.65
| epoch  11 |   600/ 2983 batches | lr 1.25 | ms/batch 26.98 | loss  4.87 | ppl   130.44
| epoch  11 |   800/ 2983 batches | lr 1.25 | ms/batch 27.63 | loss  5.33 | ppl   205.42
| epoch  11 |  1000/ 2983 batches | lr 1.25 | ms/batch 26.65 | loss  5.04 | ppl   155.09
| epoch  11 |  1200/ 2983 batches | lr 1.25 | ms/batch 26.91 | loss  5.25 | ppl   190.63
| epoch  11 |  1400/ 2983 batches | lr 1.25 | ms/batch 26.80 | loss  5.20 | ppl   180.54
| epoch  11 |  1600/ 2983 batches | lr 1.25 | ms/batch 27.00 | loss  4.83 | ppl   125.27
| epoch  11 |  1800/ 2983 batches | lr 1.25 | ms/batch 27.15 | loss  5.04 | ppl   155.17
| epoch  11 |  2000/ 2983 batches | lr 1.25 | ms/batch 26.72 | loss  4.83 | ppl   125.59
| epoch  11 |  2200/ 2983 batches | lr 1.25 | ms/batch 26.08 | loss  4.87 | ppl   130.30
| epoch  11 |  2400/ 2983 batches | lr 1.25 | ms/batch 22.22 | loss  5.11 | ppl   165.52
| epoch  11 |  2600/ 2983 batches | lr 1.25 | ms/batch 19.17 | loss  5.15 | ppl   171.76
| epoch  11 |  2800/ 2983 batches | lr 1.25 | ms/batch 19.13 | loss  4.93 | ppl   138.08
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 81.13s | valid loss  5.51 | valid ppl   246.47
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  5.41 | test ppl   222.86
=========================================================================================
