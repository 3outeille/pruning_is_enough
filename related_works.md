
* Weak LTH, Pruning+Training 
	* Learning both Weights and Connections for Efficient Neural Networks (2015)
		- Learn only the "important" connections. Combination of pruning \& training.
	* Lottery Ticket Hypothesis
		- Neural networks contain sparse subnetworks that can be effectively trained from scratch when reset to their initialization


* Strong LTH, (only) Pruning
	* Whatâ€™s Hidden in a Randomly Weighted Neural Network? 
		- Randomly weighted (overparameterized) neural network contains a subnetwork which performs near SOTA. 
		- Edge-popup (EP) algorithm

* Sparsity usinig L0 regularization

	* LEARNING SPARSE NEURAL NETWORKS THROUGH L0 REGULARIZATION
		- Suggested "surrogate" L0 regularization, in order to sparsify NN
		- Q. Not sure how they applied "reparameterization trick"
	
	* Winning the Lottery with Continuous Sparsification



* Optimization in real/binary values
	* Pseudo-boolean function


* Mode connectivity
	* Linear Mode Connectivity and the Lottery Ticket Hypothesis
	* Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs
